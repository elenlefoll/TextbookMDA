---
bibliography: references.bib
engine: knitr
---

```{css}
#| echo: false
p {
  text-align: justify
}
```

# Open Science statement

Among the wealth of Textbook English publications summarised in Chapter 3 (see also [Appendix A](https://elenlefoll.github.io/TextbookMDA/AppendixA.html)), very few have included the data and, where relevant, the code necessary to reproduce or replicate the findings that they report [thereby reflecting current sharing practices in linguistics more broadly, see @bochynskaReproducibleResearchPractices2023].[^openscience-1]

[^openscience-1]: This is also true of my own earlier work on the language of EFL textbooks [@LeFollRegisterVariationSchool2021; -@LeFollMakingteamistakes2022; -@LeFollputtingsaltmy2022]. More recent work conducted as part of this project , however, was published alongside with the data and code [@LeFollTextbookEnglishCorpusBased2022; -@LeFollconceptualreplicationMultiDimensional2023; -@LeFollSchulenglischmultidimensionalmodelsubmitted].

Although the terms are sometimes used interchangeably [see @parsonsCommunitysourcedGlossaryOpen2022 for a comprehensive glossary of Open Science terminology],  ‘reproducibility’ is used here to refer to the ability to obtain the same results using the researchers’ original data and code, whilst ‘replicability’[^openscience-2] entails repeating a study and obtaining compatible results with different data analysed with either the same or different methods [@Berez-KroekerReproducibleresearchlinguistics2018, p. 4; @PorteDoingReplicationResearch2018, pp. 6-7]. Not only does not sharing data and materials mean that published results are not reproducible, hereby making it difficult to assess their reliability, it also makes it very difficult to attempt to replicate the results to gain insights into the extent to which they are generalisable, e.g., across a different set of EFL textbooks used in a different educational context [see also @lefollWhyWeNeed2022; @mcmanusReplicationOpenScienceinpress].

[^openscience-2]: Confusingly, other terms are also frequently used to refer to the same or related concepts, e.g., *repeatability*, *robustness* and *generalisability* [see, e.g., @BelzSystematicReviewReproducibility2021, p. 2-3; @parsonsCommunitysourcedGlossaryOpen2022].

A major barrier to the reproducibility of (corpus) linguistic research is that it is often not possible for copyright or, when participants are involved, data protection reasons to make linguistic data available to the wider public. However, both research practice and the impact of our research can already be greatly improved if we publish our code or, when using GUI software, methods sections detailed enough for an independent researcher to be able to perfectly repeat the full procedure. If this is done, it is possible to conduct detailed reviews of our methodologies and replicate the effects reported in published literature using different data.

Aside from data protection and copyright restrictions, there are, of course, many more reasons why researchers may be reluctant to share their data and code [see, e.g., @al-hoorieOpenScholarshipTransparency2024; @gomesWhyDonWe2022]. It is not within the scope of this monograph to discuss these; however, it is important to acknowledge that, in many ways, such transparency makes us vulnerable. At the end of the day: to err is human. Yet, the risks involved in committing to Open Science practices are particularly tangible for researchers working on individual projects, like me, who have had no formal training in project management, programming, or versioning, and have therefore had to learn “on the job”. Nonetheless, I am convinced that the advantages outweigh the risks. Striving for transparency helps both the researchers themselves and others reviewing the work to spot and address problems. As a result, the research community can build on both the mishaps and successes of previous research, thus improving the efficiency of research processes and ultimately contributing to advancing scientific progress.

It is with this in mind that I have decided, whenever possible, to publish the data and code necessary to reproduce the results reported in the present monograph following the FAIR principles [i.e., ensuring that research materials are Findable, Accessible, Interoperable and Reusable, see @wilkinson2016]. For copyright reasons, the corpora themselves cannot be made available. However, the full, unedited tabular outputs of the tool used for automatic corpus annotation (the [MFTE Perl](https://github.com/elenlefoll/MultiFeatureTaggerEnglish); see 5.3.2 and [Appendix C](https://elenlefoll.github.io/TextbookMDA/AppendixC.html)) are published in the [Online Supplements](https://elenlefoll.github.io/TextbookMDA). Together with the commented data analysis scripts also published in the Online Supplements, as well as in the associated Open Science Framework (OSF) repository, these tables allow for the computational reproduction of all of the results and plots discussed in the following chapters.

In describing the study’s methodology, maximum transparency is strived for by reporting on how each sample size was determined and on which grounds variables and data points were excluded, manipulated and/or transformed. Most of these operations were conducted in the open-source programming language and environment R [@rcoreteam2022]. The annotated data processing and analysis scripts have been rendered to HTML pages (viewable in the Online Supplements) thus allowing researchers to review the procedures followed without necessarily installing all the required packages and running the code themselves. Furthermore, these scripts include additional analyses, tables, and plots that were made as part of this study but which, for reasons of space, were not reported on in detail here. Whenever data, packages or other open-source scripts from other researchers were used, links to these are also provided in the [Online Supplements](https://elenlefoll.github.io/TextbookMDA/references.html) (in addition to the corresponding references in the bibliography).
