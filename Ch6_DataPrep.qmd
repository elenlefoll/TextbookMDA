# Appendix D: Data Preparation for the Model of Intra-Textbook Variation {.unnumbered}

This script documents the steps taken to pre-process the Textbook English Corpus (TEC) data that were entered in the multi-dimensional model of intra-textbook linguistic variation (Chapter 6).

## Packages required

The following packages must be installed and loaded to process the data.

```{r}
#| label: set-up

#renv::restore() # Restore the project's dependencies from the lockfile to ensure that same package versions are used as in the original study

library(caret) # For its confusion matrix function
library(DT) # To display interactive HTML tables
library(here) # For dynamic file paths
library(knitr) # Loaded to display the tables using the kable() function
library(patchwork) # Needed to put together Fig. 1
library(PerformanceAnalytics) # For the correlation plot
library(psych) # For various useful, stats function
library(tidyverse) # For data wrangling

```

## Data import from MFTE output

The raw data used in this script is a tab-separated file that corresponds to the tabular output of mixed normalised frequencies as generated by the [MFTE Perl v. 3.1](https://github.com/mshakirDr/MultiFeatureTaggerEnglish) [@lefoll2021].

```{r}
#| label: raw_data
#| code-fold: true

# Read in Textbook Corpus data
TxBcounts <- read.delim(here("data", "MFTE", "TxB900MDA_3.1_normed_complex_counts.tsv"), header = TRUE, stringsAsFactors = TRUE)
TxBcounts <- TxBcounts |> 
  filter(Filename!=".DS_Store") |>  
  droplevels()
#str(TxBcounts) # Check sanity of data
#nrow(TxBcounts) # Should be 2014 files
datatable(TxBcounts,
  filter = "top",
) |> 
  formatRound(2:ncol(TxBcounts), digits=2)

```

Metadata was added on the basis of the filenames.

```{r}
#| label: metadata

# Adding a textbook proficiency level
TxBLevels <- read.delim(here("data", "metadata", "TxB900MDA_ProficiencyLevels.csv"), sep = ",")
TxBcounts <- full_join(TxBcounts, TxBLevels, by = "Filename") |>  
  mutate(Level = as.factor(Level)) |>  
  mutate(Filename = as.factor(Filename))

# Check distribution and that there are no NAs
summary(TxBcounts$Level) |> 
  kable(col.names = c("Textbook Level", "# of texts"))

# Check matching on random sample
# TxBcounts |>
#   select(Filename, Level) |>  
#   sample_n(20) 

# Adding a register variable from the file names
TxBcounts$Register <- as.factor(stringr::str_extract(TxBcounts$Filename, "Spoken|Narrative|Other|Personal|Informative|Instructional|Poetry")) # Add a variable for Textbook Register
summary(TxBcounts$Register) |> 
  kable(col.names = c("Textbook Register", "# of texts"))

TxBcounts$Register <- car::recode(TxBcounts$Register, "'Narrative' = 'Fiction'; 'Spoken' = 'Conversation'")
#colnames(TxBcounts) # Check all the variables make sense

# Adding a textbook series variable from the file names
TxBcounts$Filename <- stringr::str_replace(TxBcounts$Filename, "English_In_Mind|English_in_Mind", "EIM") 
TxBcounts$Filename <- stringr::str_replace(TxBcounts$Filename, "New_GreenLine", "NGL") # Otherwise the regex for GreenLine will override New_GreenLine
TxBcounts$Filename <- stringr::str_replace(TxBcounts$Filename, "Piece_of_cake", "POC") # Shorten label for ease of plotting
TxBcounts$Series <- as.factor(stringr::str_extract(TxBcounts$Filename, "Access|Achievers|EIM|GreenLine|HT|NB|NM|POC|JTT|NGL|Solutions")) # Extract textbook series from (ammended) filenames
summary(TxBcounts$Series)  |> 
  kable(col.names = c("Textbook Name", "# of texts"))

# Including the French textbooks for the first year of Lycée to their corresponding publisher series from collège
TxBcounts$Series <-car::recode(TxBcounts$Series, "c('NB', 'JTT') = 'JTT'; c('NM', 'HT') = 'HT'") # Recode final volumes of French series (see Section 4.3.1.1 on textbook selection for details)
summary(TxBcounts$Series) |> 
  kable(col.names = c("Textbook Series", "# of texts"))

# Adding a textbook country of use variable from the series variable
TxBcounts$Country <- TxBcounts$Series
TxBcounts$Country <- car::recode(TxBcounts$Series, "c('Access', 'GreenLine', 'NGL') = 'Germany'; c('Achievers', 'EIM', 'Solutions') = 'Spain'; c('HT', 'NB', 'NM', 'POC', 'JTT') = 'France'")
summary(TxBcounts$Country) |> 
  kable(col.names = c("Country of Use", "# of texts"))

# Re-order variables
#colnames(TxBcounts)
TxBcounts <- select(TxBcounts, order(names(TxBcounts))) %>%
  select(Filename, Country, Series, Level, Register, Words, everything())
#colnames(TxBcounts)

```

### Corpus size

This table provides some summary statistics about the number of words included in the TEC texts originally tagged for this study.

```{r}
#| label: TEC-summary-stats

TxBcounts  |>  
  group_by(Register) |>  
  summarise(totaltexts = n(), totalwords = sum(Words), mean = as.integer(mean(Words)), sd = as.integer(sd(Words)), TTRmean = mean(TTR)) |>  
  kable(digits = 2, format.args = list(big.mark = ","))

#TxBcounts <- saveRDS(TxBcounts, here("data", "processed", "TxBcounts.rds"))

```

## Data preparation for PCA

Poetry texts were removed for this analysis as there were too few compared to the other register categories.

```{r}
#| label: Registers

summary(TxBcounts$Register) |>  
  kable(col.names = c("Register", "# texts"))

```

This led to the following distribution of texts across the five textbook English registers examined in the model of intra-textbook linguistic variation:

```{r}
#| label: Poetry-removal

TxBcounts <- TxBcounts |>  
  filter(Register!="Poetry") |>  
  droplevels()

summary(TxBcounts$Register) |>  
  kable(col.names = c("Register", "# texts"))
```

### Feature distributions

The distributions of each linguistic features were examined by means of visualisation. As shown below, before transformation, many of the features displayed highly skewed distributions.

```{r}
#| label: distribution-viz
#| code-fold: true
#| fig-height: 40

TxBcounts |> 
  select(-Words) |>  
  keep(is.numeric) |>  
  tidyr::gather() |>  # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2
  ggplot(aes(value)) +
    theme_bw() +
    facet_wrap(~ key, scales = "free", ncol = 4) +
    scale_x_continuous(expand=c(0,0)) +
    geom_histogram(bins = 30, colour= "darkred", fill = "darkred", alpha = 0.5)

#ggsave(here("plots", "TEC-HistogramPlotsAllVariablesTEC-only.svg"), width = 20, height = 45)

```

### Feature removal

A number of features were removed from the dataset as they are not linguistically interpretable. In the case of the TEC, this included the variable CD because numbers spelt out as digits were removed from the textbooks before these were tagged with the MFTE. In addition, the variables LIKE and SO because these are "bin" features included in the output of the MFTE to ensure that the counts for these polysemous words do not inflate other categories due to mistags [@lefoll2021a].

Whenever linguistically meaningful, very low-frequency features were merged. Finally, features absent from more than third of texts were also excluded. For the analysis intra-textbook register variation, the following linguistic features were excluded from the analysis due to low dispersion:

```{r}
#| label: feature-removal

# Removal of meaningless features:
TxBcounts <- TxBcounts |>  
  select(-c(CD, LIKE, SO))

# Function to compute percentage of texts with occurrences meeting a condition
compute_percentage <- function(data, condition, threshold) {
  numeric_data <- Filter(is.numeric, data)
  percentage <- round(colSums(condition[, sapply(numeric_data, is.numeric)])/nrow(data) * 100, 2)
  percentage <- as.data.frame(percentage)
  colnames(percentage) <- "Percentage"
  percentage <- percentage |>  
    filter(!is.na(Percentage)) |> 
    rownames_to_column() |> 
    arrange(Percentage)
  if (!missing(threshold)) {
    percentage <- percentage |>  
      filter(Percentage > threshold)
  }
  return(percentage)
}

# Calculate percentage of texts with 0 occurrences of each feature
zero_features <- compute_percentage(TxBcounts, TxBcounts == 0, 66.6)
# zero_features |> 
#   kable(col.names = c("Feature", "% texts with zero occurrences"))

# Combine low frequency features into meaningful groups whenever this makes linguistic sense
TxBcounts <- TxBcounts |>  
  mutate(JJPR = ABLE + JJPR, ABLE = NULL) |>  
  mutate(PASS = PGET + PASS, PGET = NULL)

# Re-calculate percentage of texts with 0 occurrences of each feature
zero_features2 <- compute_percentage(TxBcounts, TxBcounts == 0, 66.6)
zero_features2 |> 
  kable(col.names = c("Feature", "% texts with zero occurrences"))

# Drop variables with low document frequency
TxBcounts <- select(TxBcounts, -one_of(zero_features2$rowname))
#ncol(TxBcounts)-8 # Number of linguistic features remaining

# List of features
#colnames(TxBcounts)

```

These feature removal operations resulted in a feature set of `{r} ncol(TxBcounts)-8` linguistic variables.

### Identifying potential outlier texts

All normalised frequencies were normalised to identify any potential outlier texts.

```{r}
#| label: z-standardisation-outliers
#| fig-width: 15
#| fig-height: 10

# First scale the normalised counts (z-standardisation) to be able to compare the various features
TxBzcounts <- TxBcounts |> 
  select(-Words) |>  
  keep(is.numeric) |>  
  scale()

boxplot(TxBzcounts, las = 3, main = "z-scores") # Slow to open!

# If necessary, remove any outliers at this stage.
TxBdata <- cbind(TxBcounts[,1:6], as.data.frame(TxBzcounts))

outliers <- TxBdata |>  
  select(-c(Words, LD, TTR)) |>  
  filter(if_any(where(is.numeric), ~ .x > 8)) |>  
  select(Filename)

```

The following outlier texts were identified and excluded in subsequent analyses.

```{r}
#| label: list-outliers
#| code-fold: true

outliers

TxBcounts <- TxBcounts |>  
  filter(!Filename %in% outliers$Filename)

#saveRDS(TxBcounts, here("data", "processed", "TxBcounts3.rds")) # Last saved 6 March 2024

TxBzcounts <- TxBcounts |> 
  select(-Words) |>  
  keep(is.numeric) |>  
  scale()

```

This resulted in `{r} nrow(TxBcounts) |> format(big.mark=",")` TEC texts being included in the model of intra-textbook linguistic variation with the following standardised feature distributions.

```{r}
#| label: z-transformed-distributions
#| fig-height: 40
#| code-fold: true

TxBzcounts |> 
  as.data.frame() |>  
  gather() |>  # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2
  ggplot(aes(value)) +
    theme_bw() +
    facet_wrap(~ key, scales = "free", ncol = 4) +
    scale_x_continuous(expand=c(0,0)) +
    geom_histogram(bins = 30, colour= "darkred", fill = "darkred", alpha = 0.5)

#ggsave(here("plots", "TEC-zscores-HistogramsAllVariablesTEC-only.svg"), width = 20, height = 45)

```

### Signed log transformation

A signed logarithmic transformation was applied to (further) deskew the feature distributions [@diwersy2014; @neumann2021].

The signed log transformation function was inspired by the SignedLog function proposed in https://cran.r-project.org/web/packages/DataVisualizations/DataVisualizations.pdf

```{r}
#| label: signed.log.transformation

# All features are signed log-transformed (note that this is also what Neumann & Evert 2021 propose)
signed.log <- function(x) {
  sign(x) * log(abs(x) + 1)
  }

TxBzlogcounts <- signed.log(TxBzcounts) # Standardise first, then signed log transform

#saveRDS(TxBzlogcounts, here("data", "processed", "TxBzlogcounts.rds")) # Last saved 6 March 2024
```

The new feature distributions are visualised below.

```{r}
#| label: signed.log.transformation-distributions
#| fig-height: 40
#| code-fold: true

TxBzlogcounts |> 
  as.data.frame() |>  
  gather() |>  # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2
  ggplot(aes(value, after_stat(density))) +
  theme_bw() +
  facet_wrap(~ key, scales = "free", ncol = 4) +
  scale_x_continuous(expand=c(0,0)) +
  scale_y_continuous(limits = c(0,NA)) +
  geom_histogram(bins = 30, colour= "black", fill = "grey") +
  geom_density(colour = "darkred", weight = 2, fill="darkred", alpha = .4)

#ggsave(here("plots", "DensityPlotsAllVariablesSignedLog-TEC-only.svg"), width = 15, height = 49)

```

The following correlation plots serve to illustrate the effect of the variable transformations performed in the above chunks.

Example feature distributions before transformations:

```{r}
#| label: example-correlation-plots
#| fig-height: 10
#| fig-width: 10
#| code-fold: true

# This is a slightly amended version of the PerformanceAnalytics::chart.Correlation() function. It simply removes the significance stars that are meaningless with this many data points (see commented out lines below)

chart.Correlation.nostars <- function (R, histogram = TRUE, method = c("pearson", "kendall", "spearman"), ...) {
  x = checkData(R, method = "matrix")
  if (missing(method)) 
    method = method[1]
  panel.cor <- function(x, y, digits = 2, prefix = "", use = "pairwise.complete.obs", method = "pearson", cex.cor, ...) {
    usr <- par("usr")
    on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- cor(x, y, use = use, method = method)
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste(prefix, txt, sep = "")
    if (missing(cex.cor)) 
      cex <- 0.8/strwidth(txt)
    test <- cor.test(as.numeric(x), as.numeric(y), method = method)
    # Signif <- symnum(test$p.value, corr = FALSE, na = FALSE, 
    #                  cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1), symbols = c("***", 
    #                                                                           "**", "*", ".", " "))
    text(0.5, 0.5, txt, cex = cex * (abs(r) + 0.3)/1.3)
    # text(0.8, 0.8, Signif, cex = cex, col = 2)
  }
  f <- function(t) {
    dnorm(t, mean = mean(x), sd = sd.xts(x))
  }
  dotargs <- list(...)
  dotargs$method <- NULL
  rm(method)
  hist.panel = function(x, ... = NULL) {
    par(new = TRUE)
    hist(x, col = "light gray", probability = TRUE, 
         axes = FALSE, main = "", breaks = "FD")
    lines(density(x, na.rm = TRUE), col = "red", lwd = 1)
    rug(x)
  }
  if (histogram) 
    pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor, 
          diag.panel = hist.panel)
  else pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor)
}

# Example plot without any variable transformation
example1 <- TxBcounts |> 
  select(NN,PROG,SPLIT,ACT,FPP1S)

#png(here("plots", "CorrChart-TEC-examples-normedcounts.png"), width = 20, height = 20, units = "cm", res = 300)
chart.Correlation.nostars(example1, histogram=TRUE, pch=19)
#dev.off()

```

Example feature distributions after transformations:

```{r}
#| label: example-correlation-plots2
#| fig-height: 10
#| fig-width: 10
#| code-fold: true

# Example plot with transformed variables
example2 <- TxBzlogcounts |> 
  as.data.frame() |>  
  select(NN,PROG,SPLIT,ACT,FPP1S)

#png(here("plots", "CorrChart-TEC-examples-zsignedlogcounts.png"), width = 20, height = 20, units = "cm", res = 300)
chart.Correlation.nostars(example2, histogram=TRUE, pch=19)
#dev.off()
```

### Feature correlations

The correlations of the transformed feature frequencies can be visualised in the form of a heatmap. Negative correlations are rendered in blue, whereas positive ones are in red.

```{r}
#| label: heatmap
#| fig-height: 15
#| code-fold: true

# Simple heatmap in base R (inspired by Stephanie Evert's SIGIL code)
cor.colours <- c(
  hsv(h=2/3, v=1, s=(10:1)/10), # blue = negative correlation 
  rgb(1,1,1), # white = no correlation 
  hsv(h=0, v=1, s=(1:10/10))) # red = positive correlation

#png(here("plots", "heatmapzlogcounts-TEC-only.png"), width = 30, height= 30, units = "cm", res = 300)
heatmap(cor(TxBzlogcounts), 
        symm=TRUE, 
        zlim=c(-1,1), 
        col=cor.colours, 
        margins=c(0,0))
#dev.off()

# Calculate the sum of all the words in the tagged texts of the TEC
totalwords <- TxBcounts |>  
  select(Words) |> 
  sum() |> 
  format(big.mark=",")
```

## Composition of TEC texts/files

These figures and tables provide summary statistics on the texts/files of the TEC that were entered in the multi-dimensional model of intra-textbook linguistic variation. In total, the TEC texts entered amounted to `{r} totalwords` words.

```{r}
#| label: TEC-metadata
#| code-fold: true
#| fig-width: 10
#| fig-height: 20

metadata <- TxBcounts |>  
  select(Filename, Country, Series, Level, Register, Words) |>  
  mutate(Volume = paste(Series, Level)) |>  
  mutate(Volume = fct_rev(Volume)) |>  
  mutate(Volume = fct_reorder(Volume, as.numeric(Level))) |>  
  group_by(Volume) |>  
  mutate(wordcount = sum(Words)) |>  
  ungroup() |>  
  distinct(Volume, .keep_all = TRUE)

# Plot for book
metadata2 <- TxBcounts |>  
  select(Country, Series, Level, Register, Words) |>  
  mutate(Volume = paste(Series, Level)) |>  
  mutate(Volume = fct_rev(Volume)) |>  
  #mutate(Volume = fct_reorder(Volume, as.numeric(Level))) |>  
  group_by(Volume, Register) |>  
  mutate(wordcount = sum(Words)) |>  
  ungroup() |>  
  distinct(Volume, Register, .keep_all = TRUE)

# This is the palette created above on the basis of the suffrager pakcage (but without needed to install the package)
palette <- c("#BD241E", "#A18A33", "#15274D", "#D54E1E", "#EA7E1E", "#4C4C4C", "#722672", "#F9B921", "#267226")

PlotSp <- metadata2 |>  
  filter(Country=="Spain") |>  
  #arrange(Volume) |>  
  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + 
    geom_bar(stat = "identity", position = "stack") +
    coord_flip(expand = FALSE) + # Removes those annoying ticks before each bar label
    theme_minimal() + theme(legend.position = "none") +
    labs(x = "Spain", y = "Cumulative word count") +
    scale_fill_manual(values = palette[c(5,4,3,2,1)], 
                      guide = guide_legend(reverse = TRUE))

PlotGer <- metadata2 |>  
  filter(Country=="Germany") |>  
  #arrange(Volume) |>  
  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + 
    geom_bar(stat = "identity", position = "stack") +
    coord_flip(expand = FALSE) +
    labs(x = "Germany", y = "") +
    scale_fill_manual(values = palette[c(5,4,3,2,1)], guide = guide_legend(reverse = TRUE)) +
    theme_minimal() + theme(legend.position = "none")

PlotFr <- metadata2 |>  
  filter(Country=="France") |>  
  #arrange(Volume) |>  
  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + 
    geom_bar(stat = "identity", position = "stack") +
    coord_flip(expand = FALSE) +
    labs(x = "France", y  = "", fill = "Register subcorpus") +
    scale_fill_manual(values = palette[c(5,4,3,2,1)], guide = guide_legend(reverse = TRUE, legend.hjust = 0)) +
    theme_minimal() + theme(legend.position = "top", legend.justification = "left")

PlotFr /
PlotGer /
PlotSp

#ggsave(here("plots", "TEC-T_wordcounts_book.svg"), width = 8, height = 12)

```

The following table provides information about the proportion of instructional language featured in each textbook series.

```{r}
#| label: TEC-metadata2
#| code-fold: true

metadataInstr <- TxBcounts |>  
  select(Country, Series, Level, Register, Words) |>  
  filter(Register=="Instructional") |>  
  mutate(Volume = paste(Series, Register)) |>  
  mutate(Volume = fct_rev(Volume)) |>  
  mutate(Volume = fct_reorder(Volume, as.numeric(Level))) |>  
  group_by(Volume, Register) |>  
  mutate(InstrWordcount = sum(Words)) |>  
  ungroup() |>  
  distinct(Volume, .keep_all = TRUE) |>  
  select(Series, InstrWordcount)

metaWordcount <- TxBcounts |>  
  select(Country, Series, Level, Register, Words) |>  
  group_by(Series) |>  
  mutate(TECwordcount = sum(Words)) |>  
  ungroup() |>  
  distinct(Series, .keep_all = TRUE) |>  
  select(Series, TECwordcount)

wordcount <- merge(metaWordcount, metadataInstr, by = "Series")

wordcount |>  
  mutate(InstrucPercent = InstrWordcount/TECwordcount*100) |>  
  arrange(InstrucPercent) |>  
  mutate(InstrucPercent = round(InstrucPercent, 2)) |>  
  kable(col.names = c("Textbook Series", "Total words", "Instructional words", "% of textbook content"), 
        digits = 2, 
        format.args = list(big.mark = ","))

```
