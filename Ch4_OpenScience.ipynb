{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "bibliography: references.bib\n",
        "---"
      ],
      "id": "d7faa1de"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{css}\n",
        "#| echo: false\n",
        "p {\n",
        "  text-align: justify\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "# Open Science statement\n",
        "\n",
        "Another important insight from the methodological part of the literature review (see Section 3.1 in book publication) is that, to the author’s best knowledge, no Textbook English study published so far has included (as an appendix or supplementary materials) the data and code necessary to reproduce or replicate the published results. As a result, it is very difficult to evaluate the reliability or robustness of the results reported [see also @lefoll2024].\n",
        "\n",
        "Though the terms are sometimes used interchangeably and different (at times incompatible) definitions abound, in computational sciences, ‘reproducibility’ usually refers to the ability to obtain the same results as an original study using the researchers’ data and code, whilst ‘replicability’ refers to obtaining compatible results with the same method but different data [@associationforcomputingmachinery2020; see also @berez-kroeker2018].\n",
        "\n",
        "A major barrier to the reproducibility of (corpus) linguistic research is that it is often not possible for copyright or, when participants are involved, data protection reasons to make linguistic data available to the wider public. However, both research practice and the impact of our research can already be greatly improved if we publish our code or, when using GUI software, methods sections detailed enough to be able to successfully replicate the full procedures. This step can enable others to conduct detailed reviews of our methodologies and conceptual replications of our results on different data.\n",
        "\n",
        "Aside from data protection and copyright regulations, there are, of course, many reasons why researchers may be reluctant to share their data and code [@berez-kroeker2018; @mcmanus2021]. It is not within the scope of this monograph to discuss these; however, it is clear that, in many ways, such transparency makes us vulnerable. At the end of the day: to err is human. Yet, the risks involved in committing to Open Science practices are particularly tangible for researchers working on individual projects, like myself, who have had no formal training in data management or programming and have therefore had to learn “on the job”. Nonetheless, I am convinced that the advantages outweigh the risks. Striving for transparency helps both the researchers themselves and others reviewing the work to spot and address problems. As a result, the research community can build on both the mishaps and successes of previous research, thus improving the efficiency of research processes and ultimately contributing to advancing scientific progress.\n",
        "\n",
        "It is with this in mind that I have decided, whenever possible, to publish all the raw data and code necessary to reproduce the results reported in the present monograph following the FAIR principles [i.e., ensuring that research data are Findable, Accessible, Interoperable and Reusable; @wilkinson2016]. For copyright reasons, the corpora themselves and annotated corpus data in the form of concordance lines cannot be made available. However, the outcome of both manual and automatic annotation processes is published in tabular formats in the Online Appendix. These tables allow for the reproduction of all the analyses reported on in the following chapters using the reproducible data analysis scripts also published in the [Online Supplements](https://elenlefoll.github.io/TextbookMDA) and in the associated Open Science Framework (OSF) repository.\n",
        "\n",
        "In all chapters of this monograph, full transparency is strived for by reporting on how each sample size was determined and on which grounds data points were excluded, manipulated and/or transformed. Most of these operations were conducted in the open-source programming language and environment R [@rcoreteam2022]. Most of the data processing and analysis scripts therefore consist of R markdown documents. These were rendered to HTML pages (viewable in the Online Supplements) thus allowing researchers to review the procedures followed without necessarily installing all the required packages and running the code themselves. These scripts also feature additional analyses, tables and plots that were made as part of this study but which, for reasons of space, were not reported on in detail here. Whenever additional software or open-source code from other researchers were used, links to these are also provided in the [Online Supplements](https://elenlefoll.github.io/TextbookMDA) (in addition to the corresponding references in the bibliography)."
      ],
      "id": "a4cb8db4"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}