[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Textbook English: A Multi-Dimensional Approach",
    "section": "",
    "text": "Preface\nThis Quarto book is work in progress. It will eventually contain the online supplements to:\n\nLe Foll, Elen. to appear. Textbook English: A Multi-Dimensional Approach [Studies in Corpus Linguistics]. Amsterdam: John Benjamins.\n\nThe book is based on my PhD thesis, which is accessible in Open Access:\n\nLe Foll, Elen. 2022. Textbook English: A Corpus-Based Analysis of the Language of EFL textbooks used in Secondary Schools in France, Germany and Spain. Osnabrück, Germany: Osnabrück University. PhD thesis. https://doi.org/10.48693/278.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "\n1  Introduction\n",
    "section": "",
    "text": "1.1 Research objectives and methodological approach\nThe above questions are critical because, as many adults’ lingering memories of school foreign language lessons testify (see also, e.g., Freudenstein 2002: 55), textbooks play an absolutely central role in classroom-based foreign language learning. In the following, we will see that the dominance of textbooks in EFL school contexts persists to this day. According to Thornbury (2012 in a response to Chong 2012: n.p.), they “(more often [than] not) instantiate the curriculum, provide the texts, and - to a large extent - guide the methodology”. In lower secondary EFL instructional contexts, in particular, textbooks constitute a major vector of foreign language input. Yet, numerous studies have shown that “considerable mismatches between naturally occurring English and the English that is put forward as a model in pedagogical descriptions” (Römer 2006: 125-26) exist. These mismatches have been observed and sometimes extensively described in textbooks’ representations of numerous language features ranging from the use of individual words and phraseological patterns (e.g., Conrad 2004 on the preposition though; Gouverneur 2008 on the high-frequency verbs make and take), to tenses and aspects (e.g., Barbieri & Eckhardt 2007 on reported speech; Römer 2005 on the progressive). More rarely, textbook language studies have also ventured into the study of spoken grammar (e.g., Gilmore 2004) and pragmatics (e.g., Hyland 1994 on hedging in ESP/EAP textbooks).\nHowever, as we will see in Chapter 2, previous EFL textbook studies have tended to focus on one or at most a handful of individual linguistic features. Taken together, they provide valuable insights into “the kind of synthetic English” (Römer 2004b: 185) that pupils are exposed to via their textbooks; yet, what is missing is a more comprehensive, broader understanding of what constitutes ‘Textbook English’ from a linguistic point of view. Although corpus-based2 textbook analysis can be traced back to the pioneering work of Dieter Mindt in the 1980s, the language of secondary school EFL textbooks (as opposed to that of general adult EFL or English for Specific Purposes [ESP] coursebooks) remains an understudied area.\nThe present study therefore sets out to describe the linguistic content of secondary school EFL textbooks and to survey the similarities and most striking differences between ‘Textbook English’ and ‘naturally occurring English’ as used outside the EFL classroom, with respect to a wide range of lexico-grammatical features.\nTo this end, a corpus of nine series of secondary school EFL textbooks (43 textbook volumes) used at lower secondary level in France, Germany, and Spain was compiled (see 4.3.1). In addition, three reference corpora are used as baselines for comparisons between the language input EFL learners are confronted with via their school textbooks and the kind of naturally occurring English that they can be expected to encounter, engage with, and produce themselves on leaving school. Two of these have been built specifically for this project with the aim of representing comparable ‘authentic’ (for a discussion of this controversial term in ELT, see 2.2) and age-appropriate learner target language.\nA bottom-up, corpus-based approach is adopted (e.g., Mindt 1992, 1995a; Biber & Quirk 2012; Biber & Gray 2015; Ronald Carter & McCarthy 2006a). A broad range of linguistic features are considered: ranging from tenses and aspects to negation and discourse markers. We will pay particular attention to the lexico-grammatical aspects of Textbook English that substantially diverge from the target learner language reference corpora and examine these with direct comparisons of textbook excerpts with comparable texts from the reference data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#outline-of-the-book",
    "href": "intro.html#outline-of-the-book",
    "title": "\n1  Introduction\n",
    "section": "\n1.2 Outline of the book",
    "text": "1.2 Outline of the book\nThe following chapter outlines the background to and motivation behind the present study. Chapter 3 then provides a literature review of state-of-the-art research on the language of school EFL textbooks. It is divided in two parts. Part 1 is a methodological review in which the various methods employed so far to analyse, describe, and evaluate Textbook English are explained and illustrated with selected studies. Part 2 summarises the results of existing studies on various aspects of Textbook English, including lexical, grammatical and pragmatic aspects. Based on the methodological limitations and the gaps identified in the existing literature, Chapter 4 elaborates the specific research questions addressed in the present study. These research questions informed the decision-making processes involved in the compilation of the Textbook English Corpus (TEC) and the selection/compilation of three reference corpora designed to represent learners’ target language. These processes and their motivations are explained in the remaining sections of Chapter 4.\nChapter 5 describes the multivariable statistical methods applied to describe the linguistic nature of Textbook English on multiple dimensions of linguistic variation. It begins by explaining the well-established multi-feature/dimensional analysis (MDA) method pioneered by Biber (1988, 1995; see also Berber Sardinha & Veirano Pinto 2014, 2019), before outlining the reasoning for the modified MDA framework applied in the present study. Chapter 6 presents the results of an MDA model of Textbook English which highlights the sources of linguistic variation within EFL textbooks across several dimensions of intra-textbook linguistic variation. Chapter 7 presents the results of a second MDA model that shows how Textbook English is both, in some respects, similar to and, in others, different from the kind of English that EFL learners are likely to encounter outside the classroom.\nChapter 8 explains how the two models contribute to a new understanding of the linguistic characteristics of Textbook English. This, in turn, has implications for teachers, textbook authors, editors, publishers, and policy-makers. These implications are discussed in Chapter 9. It first considers the potential impact of the substantial gaps between Textbook English and the target reference corpora before making suggestions as to how teachers, textbook authors, and editors may want to improve or supplement unnatural‑sounding pedagogical texts using corpora and corpus tools. Chapter 10 focuses on the study’s methodological strengths and limitations. It explains how the modified MDA framework presented and applied in this study may be of interest to corpus linguists working on a broad range of research questions. Chapter 11 concludes with a synthesis of the most important take-aways from the study. It also points to promising future research avenues.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "\n1  Introduction\n",
    "section": "",
    "text": "Dialogue from Speak English 6e série verte (Benhamou & Dominique 1977: 167). It was made popular by stand-up comedian Gad Elmaleh. More information on the context of this textbook dialogue can be found here. An extract of the comedy sketch by Gad Elmaleh that popularised the dialogue can be viewed here with English subtitles: https://youtu.be/11jG7lkwDwU?t=50.↩︎\nHere the adjectives ‘corpus-based’ and ‘corpus-driven’ are used synonymously (see, e.g., Meunier & Reppen 2015: 499 for further information as to how these terms are sometimes distinguished).↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Ch3_LitReview.html",
    "href": "Ch3_LitReview.html",
    "title": "\n2  Literature review\n",
    "section": "",
    "text": "This is a tabular overview of the Textbook English studies that I examined as part of my literature review. It presents the results of a non-exhaustive survey of Textbook English studies published over the past four decades, summarising some of the key information on each study, including its main language focus, methodological approach, information on the textbooks investigated, and, if applicable, on any reference corpora used. Empty cells represent fields that are either not applicable to this particular study or for which no information could be found. Intended as a dynamic resource, this interactive, searchable, and filterable table currently lists over 80 studies on the language content of English L2 textbooks, thereby demonstrating the breadth of Textbook English studies published as of early 2022.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature review</span>"
    ]
  },
  {
    "objectID": "Ch4_CorpusData.html",
    "href": "Ch4_CorpusData.html",
    "title": "\n3  Corpus data\n",
    "section": "",
    "text": "3.1 Textbook English Corpus (TEC)\nA detailed tabular overview of the composition of the Textbook English Corpus (TEC) together with the full bibliographic metadata is available at doi.org/10.5281/zenodo.4922819.\nNote that, for copyright reasons, the corpus itself cannot be published. If you are interested in using the corpus for non-commercial research purposes and/or in a potential research collaboration, please get in touch with me via e-mail.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corpus data</span>"
    ]
  },
  {
    "objectID": "Ch4_CorpusData.html#reference-corpora",
    "href": "Ch4_CorpusData.html#reference-corpora",
    "title": "\n3  Corpus data\n",
    "section": "\n3.2 Reference corpora",
    "text": "3.2 Reference corpora\n\n3.2.1 Spoken BNC2014\nThe original corpus files of the Spoken British National Corpus (BNC) 2014 (Love et al. 2017; Love et al. 2019) can be downloaded for free for research purposes from: http://corpora.lancs.ac.uk/bnc2014/signup.php. I used the untagged XML version.\nThe R script used to pre-process the untagged XML files as explained in Section 4.3.2.2 of the book can be found here: https://github.com/elenlefoll/TextbookEnglish/blob/main/3_Data/BNCspoken_nomark-up_JackJill.R\n\n3.2.2 Informative Texts for Teens Corpus (Info Teens)\nFor copyright reasons, the corpus itself cannot be made available. Details of its composition can be found in Section 4.3.2.5 of the book. If you are interested in using this corpus for non-commercial research purposes and/or in a potential research collaboration, please get in touch with me via e-mail.\n\n3.2.3 Youth Fiction corpus\nFor copyright reasons, the corpus itself cannot be made available. The corresponding metadata can be found here: https://github.com/elenlefoll/TextbookEnglish/blob/main/3_Data/3_Youth_Fiction_Index.csv. If you are interested in using this corpus for non-commercial research purposes and/or in a potential research collaboration, please get in touch with me via e-mail.\n\n\n\n\n\n\nLove, Robbie, Vaclav Brezina, Tony McEnery, Abi Hawtin, Andrew Hardie, and Claire Dembry. 2019. “Functional Variation in the Spoken BNC2014 and the Potential for Register Analysis.” Register Studies 1 (2): 296–317. https://doi.org/10.1075/rs.18013.lov.\n\n\nLove, Robbie, Claire Dembry, Andrew Hardie, Vaclav Brezina, and Tony McEnery. 2017. “The Spoken BNC2014.” International Journal of Corpus Linguistics 22 (3): 319–44. https://doi.org/https://doi.org/10.1075/ijcl.22.3.02lov.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corpus data</span>"
    ]
  },
  {
    "objectID": "Ch4_OpenScience.html",
    "href": "Ch4_OpenScience.html",
    "title": "\n4  Open Science statement\n",
    "section": "",
    "text": "Another important insight from the methodological part of the literature review (see Section 3.1 in book publication) is that, to the author’s best knowledge, no Textbook English study published so far has included (as an appendix or supplementary materials) the data and code necessary to reproduce or replicate the published results. As a result, it is very difficult to evaluate the reliability or robustness of the results reported (see also Le Foll 2024).\nThough the terms are sometimes used interchangeably and different (at times incompatible) definitions abound, in computational sciences, ‘reproducibility’ usually refers to the ability to obtain the same results as an original study using the researchers’ data and code, whilst ‘replicability’ refers to obtaining compatible results with the same method but different data (Association for Computing Machinery 2020; see also Berez-Kroeker et al. 2018).\nA major barrier to the reproducibility of (corpus) linguistic research is that it is often not possible for copyright or, when participants are involved, data protection reasons to make linguistic data available to the wider public. However, both research practice and the impact of our research can already be greatly improved if we publish our code or, when using GUI software, methods sections detailed enough to be able to successfully replicate the full procedures. This step can enable others to conduct detailed reviews of our methodologies and conceptual replications of our results on different data.\nAside from data protection and copyright regulations, there are, of course, many reasons why researchers may be reluctant to share their data and code (Berez-Kroeker et al. 2018; McManus 2021). It is not within the scope of this monograph to discuss these; however, it is clear that, in many ways, such transparency makes us vulnerable. At the end of the day: to err is human. Yet, the risks involved in committing to Open Science practices are particularly tangible for researchers working on individual projects, like myself, who have had no formal training in data management or programming and have therefore had to learn “on the job”. Nonetheless, I am convinced that the advantages outweigh the risks. Striving for transparency helps both the researchers themselves and others reviewing the work to spot and address problems. As a result, the research community can build on both the mishaps and successes of previous research, thus improving the efficiency of research processes and ultimately contributing to advancing scientific progress.\nIt is with this in mind that I have decided, whenever possible, to publish all the raw data and code necessary to reproduce the results reported in the present monograph following the FAIR principles (i.e., ensuring that research data are Findable, Accessible, Interoperable and Reusable, see Wilkinson et al. 2016). For copyright reasons, the corpora themselves and annotated corpus data in the form of concordance lines cannot be made available. However, the outcome of both manual and automatic annotation processes is published in tabular formats in the Online Appendix. These tables allow for the reproduction of all the analyses reported on in the following chapters using the reproducible data analysis scripts also published in the Online Supplements and in the associated Open Science Framework (OSF) repository.\nIn all chapters of this monograph, full transparency is strived for by reporting on how each sample size was determined and on which grounds data points were excluded, manipulated and/or transformed. Most of these operations were conducted in the open-source programming language and environment R (R Core Team 2022). Most of the data processing and analysis scripts therefore consist of R markdown documents. These were rendered to HTML pages (viewable in the Online Supplements) thus allowing researchers to review the procedures followed without necessarily installing all the required packages and running the code themselves. These scripts also feature additional analyses, tables and plots that were made as part of this study but which, for reasons of space, were not reported on in detail here. Whenever additional software or open-source code from other researchers were used, links to these are also provided in the Online Supplements (in addition to the corresponding references in the bibliography).\n\n\n\n\n\n\nAssociation for Computing Machinery, (ACM). 2020. “Artifact Review and Badging Version 1.1.” https://www.acm.org/publications/policies/artifact-review-and-badging-current.\n\n\nBerez-Kroeker, Andrea L., Lauren Gawne, Susan Smythe Kung, Barbara F. Kelly, Tyler Heston, Gary Holton, Peter Pulsifer, et al. 2018. “Reproducible Research in Linguistics: A Position Statement on Data Citation and Attribution in Our Field.” Linguistics 56 (1): 1–18. https://doi.org/10.1515/ling-2017-0032.\n\n\nLe Foll, Elen. 2024. “Why We Need Open Science and Open Education to Bridge the Corpus Researchpractice Gap.” In, edited by Peter Crosthwaite, 142–56. London: Routledge.\n\n\nMcManus, Kevin. 2021. “Are Replication Studies Infrequent Because of Negative Attitudes? Insights from a Survey of Attitudes and Practices in Second Language Research.” Studies in Second Language Acquisition, December, 1–14. https://doi.org/10.1017/S0272263121000838.\n\n\nR Core Team. 2022. “R: A Language and Environment for Statistical Computing.” Vienna, Austria. https://www.R-project.org/.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Open Science statement</span>"
    ]
  },
  {
    "objectID": "Ch6_data_prep.html",
    "href": "Ch6_data_prep.html",
    "title": "\n5  A Model of Intra-Textbook Linguistic Variation: Data Preparation\n",
    "section": "",
    "text": "5.1 Packages required\nThe following packages must be installed and loaded to process the data.\n#renv::restore() # Restore the project's dependencies from the lockfile to ensure that same package versions are used as in the original study\n\nlibrary(caret) # For its confusion matrix function\nlibrary(DT) # To display interactive HTML tables\nlibrary(here) # For dynamic file paths\nlibrary(knitr) # Loaded to display the tables using the kable() function\nlibrary(patchwork) # Needed to put together Fig. 1\nlibrary(PerformanceAnalytics) # For the correlation plot\nlibrary(psych) # For various useful, stats function\nlibrary(tidyverse) # For data wrangling",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Model of Intra-Textbook Linguistic Variation: Data Preparation</span>"
    ]
  },
  {
    "objectID": "Ch6_data_prep.html#data-import-from-mfte-output",
    "href": "Ch6_data_prep.html#data-import-from-mfte-output",
    "title": "\n5  A Model of Intra-Textbook Linguistic Variation: Data Preparation\n",
    "section": "\n5.2 Data import from MFTE output",
    "text": "5.2 Data import from MFTE output\nThe raw data used in this script is a tab-separated file that corresponds to the tabular output of mixed normalised frequencies as generated by the MFTE Perl v. 3.1 (Le Foll 2021a).\n\nCode# Read in Textbook Corpus data\nTxBcounts &lt;- read.delim(here(\"MFTE_data\", \"Outputs\", \"TxB900MDA_3.1_normed_complex_counts.tsv\"), header = TRUE, stringsAsFactors = TRUE)\nTxBcounts &lt;- TxBcounts |&gt; \n  filter(Filename!=\".DS_Store\") |&gt;  \n  droplevels()\n#str(TxBcounts) # Check sanity of data\n#nrow(TxBcounts) # Should be 2014 files\ndatatable(TxBcounts,\n  filter = \"top\",\n) |&gt; \n  formatRound(2:ncol(TxBcounts), digits=2)\n\n\n\n\n\nMetadata was added on the basis of the filenames.\n\n# Adding a textbook proficiency level\nTxBLevels &lt;- read.delim(here(\"metadata\", \"TxB900MDA_ProficiencyLevels.csv\"), sep = \",\")\nTxBcounts &lt;- full_join(TxBcounts, TxBLevels, by = \"Filename\") |&gt;  \n  mutate(Level = as.factor(Level)) |&gt;  \n  mutate(Filename = as.factor(Filename))\n\n# Check distribution and that there are no NAs\nsummary(TxBcounts$Level) |&gt; \n  kable(col.names = c(\"Textbook Level\", \"# of texts\"))\n\n\n\nTextbook Level\n# of texts\n\n\n\nA\n292\n\n\nB\n407\n\n\nC\n506\n\n\nD\n478\n\n\nE\n331\n\n\n\n\n# Check matching on random sample\n# TxBcounts |&gt;\n#   select(Filename, Level) |&gt;  \n#   sample_n(20) \n\n# Adding a register variable from the file names\nTxBcounts$Register &lt;- as.factor(stringr::str_extract(TxBcounts$Filename, \"Spoken|Narrative|Other|Personal|Informative|Instructional|Poetry\")) # Add a variable for Textbook Register\nsummary(TxBcounts$Register) |&gt; \n  kable(col.names = c(\"Textbook Register\", \"# of texts\"))\n\n\n\nTextbook Register\n# of texts\n\n\n\nInformative\n364\n\n\nInstructional\n647\n\n\nNarrative\n285\n\n\nPersonal\n88\n\n\nPoetry\n37\n\n\nSpoken\n593\n\n\n\n\nTxBcounts$Register &lt;- car::recode(TxBcounts$Register, \"'Narrative' = 'Fiction'; 'Spoken' = 'Conversation'\")\n#colnames(TxBcounts) # Check all the variables make sense\n\n# Adding a textbook series variable from the file names\nTxBcounts$Filename &lt;- stringr::str_replace(TxBcounts$Filename, \"English_In_Mind|English_in_Mind\", \"EIM\") \nTxBcounts$Filename &lt;- stringr::str_replace(TxBcounts$Filename, \"New_GreenLine\", \"NGL\") # Otherwise the regex for GreenLine will override New_GreenLine\nTxBcounts$Filename &lt;- stringr::str_replace(TxBcounts$Filename, \"Piece_of_cake\", \"POC\") # Shorten label for ease of plotting\nTxBcounts$Series &lt;- as.factor(stringr::str_extract(TxBcounts$Filename, \"Access|Achievers|EIM|GreenLine|HT|NB|NM|POC|JTT|NGL|Solutions\")) # Extract textbook series from (ammended) filenames\nsummary(TxBcounts$Series)  |&gt; \n  kable(col.names = c(\"Textbook Name\", \"# of texts\"))\n\n\n\nTextbook Name\n# of texts\n\n\n\nAccess\n315\n\n\nAchievers\n240\n\n\nEIM\n180\n\n\nGreenLine\n209\n\n\nHT\n115\n\n\nJTT\n129\n\n\nNB\n44\n\n\nNGL\n298\n\n\nNM\n59\n\n\nPOC\n98\n\n\nSolutions\n327\n\n\n\n\n# Including the French textbooks for the first year of Lycée to their corresponding publisher series from collège\nTxBcounts$Series &lt;-car::recode(TxBcounts$Series, \"c('NB', 'JTT') = 'JTT'; c('NM', 'HT') = 'HT'\") # # Recode final volumes of French series (see Section 4.3.1.1 on textbook selection for details)\nsummary(TxBcounts$Series) |&gt; \n  kable(col.names = c(\"Textbook Series\", \"# of texts\"))\n\n\n\nTextbook Series\n# of texts\n\n\n\nAccess\n315\n\n\nAchievers\n240\n\n\nEIM\n180\n\n\nGreenLine\n209\n\n\nHT\n174\n\n\nJTT\n173\n\n\nNGL\n298\n\n\nPOC\n98\n\n\nSolutions\n327\n\n\n\n\n# Adding a textbook country of use variable from the series variable\nTxBcounts$Country &lt;- TxBcounts$Series\nTxBcounts$Country &lt;- car::recode(TxBcounts$Series, \"c('Access', 'GreenLine', 'NGL') = 'Germany'; c('Achievers', 'EIM', 'Solutions') = 'Spain'; c('HT', 'NB', 'NM', 'POC', 'JTT') = 'France'\")\nsummary(TxBcounts$Country) |&gt; \n  kable(col.names = c(\"Country of Use\", \"# of texts\"))\n\n\n\nCountry of Use\n# of texts\n\n\n\nFrance\n445\n\n\nGermany\n822\n\n\nSpain\n747\n\n\n\n\n# Re-order variables\n#colnames(TxBcounts)\nTxBcounts &lt;- select(TxBcounts, order(names(TxBcounts))) %&gt;%\n  select(Filename, Country, Series, Level, Register, Words, everything())\n#colnames(TxBcounts)\n\n\n5.2.1 Corpus size\nThis table provides some summary statistics about the number of words included in the TEC texts originally tagged for this study.\n\nTxBcounts  |&gt;  \n  group_by(Register) |&gt;  \n  summarise(totaltexts = n(), totalwords = sum(Words), mean = as.integer(mean(Words)), sd = as.integer(sd(Words)), TTRmean = mean(TTR)) |&gt;  \n  kable(digits = 2, format.args = list(big.mark = \",\"))\n\n\n\nRegister\ntotaltexts\ntotalwords\nmean\nsd\nTTRmean\n\n\n\nConversation\n593\n505,147\n851\n301\n0.44\n\n\nFiction\n285\n241,512\n847\n208\n0.47\n\n\nInformative\n364\n304,695\n837\n177\n0.51\n\n\nInstructional\n647\n585,049\n904\n94\n0.42\n\n\nPersonal\n88\n69,570\n790\n177\n0.48\n\n\nPoetry\n37\n26,445\n714\n192\n0.44\n\n\n\n\n#TxBcounts &lt;- saveRDS(TxBcounts, here(\"processed_data\", \"TxBcounts.rds\"))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Model of Intra-Textbook Linguistic Variation: Data Preparation</span>"
    ]
  },
  {
    "objectID": "Ch6_data_prep.html#data-preparation-for-pca",
    "href": "Ch6_data_prep.html#data-preparation-for-pca",
    "title": "\n5  A Model of Intra-Textbook Linguistic Variation: Data Preparation\n",
    "section": "\n5.3 Data preparation for PCA",
    "text": "5.3 Data preparation for PCA\nPoetry texts were removed for this analysis as there were too few compared to the other register categories.\n\nsummary(TxBcounts$Register) |&gt;  \n  kable(col.names = c(\"Register\", \"# texts\"))\n\n\n\nRegister\n# texts\n\n\n\nConversation\n593\n\n\nFiction\n285\n\n\nInformative\n364\n\n\nInstructional\n647\n\n\nPersonal\n88\n\n\nPoetry\n37\n\n\n\n\n\nThis led to the following distribution of texts across the five textbook English registers examined in the model of intra-textbook linguistic variation:\n\nTxBcounts &lt;- TxBcounts |&gt;  \n  filter(Register!=\"Poetry\") |&gt;  \n  droplevels()\n\nsummary(TxBcounts$Register) |&gt;  \n  kable(col.names = c(\"Register\", \"# texts\"))\n\n\n\nRegister\n# texts\n\n\n\nConversation\n593\n\n\nFiction\n285\n\n\nInformative\n364\n\n\nInstructional\n647\n\n\nPersonal\n88\n\n\n\n\n\n\n5.3.1 Feature distributions\nThe distributions of each linguistic features were examined by means of visualisation. As shown below, before transformation, many of the features displayed highly skewed distributions.\n\nCodeTxBcounts |&gt; \n  select(-Words) |&gt;  \n  keep(is.numeric) |&gt;  \n  tidyr::gather() |&gt;  # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2\n  ggplot(aes(value)) +\n    theme_bw() +\n    facet_wrap(~ key, scales = \"free\", ncol = 4) +\n    scale_x_continuous(expand=c(0,0)) +\n    geom_histogram(bins = 30, colour= \"darkred\", fill = \"darkred\", alpha = 0.5)\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TEC-HistogramPlotsAllVariablesTEC-only.svg\"), width = 20, height = 45)\n\n\n\n5.3.2 Feature removal\nA number of features were removed from the dataset as they are not linguistically interpretable. In the case of the TEC, this included the variable CD because numbers spellt out as digits were removed from the textbooks before these were tagged with the MFTE. In addition, the variables LIKE and SO because these are “bin” features included in the output of the MFTE to ensure that the counts for these polysemous words do not inflate other categories due to mistags (Le Foll 2021b).\nWhenever linguistically meaningful, very low-frequency features were merged. Finally, features absent from more than third of texts were also excluded. For the analysis intra-textbook register variation, the following linguistic features were excluded from the analysis due to low dispersion:\n\n# Removal of meaningless features:\nTxBcounts &lt;- TxBcounts |&gt;  \n  select(-c(CD, LIKE, SO))\n\n# Function to compute percentage of texts with occurrences meeting a condition\ncompute_percentage &lt;- function(data, condition, threshold) {\n  numeric_data &lt;- Filter(is.numeric, data)\n  percentage &lt;- round(colSums(condition[, sapply(numeric_data, is.numeric)])/nrow(data) * 100, 2)\n  percentage &lt;- as.data.frame(percentage)\n  colnames(percentage) &lt;- \"Percentage\"\n  percentage &lt;- percentage |&gt;  \n    filter(!is.na(Percentage)) |&gt; \n    rownames_to_column() |&gt; \n    arrange(Percentage)\n  if (!missing(threshold)) {\n    percentage &lt;- percentage |&gt;  \n      filter(Percentage &gt; threshold)\n  }\n  return(percentage)\n}\n\n# Calculate percentage of texts with 0 occurrences of each feature\nzero_features &lt;- compute_percentage(TxBcounts, TxBcounts == 0, 66.6)\n#print(zero_features)\n\n# Combine low frequency features into meaningful groups whenever this makes linguistic sense\nTxBcounts &lt;- TxBcounts |&gt;  \n  mutate(JJPR = ABLE + JJPR, ABLE = NULL) |&gt;  \n  mutate(PASS = PGET + PASS, PGET = NULL)\n\n# Re-calculate percentage of texts with 0 occurrences of each feature\nzero_features2 &lt;- compute_percentage(TxBcounts, TxBcounts == 0, 66.6)\nprint(zero_features2)\n\n   rowname Percentage\n1      GTO      67.07\n2     ELAB      69.30\n3     MDMM      70.81\n4     HGOT      73.75\n5     CONC      80.48\n6     DWNT      81.44\n7    QUTAG      85.99\n8      URL      96.51\n9      EMO      97.82\n10     PRP      98.33\n11     HST      99.44\n\n# Drop variables with low document frequency\nTxBcounts &lt;- select(TxBcounts, -one_of(zero_features2$rowname))\n#ncol(TxBcounts)-8 # Number of linguistic features remaining\n\n# List of features\n#colnames(TxBcounts)\n\nThese feature removal operations resulted in a feature set of 64 linguistic variables.\n\n5.3.3 Identifying potential outlier texts\nAll normalised frequencies were normalised to identify any potential outlier texts.\n\n# First scale the normalised counts (z-standardisation) to be able to compare the various features\nTxBcounts |&gt; \n  select(-Words) |&gt;  \n  keep(is.numeric) |&gt;  \n  scale() -&gt;\n  TxBzcounts\n\nboxplot(TxBzcounts, las = 3, main = \"z-scores\") # Slow to open!\n\n\n\n\n\n\n# If necessary, remove any outliers at this stage.\nTxBdata &lt;- cbind(TxBcounts[,1:6], as.data.frame(TxBzcounts))\n\noutliers &lt;- TxBdata |&gt;  \n  select(-c(Words, LD, TTR)) |&gt;  \n  filter(if_any(where(is.numeric), ~ .x &gt; 8)) |&gt;  \n  select(Filename)\n\nThe following outlier texts were identified and excluded in subsequent analyses.\n\nCodeoutliers\n\n                                            Filename\n1                             POC_4e_Spoken_0007.txt\n2             Solutions_Elementary_Personal_0001.txt\n3                       NGL_5_Instructional_0018.txt\n4                           Access_1_Spoken_0011.txt\n5                              EIM_1_Spoken_0012.txt\n6                              NGL_4_Spoken_0011.txt\n7      Solutions_Intermediate_Plus_Personal_0001.txt\n8           Solutions_Elementary_ELF_Spoken_0021.txt\n9                          NB_2_Informative_0009.txt\n10       Solutions_Intermediate_Plus_Spoken_0022.txt\n11     Solutions_Intermediate_Instructional_0025.txt\n12 Solutions_Pre-Intermediate_Instructional_0024.txt\n13                            POC_4e_Spoken_0010.txt\n14            Solutions_Intermediate_Spoken_0019.txt\n15                          Access_1_Spoken_0019.txt\n16    Solutions_Pre-Intermediate_ELF_Spoken_0005.txt\n\nCodeTxBcounts &lt;- TxBcounts |&gt;  \n  filter(!Filename %in% outliers$Filename)\n\nTxBcounts |&gt; \n  select(-Words) |&gt;  \n  keep(is.numeric) |&gt;  \n  scale() -&gt;\n  TxBzcounts\n\n\nThis resulted in 1,961 TEC texts being included in the model of intra-textbook linguistic variation with the following normalised feature distributions.\n\nCodeTxBzcounts |&gt; \n  as.data.frame() |&gt;  \n  gather() |&gt;  # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2\n  ggplot(aes(value)) +\n    theme_bw() +\n    facet_wrap(~ key, scales = \"free\", ncol = 4) +\n    scale_x_continuous(expand=c(0,0)) +\n    geom_histogram(bins = 30, colour= \"darkred\", fill = \"darkred\", alpha = 0.5)\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TEC-zscores-HistogramsAllVariablesTEC-only.svg\"), width = 20, height = 45)\n\n\n\n5.3.4 Signed log transformation\nA signed logarithmic transformation was applied to (further) deskew the feature distributions (Diwersy, Evert, and Neumann 2014; Neumann and Evert 2021).\nThe signed log transformation function was inspired by the SignedLog function proposed in https://cran.r-project.org/web/packages/DataVisualizations/DataVisualizations.pdf\n\n# All features are signed log-transformed (note that this is also what Neumann & Evert 2021 propose)\nsigned.log &lt;- function(x) {\n  sign(x) * log(abs(x) + 1)\n  }\n\nTxBzlogcounts &lt;- signed.log(TxBzcounts) # Standardise first, then signed log transform\n\n#saveRDS(TxBzlogcounts, here(\"processed_data\", \"TxBzlogcounts.rds\")) # Last saved 16 Feb 2024\n\nThe new feature distributions are visualised below.\n\nCodeTxBzlogcounts |&gt; \n  as.data.frame() |&gt;  \n  gather() |&gt;  # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2\n  ggplot(aes(value, after_stat(density))) +\n  theme_bw() +\n  facet_wrap(~ key, scales = \"free\", ncol = 4) +\n  scale_x_continuous(expand=c(0,0)) +\n  scale_y_continuous(limits = c(0,NA)) +\n  geom_histogram(bins = 30, colour= \"black\", fill = \"grey\") +\n  geom_density(colour = \"darkred\", weight = 2, fill=\"darkred\", alpha = .4)\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"DensityPlotsAllVariablesSignedLog-TEC-only.svg\"), width = 15, height = 49)\n\n\nThe following correlation plots serve to illustrate the effect of the variable transformations performed in the above chunks.\nExample feature distributions before transformations:\n\nCode# This is a slightly amended version of the PerformanceAnalytics::chart.Correlation() function. It simply removes the significance stars that are meaningless with this many data points (see commented out lines below)\n\nchart.Correlation.nostars &lt;- function (R, histogram = TRUE, method = c(\"pearson\", \"kendall\", \"spearman\"), ...) {\n  x = checkData(R, method = \"matrix\")\n  if (missing(method)) \n    method = method[1]\n  panel.cor &lt;- function(x, y, digits = 2, prefix = \"\", use = \"pairwise.complete.obs\", method = \"pearson\", cex.cor, ...) {\n    usr &lt;- par(\"usr\")\n    on.exit(par(usr))\n    par(usr = c(0, 1, 0, 1))\n    r &lt;- cor(x, y, use = use, method = method)\n    txt &lt;- format(c(r, 0.123456789), digits = digits)[1]\n    txt &lt;- paste(prefix, txt, sep = \"\")\n    if (missing(cex.cor)) \n      cex &lt;- 0.8/strwidth(txt)\n    test &lt;- cor.test(as.numeric(x), as.numeric(y), method = method)\n    # Signif &lt;- symnum(test$p.value, corr = FALSE, na = FALSE, \n    #                  cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1), symbols = c(\"***\", \n    #                                                                           \"**\", \"*\", \".\", \" \"))\n    text(0.5, 0.5, txt, cex = cex * (abs(r) + 0.3)/1.3)\n    # text(0.8, 0.8, Signif, cex = cex, col = 2)\n  }\n  f &lt;- function(t) {\n    dnorm(t, mean = mean(x), sd = sd.xts(x))\n  }\n  dotargs &lt;- list(...)\n  dotargs$method &lt;- NULL\n  rm(method)\n  hist.panel = function(x, ... = NULL) {\n    par(new = TRUE)\n    hist(x, col = \"light gray\", probability = TRUE, \n         axes = FALSE, main = \"\", breaks = \"FD\")\n    lines(density(x, na.rm = TRUE), col = \"red\", lwd = 1)\n    rug(x)\n  }\n  if (histogram) \n    pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor, \n          diag.panel = hist.panel)\n  else pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor)\n}\n\n# Example plot without any variable transformation\nexample1 &lt;- TxBcounts |&gt; \n  select(NN,PROG,SPLIT,ACT,FPP1S)\n\n#png(here(\"plots\", \"CorrChart-TEC-examples-normedcounts.png\"), width = 20, height = 20, units = \"cm\", res = 300)\nchart.Correlation.nostars(example1, histogram=TRUE, pch=19)\n\n\n\n\n\n\nCode#dev.off()\n\n\nExample feature distributions after transformations:\n\nCode# Example plot with transformed variables\nexample2 &lt;- TxBzlogcounts |&gt; \n  as.data.frame() |&gt;  \n  select(NN,PROG,SPLIT,ACT,FPP1S)\n\n#png(here(\"plots\", \"CorrChart-TEC-examples-zsignedlogcounts.png\"), width = 20, height = 20, units = \"cm\", res = 300)\nchart.Correlation.nostars(example2, histogram=TRUE, pch=19)\n\n\n\n\n\n\nCode#dev.off()\n\n\n\n5.3.5 Feature correlations\nThe correlations of the transformed feature frequencies can be visualised in the form of a heatmap. Negative correlations are rendered in blue, whereas positive ones are in red.\n\nCode# Simple heatmap in base R (inspired by Stephanie Evert's SIGIL code)\ncor.colours &lt;- c(\n  hsv(h=2/3, v=1, s=(10:1)/10), # blue = negative correlation \n  rgb(1,1,1), # white = no correlation \n  hsv(h=0, v=1, s=(1:10/10))) # red = positive correlation\n\n#png(here(\"plots\", \"heatmapzlogcounts-TEC-only.png\"), width = 30, height= 30, units = \"cm\", res = 300)\nheatmap(cor(TxBzlogcounts), \n        symm=TRUE, \n        zlim=c(-1,1), \n        col=cor.colours, \n        margins=c(0,0))\n\n\n\n\n\n\nCode#dev.off()\n\n# Calculate the sum of all the words in the tagged texts of the TEC\ntotalwords &lt;- TxBcounts |&gt;  \n  select(Words) |&gt; \n  sum() |&gt; \n  format(big.mark=\",\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Model of Intra-Textbook Linguistic Variation: Data Preparation</span>"
    ]
  },
  {
    "objectID": "Ch6_data_prep.html#composition-of-tec-textsfiles",
    "href": "Ch6_data_prep.html#composition-of-tec-textsfiles",
    "title": "\n5  A Model of Intra-Textbook Linguistic Variation: Data Preparation\n",
    "section": "\n5.4 Composition of TEC texts/files",
    "text": "5.4 Composition of TEC texts/files\nThese figures and tables provide summary statistics on the texts/files of the TEC that were entered in the multi-dimensional model of intra-textbook linguistic variation. In total, the TEC texts entered amounted to 1,693,650 words.\n\nCodemetadata &lt;- TxBcounts |&gt;  \n  select(Filename, Country, Series, Level, Register, Words) |&gt;  \n  mutate(Volume = paste(Series, Level)) |&gt;  \n  mutate(Volume = fct_rev(Volume)) |&gt;  \n  mutate(Volume = fct_reorder(Volume, as.numeric(Level))) |&gt;  \n  group_by(Volume) |&gt;  \n  mutate(wordcount = sum(Words)) |&gt;  \n  ungroup() |&gt;  \n  distinct(Volume, .keep_all = TRUE)\n\n# Plot for book\nmetadata2 &lt;- TxBcounts |&gt;  \n  select(Country, Series, Level, Register, Words) |&gt;  \n  mutate(Volume = paste(Series, Level)) |&gt;  \n  mutate(Volume = fct_rev(Volume)) |&gt;  \n  #mutate(Volume = fct_reorder(Volume, as.numeric(Level))) |&gt;  \n  group_by(Volume, Register) |&gt;  \n  mutate(wordcount = sum(Words)) |&gt;  \n  ungroup() |&gt;  \n  distinct(Volume, Register, .keep_all = TRUE)\n\n# This is the palette created above on the basis of the suffrager pakcage (but without needed to install the package)\npalette &lt;- c(\"#BD241E\", \"#A18A33\", \"#15274D\", \"#D54E1E\", \"#EA7E1E\", \"#4C4C4C\", \"#722672\", \"#F9B921\", \"#267226\")\n\nPlotSp &lt;- metadata2 |&gt;  \n  filter(Country==\"Spain\") |&gt;  \n  #arrange(Volume) |&gt;  \n  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + \n    geom_bar(stat = \"identity\", position = \"stack\") +\n    coord_flip(expand = FALSE) + # Removes those annoying ticks before each bar label\n    theme_minimal() + theme(legend.position = \"none\") +\n    labs(x = \"Spain\", y = \"Cumulative word count\") +\n    scale_fill_manual(values = palette[c(5,4,3,2,1)], \n                      guide = guide_legend(reverse = TRUE))\n\nPlotGer &lt;- metadata2 |&gt;  \n  filter(Country==\"Germany\") |&gt;  \n  #arrange(Volume) |&gt;  \n  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + \n    geom_bar(stat = \"identity\", position = \"stack\") +\n    coord_flip(expand = FALSE) +\n    labs(x = \"Germany\", y = \"\") +\n    scale_fill_manual(values = palette[c(5,4,3,2,1)], guide = guide_legend(reverse = TRUE)) +\n    theme_minimal() + theme(legend.position = \"none\")\n\nPlotFr &lt;- metadata2 |&gt;  \n  filter(Country==\"France\") |&gt;  \n  #arrange(Volume) |&gt;  \n  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + \n    geom_bar(stat = \"identity\", position = \"stack\") +\n    coord_flip(expand = FALSE) +\n    labs(x = \"France\", y  = \"\", fill = \"Register subcorpus\") +\n    scale_fill_manual(values = palette[c(5,4,3,2,1)], guide = guide_legend(reverse = TRUE, legend.hjust = 0)) +\n    theme_minimal() + theme(legend.position = \"top\", legend.justification = \"left\")\n\nPlotFr /\nPlotGer /\nPlotSp\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TEC-T_wordcounts_book.svg\"), width = 8, height = 12)\n\n\nThe following table provides information about the proportion of instructional language featured in each textbook series.\n\nCodemetadataInstr &lt;- TxBcounts |&gt;  \n  select(Country, Series, Level, Register, Words) |&gt;  \n  filter(Register==\"Instructional\") |&gt;  \n  mutate(Volume = paste(Series, Register)) |&gt;  \n  mutate(Volume = fct_rev(Volume)) |&gt;  \n  mutate(Volume = fct_reorder(Volume, as.numeric(Level))) |&gt;  \n  group_by(Volume, Register) |&gt;  \n  mutate(InstrWordcount = sum(Words)) |&gt;  \n  ungroup() |&gt;  \n  distinct(Volume, .keep_all = TRUE) |&gt;  \n  select(Series, InstrWordcount)\n\nmetaWordcount &lt;- TxBcounts |&gt;  \n  select(Country, Series, Level, Register, Words) |&gt;  \n  group_by(Series) |&gt;  \n  mutate(TECwordcount = sum(Words)) |&gt;  \n  ungroup() |&gt;  \n  distinct(Series, .keep_all = TRUE) |&gt;  \n  select(Series, TECwordcount)\n\nwordcount &lt;- merge(metaWordcount, metadataInstr, by = \"Series\")\n\nwordcount |&gt;  \n  mutate(InstrucPercent = InstrWordcount/TECwordcount*100) |&gt;  \n  arrange(InstrucPercent) |&gt;  \n  mutate(InstrucPercent = round(InstrucPercent, 2)) |&gt;  \n  kable(col.names = c(\"Textbook Series\", \"Total words\", \"Instructional words\", \"% of textbook content\"), \n        digits = 2, \n        format.args = list(big.mark = \",\"))\n\n\n\n\n\n\n\n\n\nTextbook Series\nTotal words\nInstructional words\n% of textbook content\n\n\n\nAccess\n259,679\n60,938\n23.47\n\n\nNGL\n278,316\n79,312\n28.50\n\n\nGreenLine\n172,267\n54,263\n31.50\n\n\nSolutions\n270,278\n87,829\n32.50\n\n\nJTT\n137,557\n48,375\n35.17\n\n\nHT\n142,676\n51,550\n36.13\n\n\nPOC\n76,714\n30,548\n39.82\n\n\nEIM\n147,185\n59,928\n40.72\n\n\nAchievers\n208,978\n109,886\n52.58\n\n\n\n\n\n\n\n\n\n\n\nDiwersy, Sascha, Stephanie Evert, and Stella Neumann. 2014. “A Weakly Supervised Multivariate Approach to the Study of Language Variation.” In, edited by Benedikt Szmrecsanyi and Bernhard Wälchli, 174–204. Berlin: De Gruyter.\n\n\nLe Foll, Elen. 2021a. Introducing the Multi-Feature Tagger of English (MFTE). Osnabrück University. https://github.com/elenlefoll/MultiFeatureTaggerEnglish.\n\n\n———. 2021b. Introducing the Multi-Feature Tagger of English (MFTE). Osnabrück University. https://github.com/elenlefoll/MultiFeatureTaggerEnglish.\n\n\nNeumann, Stella, and Stephanie Evert. 2021. “A Register Variation Perspective on Varieties of English.” In, edited by Elena Seoane and Douglas Biber, 144178. Studies in Corpus Linguistics 103. Amsterdam: Benjamins.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Model of Intra-Textbook Linguistic Variation: Data Preparation</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Association for Computing Machinery, (ACM). 2020. “Artifact Review\nand Badging Version 1.1.” https://www.acm.org/publications/policies/artifact-review-and-badging-current.\n\n\nBerez-Kroeker, Andrea L., Lauren Gawne, Susan Smythe Kung, Barbara F.\nKelly, Tyler Heston, Gary Holton, Peter Pulsifer, et al. 2018.\n“Reproducible Research in Linguistics: A Position Statement on\nData Citation and Attribution in Our Field.” Linguistics\n56 (1): 1–18. https://doi.org/10.1515/ling-2017-0032.\n\n\nDiwersy, Sascha, Stephanie Evert, and Stella Neumann. 2014. “A\nWeakly Supervised Multivariate Approach to the Study of Language\nVariation.” In, edited by Benedikt Szmrecsanyi and Bernhard\nWälchli, 174–204. Berlin: De Gruyter.\n\n\nLe Foll, Elen. 2021a. Introducing the Multi-Feature Tagger of\nEnglish (MFTE). Osnabrück University. https://github.com/elenlefoll/MultiFeatureTaggerEnglish.\n\n\n———. 2021b. Introducing the Multi-Feature Tagger of English\n(MFTE). Osnabrück University. https://github.com/elenlefoll/MultiFeatureTaggerEnglish.\n\n\n———. 2024. “Why We Need Open Science and Open Education to Bridge\nthe Corpus Researchpractice Gap.” In, edited by\nPeter Crosthwaite, 142–56. London: Routledge.\n\n\nLove, Robbie, Vaclav Brezina, Tony McEnery, Abi Hawtin, Andrew Hardie,\nand Claire Dembry. 2019. “Functional Variation in the Spoken\nBNC2014 and the Potential for Register Analysis.” Register\nStudies 1 (2): 296–317. https://doi.org/10.1075/rs.18013.lov.\n\n\nLove, Robbie, Claire Dembry, Andrew Hardie, Vaclav Brezina, and Tony\nMcEnery. 2017. “The Spoken BNC2014.” International\nJournal of Corpus Linguistics 22 (3): 319–44. https://doi.org/https://doi.org/10.1075/ijcl.22.3.02lov.\n\n\nMcManus, Kevin. 2021. “Are Replication Studies Infrequent Because\nof Negative Attitudes? Insights from a Survey of Attitudes and Practices\nin Second Language Research.” Studies in Second Language\nAcquisition, December, 1–14. https://doi.org/10.1017/S0272263121000838.\n\n\nNeumann, Stella, and Stephanie Evert. 2021. “A Register Variation\nPerspective on Varieties of English.” In, edited by Elena Seoane\nand Douglas Biber, 144178. Studies in Corpus Linguistics 103. Amsterdam:\nBenjamins.\n\n\nR Core Team. 2022. “R: A Language and Environment for Statistical\nComputing.” Vienna, Austria. https://www.R-project.org/.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg,\nGabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al.\n2016. “The FAIR Guiding Principles for Scientific Data Management\nand Stewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18.",
    "crumbs": [
      "References"
    ]
  }
]