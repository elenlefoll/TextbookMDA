[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Textbook English: A Multi-Dimensional Approach",
    "section": "",
    "text": "About\n\n\n\n\n\n\nWarning\n\n\n\nThis Quarto book is work in progress. It will eventually contain the online supplements to:\n\nLe Foll, Elen. to appear. Textbook English: A Multi-Dimensional Approach [Studies in Corpus Linguistics]. Amsterdam: John Benjamins.\n\n\n\nThe book is based on my PhD thesis, which is accessible in Open Access:\n\nLe Foll, Elen. 2022. Textbook English: A Corpus-Based Analysis of the Language of EFL textbooks used in Secondary Schools in France, Germany and Spain. Osnabrück, Germany: Osnabrück University. PhD thesis. https://doi.org/10.48693/278.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "\n1  Introduction\n",
    "section": "",
    "text": "1.1 Research objectives and methodological approach\nThe above questions are critical because, as many adults’ lingering memories of school foreign language lessons testify (see also, e.g., Freudenstein 2002: 55), textbooks play an absolutely central role in classroom-based foreign language learning. In the following, we will see that the dominance of textbooks in EFL school contexts persists to this day. According to Thornbury (2012 in a response to Chong 2012: n.p.), they “(more often [than] not) instantiate the curriculum, provide the texts, and - to a large extent - guide the methodology”. In lower secondary EFL instructional contexts, in particular, textbooks constitute a major vector of foreign language input. Yet, numerous studies have shown that “considerable mismatches between naturally occurring English and the English that is put forward as a model in pedagogical descriptions” (Römer 2006: 125-26) exist. These mismatches have been observed and sometimes extensively described in textbooks’ representations of numerous language features ranging from the use of individual words and phraseological patterns (e.g., Conrad 2004 on the preposition though; Gouverneur 2008 on the high-frequency verbs make and take), to tenses and aspects (e.g., Barbieri & Eckhardt 2007 on reported speech; Römer 2005 on the progressive). More rarely, textbook language studies have also ventured into the study of spoken grammar (e.g., Gilmore 2004) and pragmatics (e.g., Hyland 1994 on hedging in ESP/EAP textbooks).\nHowever, as we will see in Chapter 2, previous EFL textbook studies have tended to focus on one or at most a handful of individual linguistic features. Taken together, they provide valuable insights into “the kind of synthetic English” (Römer 2004b: 185) that pupils are exposed to via their textbooks; yet, what is missing is a more comprehensive, broader understanding of what constitutes ‘Textbook English’ from a linguistic point of view. Although corpus-based2 textbook analysis can be traced back to the pioneering work of Dieter Mindt in the 1980s, the language of secondary school EFL textbooks (as opposed to that of general adult EFL or English for Specific Purposes [ESP] coursebooks) remains an understudied area.\nThe present study therefore sets out to describe the linguistic content of secondary school EFL textbooks and to survey the similarities and most striking differences between ‘Textbook English’ and ‘naturally occurring English’ as used outside the EFL classroom, with respect to a wide range of lexico-grammatical features.\nTo this end, a corpus of nine series of secondary school EFL textbooks (43 textbook volumes) used at lower secondary level in France, Germany, and Spain was compiled (see 4.3.1). In addition, three reference corpora are used as baselines for comparisons between the language input EFL learners are confronted with via their school textbooks and the kind of naturally occurring English that they can be expected to encounter, engage with, and produce themselves on leaving school. Two of these have been built specifically for this project with the aim of representing comparable ‘authentic’ (for a discussion of this controversial term in ELT, see 2.2) and age-appropriate learner target language.\nA bottom-up, corpus-based approach is adopted (e.g., Mindt 1992, 1995a; Biber & Quirk 2012; Biber & Gray 2015; Ronald Carter & McCarthy 2006a). A broad range of linguistic features are considered: ranging from tenses and aspects to negation and discourse markers. We will pay particular attention to the lexico-grammatical aspects of Textbook English that substantially diverge from the target learner language reference corpora and examine these with direct comparisons of textbook excerpts with comparable texts from the reference data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#outline-of-the-book",
    "href": "intro.html#outline-of-the-book",
    "title": "\n1  Introduction\n",
    "section": "\n1.2 Outline of the book",
    "text": "1.2 Outline of the book\nThe following chapter outlines the background to and motivation behind the present study. Chapter 3 then provides a literature review of state-of-the-art research on the language of school EFL textbooks. It is divided in two parts. Part 1 is a methodological review in which the various methods employed so far to analyse, describe, and evaluate Textbook English are explained and illustrated with selected studies. Part 2 summarises the results of existing studies on various aspects of Textbook English, including lexical, grammatical and pragmatic aspects. Based on the methodological limitations and the gaps identified in the existing literature, Chapter 4 elaborates the specific research questions addressed in the present study. These research questions informed the decision-making processes involved in the compilation of the Textbook English Corpus (TEC) and the selection/compilation of three reference corpora designed to represent learners’ target language. These processes and their motivations are explained in the remaining sections of Chapter 4.\nChapter 5 describes the multivariable statistical methods applied to describe the linguistic nature of Textbook English on multiple dimensions of linguistic variation. It begins by explaining the well-established multi-feature/dimensional analysis (MDA) method pioneered by Biber (1988, 1995; see also Berber Sardinha & Veirano Pinto 2014, 2019), before outlining the reasoning for the modified MDA framework applied in the present study. Chapter 6 presents the results of an MDA model of Textbook English which highlights the sources of linguistic variation within EFL textbooks across several dimensions of intra-textbook linguistic variation. Chapter 7 presents the results of a second MDA model that shows how Textbook English is both, in some respects, similar to and, in others, different from the kind of English that EFL learners are likely to encounter outside the classroom.\nChapter 8 explains how the two models contribute to a new understanding of the linguistic characteristics of Textbook English. This, in turn, has implications for teachers, textbook authors, editors, publishers, and policy-makers. These implications are discussed in Chapter 9. It first considers the potential impact of the substantial gaps between Textbook English and the target reference corpora before making suggestions as to how teachers, textbook authors, and editors may want to improve or supplement unnatural‑sounding pedagogical texts using corpora and corpus tools. Chapter 10 focuses on the study’s methodological strengths and limitations. It explains how the modified MDA framework presented and applied in this study may be of interest to corpus linguists working on a broad range of research questions. Chapter 11 concludes with a synthesis of the most important take-aways from the study. It also points to promising future research avenues.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "\n1  Introduction\n",
    "section": "",
    "text": "Dialogue from Speak English 6e série verte (Benhamou & Dominique 1977: 167). It was made popular by stand-up comedian Gad Elmaleh. More information on the context of this textbook dialogue can be found here. An extract of the comedy sketch by Gad Elmaleh that popularised the dialogue can be viewed here with English subtitles: https://youtu.be/11jG7lkwDwU?t=50.↩︎\nHere the adjectives ‘corpus-based’ and ‘corpus-driven’ are used synonymously (see, e.g., Meunier & Reppen 2015: 499 for further information as to how these terms are sometimes distinguished).↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "OpenScience.html",
    "href": "OpenScience.html",
    "title": "\n2  Open Science statement\n",
    "section": "",
    "text": "Another important insight from the methodological part of the literature review (see Section 3.1 in book publication) is that, to the author’s best knowledge, no Textbook English study published so far has included (as an appendix or supplementary materials) the data and code necessary to reproduce or replicate the published results. As a result, it is very difficult to evaluate the reliability or robustness of the results reported (see also Le Foll 2024).\nThough the terms are sometimes used interchangeably and different (at times incompatible) definitions abound, in computational sciences, ‘reproducibility’ usually refers to the ability to obtain the same results as an original study using the researchers’ data and code, whilst ‘replicability’ refers to obtaining compatible results with the same method but different data (Association for Computing Machinery 2020; see also Berez-Kroeker et al. 2018).\nA major barrier to the reproducibility of (corpus) linguistic research is that it is often not possible for copyright or, when participants are involved, data protection reasons to make linguistic data available to the wider public. However, both research practice and the impact of our research can already be greatly improved if we publish our code or, when using GUI software, methods sections detailed enough to be able to successfully replicate the full procedures. This step can enable others to conduct detailed reviews of our methodologies and conceptual replications of our results on different data.\nAside from data protection and copyright regulations, there are, of course, many reasons why researchers may be reluctant to share their data and code (Berez-Kroeker et al. 2018; McManus 2021). It is not within the scope of this monograph to discuss these; however, it is clear that, in many ways, such transparency makes us vulnerable. At the end of the day: to err is human. Yet, the risks involved in committing to Open Science practices are particularly tangible for researchers working on individual projects, like myself, who have had no formal training in data management or programming and have therefore had to learn “on the job”. Nonetheless, I am convinced that the advantages outweigh the risks. Striving for transparency helps both the researchers themselves and others reviewing the work to spot and address problems. As a result, the research community can build on both the mishaps and successes of previous research, thus improving the efficiency of research processes and ultimately contributing to advancing scientific progress.\nIt is with this in mind that I have decided, whenever possible, to publish all the raw data and code necessary to reproduce the results reported in the present monograph following the FAIR principles (i.e., ensuring that research data are Findable, Accessible, Interoperable and Reusable, see Wilkinson et al. 2016). For copyright reasons, the corpora themselves and annotated corpus data in the form of concordance lines cannot be made available. However, the outcome of both manual and automatic annotation processes is published in tabular formats in the Online Appendix. These tables allow for the reproduction of all the analyses reported on in the following chapters using the reproducible data analysis scripts also published in the Online Supplements and in the associated Open Science Framework (OSF) repository.\nIn all chapters of this monograph, full transparency is strived for by reporting on how each sample size was determined and on which grounds data points were excluded, manipulated and/or transformed. Most of these operations were conducted in the open-source programming language and environment R (R Core Team 2022). Most of the data processing and analysis scripts therefore consist of R markdown documents. These were rendered to HTML pages (viewable in the Online Supplements) thus allowing researchers to review the procedures followed without necessarily installing all the required packages and running the code themselves. These scripts also feature additional analyses, tables and plots that were made as part of this study but which, for reasons of space, were not reported on in detail here. Whenever additional software or open-source code from other researchers were used, links to these are also provided in the Online Supplements (in addition to the corresponding references in the bibliography).\n\n\n\n\n\n\nAssociation for Computing Machinery, (ACM). 2020. “Artifact Review and Badging Version 1.1.” https://www.acm.org/publications/policies/artifact-review-and-badging-current.\n\n\nBerez-Kroeker, Andrea L., Lauren Gawne, Susan Smythe Kung, Barbara F. Kelly, Tyler Heston, Gary Holton, Peter Pulsifer, et al. 2018. “Reproducible Research in Linguistics: A Position Statement on Data Citation and Attribution in Our Field.” Linguistics 56 (1): 1–18. https://doi.org/10.1515/ling-2017-0032.\n\n\nLe Foll, Elen. 2024. “Why We Need Open Science and Open Education to Bridge the Corpus Researchpractice Gap.” In, edited by Peter Crosthwaite, 142–56. London: Routledge.\n\n\nMcManus, Kevin. 2021. “Are Replication Studies Infrequent Because of Negative Attitudes? Insights from a Survey of Attitudes and Practices in Second Language Research.” Studies in Second Language Acquisition, December, 1–14. https://doi.org/10.1017/S0272263121000838.\n\n\nR Core Team. 2022. “R: A Language and Environment for Statistical Computing.” Vienna, Austria. https://www.R-project.org/.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Open Science statement</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "Warning\n\n\n\nThis website is a work in progress.\n\n\nThis page will soon feature a summary of the book.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Association for Computing Machinery, (ACM). 2020. “Artifact Review\nand Badging Version 1.1.” https://www.acm.org/publications/policies/artifact-review-and-badging-current.\n\n\nBerez-Kroeker, Andrea L., Lauren Gawne, Susan Smythe Kung, Barbara F.\nKelly, Tyler Heston, Gary Holton, Peter Pulsifer, et al. 2018.\n“Reproducible Research in Linguistics: A Position Statement on\nData Citation and Attribution in Our Field.” Linguistics\n56 (1): 1–18. https://doi.org/10.1515/ling-2017-0032.\n\n\nDiwersy, Sascha, Stephanie Evert, and Stella Neumann. 2014. “A\nWeakly Supervised Multivariate Approach to the Study of Language\nVariation.” In, edited by Benedikt Szmrecsanyi and Bernhard\nWälchli, 174–204. Berlin: De Gruyter.\n\n\nLe Foll, Elen. 2021a. Introducing the Multi-Feature Tagger of\nEnglish (MFTE). Osnabrück University. https://github.com/elenlefoll/MultiFeatureTaggerEnglish.\n\n\n———. 2021b. Introducing the Multi-Feature Tagger of English\n(MFTE). Osnabrück University. https://github.com/elenlefoll/MultiFeatureTaggerEnglish.\n\n\n———. 2024. “Why We Need Open Science and Open Education to Bridge\nthe Corpus Researchpractice Gap.” In, edited by\nPeter Crosthwaite, 142–56. London: Routledge.\n\n\n———. n.d. “Schulenglisch: A\nMulti-Dimensional Model of the Variety of English Taught in German\nSecondary Schools.” AAA: Arbeiten Aus Anglistik Und\nAmerikanistik 49.\n\n\nLe Foll, Elen, and Muhammad Shakir. 2023. “Introducing a New\nOpen-Source Corpus-Linguistic Tool: The Multi-Feature Tagger of English\n(MFTE),” May.\n\n\nLove, Robbie, Vaclav Brezina, Tony McEnery, Abi Hawtin, Andrew Hardie,\nand Claire Dembry. 2019. “Functional Variation in the Spoken\nBNC2014 and the Potential for Register Analysis.” Register\nStudies 1 (2): 296–317. https://doi.org/10.1075/rs.18013.lov.\n\n\nLove, Robbie, Claire Dembry, Andrew Hardie, Vaclav Brezina, and Tony\nMcEnery. 2017. “The Spoken BNC2014.” International\nJournal of Corpus Linguistics 22 (3): 319–44. https://doi.org/https://doi.org/10.1075/ijcl.22.3.02lov.\n\n\nMcManus, Kevin. 2021. “Are Replication Studies Infrequent Because\nof Negative Attitudes? Insights from a Survey of Attitudes and Practices\nin Second Language Research.” Studies in Second Language\nAcquisition, December, 1–14. https://doi.org/10.1017/S0272263121000838.\n\n\nNeumann, Stella, and Stephanie Evert. 2021. “A Register Variation\nPerspective on Varieties of English.” In, edited by Elena Seoane\nand Douglas Biber, 144178. Studies in Corpus Linguistics 103. Amsterdam:\nBenjamins.\n\n\nR Core Team. 2022. “R: A Language and Environment for Statistical\nComputing.” Vienna, Austria. https://www.R-project.org/.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg,\nGabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al.\n2016. “The FAIR Guiding Principles for Scientific Data Management\nand Stewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "AppendixA.html",
    "href": "AppendixA.html",
    "title": "Appendix A — Literature Review Data",
    "section": "",
    "text": "This table provides a tabular overview of the studies surveyed as part of this study’s literature review on textbook research examining describing and/or evaluating the language of English textbooks designed for English L2 learners in various instructional settings.\nIt presents the results of this non-exhaustive survey of Textbook English studies published over the past four decades, summarising some of the key information on each study, including its main language focus, methodological approach, information on the textbooks investigated, and, if applicable, on any reference corpora used. Empty cells represent fields for which no information was published. The table is fully searchable and filterable. You can adjust the widths of the individual columns to best fit your screen size.\nThis list is intended to be a dynamic resource that will grow over time. If you would like to contribute any studies to the table, please either fork the corresponding CSV file in the repository or send me an e-mail with the corresponding details of the studies that you would like to add.\nThis page was last updated on 15 March 2024.\n\n\n\n\n\n\nThe raw data can be downloaded as a comma-separated file from the project GitHub repository.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Literature Review Data</span>"
    ]
  },
  {
    "objectID": "AppendixB.html",
    "href": "AppendixB.html",
    "title": "Appendix B — Corpus Data",
    "section": "",
    "text": "B.1 Textbook English Corpus (TEC)\nA detailed tabular overview of the composition of the Textbook English Corpus (TEC) together with the full bibliographic metadata is available at doi.org/10.5281/zenodo.4922819.\nNote that, for copyright reasons, the corpus itself cannot be published. If you are interested in using the corpus for non-commercial research purposes and/or in a potential research collaboration, please get in touch with me via e-mail.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Corpus Data</span>"
    ]
  },
  {
    "objectID": "AppendixB.html#reference-corpora",
    "href": "AppendixB.html#reference-corpora",
    "title": "Appendix B — Corpus Data",
    "section": "\nB.2 Reference corpora",
    "text": "B.2 Reference corpora\n\nB.2.1 Spoken BNC2014\nThe original corpus files of the Spoken British National Corpus (BNC) 2014 (Love et al. 2017; Love et al. 2019) can be downloaded for free for research purposes from: http://corpora.lancs.ac.uk/bnc2014/signup.php. I used the untagged XML version.\nThe R script used to pre-process the untagged XML files into the format used in this study (the “John and Jill in Ivybridge” version with added full stops at speaker turns, as explained in Section 4.3.2.2 of the book) can be found here: https://github.com/elenlefoll/TextbookEnglish/blob/main/3_Data/BNCspoken_nomark-up_JackJill.R\n\nB.2.2 Informative Texts for Teens Corpus (Info Teens)\nFor copyright reasons, the corpus itself cannot be made available. Details of its composition can be found in Section 4.3.2.5 of the book. If you are interested in using this corpus for non-commercial research purposes and/or in a potential research collaboration, please get in touch with me via e-mail.\n\nB.2.3 Youth Fiction corpus\nFor copyright reasons, the corpus itself cannot be made available. The corresponding metadata can be found here: https://github.com/elenlefoll/TextbookEnglish/blob/main/3_Data/3_Youth_Fiction_Index.csv. If you are interested in using this corpus for non-commercial research purposes and/or in a potential research collaboration, please get in touch with me via e-mail.\n\n\n\n\n\n\nLove, Robbie, Vaclav Brezina, Tony McEnery, Abi Hawtin, Andrew Hardie, and Claire Dembry. 2019. “Functional Variation in the Spoken BNC2014 and the Potential for Register Analysis.” Register Studies 1 (2): 296–317. https://doi.org/10.1075/rs.18013.lov.\n\n\nLove, Robbie, Claire Dembry, Andrew Hardie, Vaclav Brezina, and Tony McEnery. 2017. “The Spoken BNC2014.” International Journal of Corpus Linguistics 22 (3): 319–44. https://doi.org/https://doi.org/10.1075/ijcl.22.3.02lov.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Corpus Data</span>"
    ]
  },
  {
    "objectID": "AppendixC.html",
    "href": "AppendixC.html",
    "title": "Appendix C — Evaluation of the Multi-Feature Tagger of English (MFTE)",
    "section": "",
    "text": "C.1 Packages required\nThe following packages must be installed and loaded to process the evaluation data.\n#renv::restore() # Restore the project's dependencies from the lockfile to ensure that same package versions are used as in the original thesis.\n\nlibrary(caret) # For computing confusion matrices\nlibrary(harrypotter) # Only for colour scheme\nlibrary(here) # For path management\nlibrary(knitr) # Loaded to display the tables using the kable() function\nlibrary(paletteer) # For nice colours\nlibrary(readxl) # For the direct import of Excel files\nlibrary(tidyverse) # For everything else!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Evaluation of the Multi-Feature Tagger of English (MFTE)</span>"
    ]
  },
  {
    "objectID": "AppendixC.html#data-import-from-evaluation-files",
    "href": "AppendixC.html#data-import-from-evaluation-files",
    "title": "Appendix C — Evaluation of the Multi-Feature Tagger of English (MFTE)",
    "section": "\nC.2 Data import from evaluation files",
    "text": "C.2 Data import from evaluation files\nThe data is imported directly from the Excel files in which the manual tag check and corrections was performed. A number of data wrangling steps need to be made for the data to be converted to a tidy format.\n\nCode# Function to import and wrangle the evaluation data from the Excel files in which the manual evaluation was conducted\nimportEval3 &lt;- function(file, fileID, register, corpus) {\n  Tag1 &lt;- file |&gt; \n  add_column(FileID = fileID, Register = register, Corpus = corpus) |&gt;\n  select(FileID, Corpus, Register, Output, Tokens, Tag1, Tag1Gold) |&gt; \n  rename(Tag = Tag1, TagGold = Tag1Gold, Token = Tokens) |&gt; \n  mutate(Evaluation = ifelse(is.na(TagGold), TRUE, FALSE)) |&gt; \n  mutate(TagGold = ifelse(is.na(TagGold), as.character(Tag), as.character(TagGold))) |&gt;\n  filter(!is.na(Tag)) |&gt; \n  mutate_if(is.character, as.factor)\n  \n  Tag2 &lt;- file |&gt; \n  add_column(FileID = fileID, Register = register, Corpus = corpus) |&gt;\n  select(FileID, Corpus, Register, Output, Tokens, Tag2, Tag2Gold) |&gt; \n  rename(Tag = Tag2, TagGold = Tag2Gold, Token = Tokens) |&gt; \n  mutate(Evaluation = ifelse(is.na(TagGold), TRUE, FALSE)) |&gt; \n  mutate(TagGold = ifelse(is.na(TagGold), as.character(Tag), as.character(TagGold))) |&gt;\n  filter(!is.na(Tag)) |&gt; \n  mutate_if(is.character, as.factor)\n\nTag3 &lt;- file |&gt; \n  add_column(FileID = fileID, Register = register, Corpus = corpus) |&gt;\n  select(FileID, Corpus, Register, Output, Tokens, Tag3, Tag3Gold) |&gt; \n  rename(Tag = Tag3, TagGold = Tag3Gold, Token = Tokens) |&gt; \n  mutate(Evaluation = ifelse(is.na(TagGold), TRUE, FALSE)) |&gt; \n  mutate(TagGold = ifelse(is.na(TagGold), as.character(Tag), as.character(TagGold))) |&gt;\n  filter(!is.na(Tag)) |&gt; \n  mutate_if(is.character, as.factor)\n\noutput &lt;- rbind(Tag1, Tag2, Tag3) |&gt; \n  mutate(across(where(is.factor), str_remove_all, pattern = fixed(\" \"))) |&gt; # Removes all white spaces which are found in the excel files\n  filter(!is.na(Output)) |&gt; \n  mutate_if(is.character, as.factor)\n}\n\n# Second function to import and wrangle the evaluation data for Excel files with four tag columns as opposed to three\nimportEval4 &lt;- function(file, fileID, register, corpus) {\n  Tag1 &lt;- file |&gt; \n  add_column(FileID = fileID, Register = register, Corpus = corpus) |&gt;\n  select(FileID, Corpus, Register, Output, Tokens, Tag1, Tag1Gold) |&gt; \n  rename(Tag = Tag1, TagGold = Tag1Gold, Token = Tokens) |&gt; \n  mutate(Evaluation = ifelse(is.na(TagGold), TRUE, FALSE)) |&gt; \n  mutate(TagGold = ifelse(is.na(TagGold), as.character(Tag), as.character(TagGold))) |&gt;\n  filter(!is.na(Tag)) |&gt; \n  mutate_if(is.character, as.factor)\n  \n  Tag2 &lt;- file |&gt; \n  add_column(FileID = fileID, Register = register, Corpus = corpus) |&gt;\n  select(FileID, Corpus, Register, Output, Tokens, Tag2, Tag2Gold) |&gt; \n  rename(Tag = Tag2, TagGold = Tag2Gold, Token = Tokens) |&gt; \n  mutate(Evaluation = ifelse(is.na(TagGold), TRUE, FALSE)) |&gt; \n  mutate(TagGold = ifelse(is.na(TagGold), as.character(Tag), as.character(TagGold))) |&gt;\n  filter(!is.na(Tag)) |&gt; \n  mutate_if(is.character, as.factor)\n\nTag3 &lt;- file |&gt; \n  add_column(FileID = fileID, Register = register, Corpus = corpus) |&gt;\n  select(FileID, Corpus, Register, Output, Tokens, Tag3, Tag3Gold) |&gt; \n  rename(Tag = Tag3, TagGold = Tag3Gold, Token = Tokens) |&gt; \n  mutate(Evaluation = ifelse(is.na(TagGold), TRUE, FALSE)) |&gt; \n  mutate(TagGold = ifelse(is.na(TagGold), as.character(Tag), as.character(TagGold))) |&gt;\n  filter(!is.na(Tag)) |&gt; \n  mutate_if(is.character, as.factor)\n\nTag4 &lt;- file |&gt; \n  add_column(FileID = fileID, Register = register, Corpus = corpus) |&gt;\n  select(FileID, Corpus, Register, Output, Tokens, Tag4, Tag4Gold) |&gt; \n  rename(Tag = Tag4, TagGold = Tag4Gold, Token = Tokens) |&gt; \n  mutate(Evaluation = ifelse(is.na(TagGold), TRUE, FALSE)) |&gt; \n  mutate(TagGold = ifelse(is.na(TagGold), as.character(Tag), as.character(TagGold))) |&gt;\n  filter(!is.na(Tag)) |&gt; \n  mutate_if(is.character, as.factor)\n\noutput &lt;- rbind(Tag1, Tag2, Tag3, Tag4) |&gt; \n  mutate(across(where(is.factor), str_remove_all, pattern = fixed(\" \"))) |&gt; # Removes all white spaces which are found in the excel files\n  filter(!is.na(Tag)) |&gt; \n  mutate_if(is.character, as.factor)\n\n}\n\n# Function to decide which of the two above functions should be used\nimportEval &lt;- function(file, fileID, register, corpus) { \n  if(sum(!is.na(file$Tag4)) &gt; 0) {\n    output = importEval4(file = file, fileID = fileID, register = register, corpus = corpus)\n  }\n  else{\n    output = importEval3(file = file, fileID = fileID, register = register, corpus = corpus)\n  }\n}\n\nSolutions_Intermediate_Spoken_0032 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"Solutions_Intermediate_Spoken_0032_Evaluation.xlsx\")), fileID = \"Solutions_Intermediate_Spoken_0032\", register = \"Conversation\", corpus = \"TEC-Sp\")\n\nHT_5_Poetry_0001 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"HT_5_Poetry_0001_Evaluation.xlsx\")), fileID = \"HT_5_Poetry_0001\", register = \"Poetry\", corpus = \"TEC-Fr\")\n\nAchievers_A1_Informative_0006 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"Achievers_A1_Informative_0006_Evaluation.xlsx\")), fileID = \"Achievers_A1_Informative_0006\", register = \"Informative\", corpus = \"TEC-Sp\")\n\nNew_GreenLine_5_Personal_0003 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"New_GreenLine_5_Personal_0003_Evaluation.xlsx\")), fileID = \"New_GreenLine_5_Personal_0003\", register = \"Personal communication\", corpus = \"TEC-Ger\")\n\nPiece_of_cake_3e_Instructional_0006 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"Piece_of_cake_3e_Instructional_0006_Evaluation.xlsx\")), fileID = \"Piece_of_cake_3e_Instructional_0006\", register = \"Instructional\", corpus = \"TEC-Fr\")\n\nAccess_4_Narrative_0006 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"Access_4_Narrative_0006_Evaluation.xlsx\")), fileID = \"Access_4_Narrative_0006\", register = \"Fiction\", corpus = \"TEC-Ger\")\n\nBNCBFict_b2 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"BNCBFict_b2.xlsx\")), fileID = \"BNCBFict_b2\", register = \"fiction\", corpus = \"BNC2014\")\n\nBNCBFict_m54 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"BNCBFict_m54.xlsx\")), fileID = \"BNCBFict_m54\", register = \"fiction\", corpus = \"BNC2014\")\n\nBNCBFict_e27 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"BNCBFict_e27.xlsx\")), fileID = \"BNCBFict_e27\", register = \"fiction\", corpus = \"BNC2014\")\n\nBNCBMass16 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"BNCBMass16.xlsx\")), fileID = \"BNCBMass16\", register = \"news\", corpus = \"BNC2014\")\n\nBNCBMass23 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"BNCBMass23.xlsx\")), fileID = \"BNCBMass23\", register = \"news\", corpus = \"BNC2014\")\n\nBNCBReg111 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"BNCBReg111.xlsx\")), fileID = \"BNCBReg111\", register = \"news\", corpus = \"BNC2014\")\n\nBNCBReg750 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"BNCBReg750.xlsx\")), fileID = \"BNCBReg750\", register = \"news\", corpus = \"BNC2014\")\n\nBNCBSer486 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"BNCBSer486.xlsx\")), fileID = \"BNCBSer486\", register = \"news\", corpus = \"BNC2014\")\n\nBNCBSer562 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"BNCBSer562.xlsx\")), fileID = \"BNCBSer562\", register = \"news\", corpus = \"BNC2014\")\n\nBNCBEBl8 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"BNCBEBl8.xlsx\")), fileID = \"BNCBEBl8\", register = \"internet\", corpus = \"BNC2014\")\n\nBNCBEFor32 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"BNCBEFor32.xlsx\")), fileID = \"BNCBEFor32\", register = \"internet\", corpus = \"BNC2014\")\n\nS2DD &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"S2DD.xlsx\")), fileID = \"S2DD\", register = \"spoken\", corpus = \"BNC2014\")\n\nS3AV &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"S3AV.xlsx\")), fileID = \"S3AV\", register = \"spoken\", corpus = \"BNC2014\")\n\nSEL5 &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"SEL5.xlsx\")), fileID = \"SEL5\", register = \"spoken\", corpus = \"BNC2014\")\n\nSVLK &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"SVLK.xlsx\")), fileID = \"SVLK\", register = \"spoken\", corpus = \"BNC2014\")\n\nSZXQ &lt;- importEval(file = read_excel(here(\"data\", \"MFTE\", \"evaluation\", \"SZXQ.xlsx\")), fileID = \"SZXQ\", register = \"spoken\", corpus = \"BNC2014\")\n\nTaggerEval &lt;- rbind(Solutions_Intermediate_Spoken_0032, HT_5_Poetry_0001, Achievers_A1_Informative_0006, New_GreenLine_5_Personal_0003, Piece_of_cake_3e_Instructional_0006, Access_4_Narrative_0006, BNCBEBl8, BNCBFict_b2, BNCBFict_m54, BNCBFict_e27, BNCBEFor32, BNCBMass16, BNCBMass23, BNCBReg111, BNCBReg750, BNCBSer486, BNCBSer562, S2DD, S3AV, SEL5, SVLK, SZXQ)\n\n\nSome tags had to be merged to account for changes made to the MFTE between the evaluation and the tagging of the corpora included in the present study.\n\nCodeTaggerEval &lt;- TaggerEval |&gt; \n  mutate(Tag = ifelse(Tag == \"PHC\", \"CC\", as.character(Tag))) |&gt; \n  mutate(TagGold = ifelse(TagGold == \"PHC\", \"CC\", as.character(TagGold))) |&gt; \n  mutate(Tag = ifelse(Tag == \"QLIKE\", \"LIKE\", as.character(Tag))) |&gt; \n  mutate(TagGold = ifelse(TagGold == \"QLIKE\", \"LIKE\", as.character(TagGold))) |&gt; \n  mutate(Tag = ifelse(Tag == \"TO\", \"IN\", as.character(Tag))) |&gt; \n  mutate(TagGold = ifelse(TagGold == \"TO\", \"IN\", as.character(TagGold))) |&gt; \n  mutate_if(is.character, as.factor) |&gt; \n  mutate(Evaluation = ifelse(as.character(Tag) == as.character(TagGold), TRUE, FALSE))\n\n# head(TaggerEval) # Check sanity of data\n# summary(TaggerEval) # Check sanity of data\n\n# saveRDS(TaggerEval, here(\"data\", \"processed\", \"MFTE_Evaluation_Results.rds\"))\n\n# write.csv(TaggerEval, here(\"data\", \"processed\", \"MFTE_Evaluation_Results.csv\"))\n\n\nThis table provides a summary of the complete evaluation dataset. It comprises 25,233 tags that were checked (and, if needs be, corrected) by at least one human annotator. This number includes tags for punctuation marks, which make up a considerable proportion of the tags.\n\n\n          FileID          Corpus               Register        Output     \n BNCBFict_b2 : 2621   TEC-Sp : 1042   fiction      :6500   ._.    : 1156  \n BNCBFict_e27: 2104   TEC-Fr : 2058   news         :6312   the_DT :  820  \n BNCBFict_m54: 1775   TEC-Ger: 1415   spoken       :6047   ,_,    :  720  \n BNCBMass16  : 1619   BNC2014:20718   internet     :1859   a_DT   :  466  \n SEL5        : 1463                   Instructional:1048   of_IN  :  328  \n BNCBEFor32  : 1305                   Poetry       :1010   (Other):21742  \n (Other)     :14346                   (Other)      :2457   NA's   :    1  \n     Token            Tag           TagGold      Evaluation     \n .      : 1156   NN     : 4415   NN     : 4328   Mode :logical  \n the    :  820   IN     : 2145   IN     : 2113   FALSE:832      \n ,      :  720   DT     : 1454   DT     : 1457   TRUE :24401    \n to     :  495   .      : 1367   .      : 1367                  \n 's     :  493   VPRT   : 1044   VPRT   : 1054                  \n (Other):21547   VBD    :  899   VBD    :  895                  \n NA's   :    2   (Other):13909   (Other):14019",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Evaluation of the Multi-Feature Tagger of English (MFTE)</span>"
    ]
  },
  {
    "objectID": "AppendixC.html#estimating-mfte-accuracy-for-textbook-english",
    "href": "AppendixC.html#estimating-mfte-accuracy-for-textbook-english",
    "title": "Appendix C — Evaluation of the Multi-Feature Tagger of English (MFTE)",
    "section": "\nC.3 Estimating MFTE accuracy for Textbook English",
    "text": "C.3 Estimating MFTE accuracy for Textbook English\nIn total, 4,515 tags from the TEC were manually checked. This chunk calculates the recall and precision rates of each feature, ignoring all punctuation and symbols.\n\nCodedata &lt;- TaggerEval |&gt; \n  filter(Corpus %in% c(\"TEC-Fr\", \"TEC-Ger\", \"TEC-Sp\")) |&gt; \n  filter(TagGold != \"UNCLEAR\") |&gt; \n  filter(Tag %in% c(str_extract(Tag, \"[A-Z0-9]+\"))) |&gt; # Remove punctuation tags which are uninteresting here.\n  filter(Tag != \"SYM\" & Tag != \"``\") |&gt; \n  droplevels() |&gt; \n  mutate(Tag = factor(Tag, levels = union(levels(Tag), levels(TagGold)))) |&gt; # Ensure that the factor levels are the same for the next caret operation\n  mutate(TagGold = factor(TagGold, levels = union(levels(Tag), levels(TagGold))))\n\n# Spot gold tag corrections that are not actually errors (should return zero rows if all is well)\n# data[data$Tag==data$TagGold & data$Evaluation == FALSE,] |&gt; as.data.frame()\n\n\nThe breakdown of inaccurate vs. accurate tags in this TEC evaluation sample is:\n\n\n   Mode   FALSE    TRUE \nlogical     114    3831 \n\n\nNote that the following accuracy metrics calculated using the caret::confusionMatrix are not very representative because they include tags, which were not entered in the study, e.g., LS and FW.\n\n\n      Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull \n          0.97           0.97           0.97           0.98           0.20 \nAccuracyPValue  McnemarPValue \n          0.00            NaN \n\n\nAccuracy metrics per feature are more interesting and relevant.\n\n\n\n\n\nPrecision\nRecall\nF1\n\n\n\nClass: ABLE\n1.00\n1.00\n1.00\n\n\nClass: ACT\n0.97\n0.98\n0.98\n\n\nClass: AMP\n1.00\n1.00\n1.00\n\n\nClass: ASPECT\n1.00\n1.00\n1.00\n\n\nClass: BEMA\n1.00\n1.00\n1.00\n\n\nClass: CAUSE\n1.00\n1.00\n1.00\n\n\nClass: CC\n1.00\n0.99\n1.00\n\n\nClass: CD\n0.95\n0.95\n0.95\n\n\nClass: COMM\n1.00\n0.98\n0.99\n\n\nClass: COND\n1.00\n1.00\n1.00\n\n\nClass: CONT\n1.00\n1.00\n1.00\n\n\nClass: CUZ\n1.00\n1.00\n1.00\n\n\nClass: DEMO\n0.97\n0.97\n0.97\n\n\nClass: DMA\n1.00\n1.00\n1.00\n\n\nClass: DOAUX\n0.86\n1.00\n0.92\n\n\nClass: DT\n1.00\n1.00\n1.00\n\n\nClass: DWNT\n0.67\n1.00\n0.80\n\n\nClass: ELAB\n1.00\n1.00\n1.00\n\n\nClass: EMPH\n0.83\n1.00\n0.91\n\n\nClass: EX\n1.00\n1.00\n1.00\n\n\nClass: EXIST\n1.00\n1.00\n1.00\n\n\nClass: FPP1P\n1.00\n1.00\n1.00\n\n\nClass: FPP1S\n1.00\n1.00\n1.00\n\n\nClass: FPUH\n1.00\n1.00\n1.00\n\n\nClass: FREQ\n1.00\n1.00\n1.00\n\n\nClass: FW\n0.10\n1.00\n0.18\n\n\nClass: GTO\n1.00\n1.00\n1.00\n\n\nClass: HDG\n1.00\n1.00\n1.00\n\n\nClass: HGOT\n1.00\n1.00\n1.00\n\n\nClass: IN\n1.00\n1.00\n1.00\n\n\nClass: JJ\n0.96\n0.98\n0.97\n\n\nClass: JPRED\n0.97\n0.90\n0.94\n\n\nClass: LIKE\n0.83\n1.00\n0.91\n\n\nClass: MDCA\n1.00\n1.00\n1.00\n\n\nClass: MDCO\n1.00\n1.00\n1.00\n\n\nClass: MDMM\n1.00\n0.67\n0.80\n\n\nClass: MDNE\n1.00\n0.80\n0.89\n\n\nClass: MDWO\n1.00\n1.00\n1.00\n\n\nClass: MDWS\n1.00\n1.00\n1.00\n\n\nClass: MENTAL\n0.99\n0.99\n0.99\n\n\nClass: NCOMP\n0.88\n1.00\n0.94\n\n\nClass: NN\n0.95\n0.99\n0.97\n\n\nClass: NULL\n1.00\n0.08\n0.14\n\n\nClass: OCCUR\n0.94\n1.00\n0.97\n\n\nClass: PASS\n0.89\n0.89\n0.89\n\n\nClass: PEAS\n1.00\n0.87\n0.93\n\n\nClass: PGET\n1.00\n1.00\n1.00\n\n\nClass: PIT\n1.00\n1.00\n1.00\n\n\nClass: PLACE\n1.00\n0.83\n0.91\n\n\nClass: POLITE\n1.00\n1.00\n1.00\n\n\nClass: POS\n1.00\n1.00\n1.00\n\n\nClass: PROG\n1.00\n0.89\n0.94\n\n\nClass: QUAN\n0.96\n0.98\n0.97\n\n\nClass: QUPR\n1.00\n1.00\n1.00\n\n\nClass: RB\n1.00\n0.99\n0.99\n\n\nClass: RP\n1.00\n1.00\n1.00\n\n\nClass: SO\n1.00\n0.64\n0.78\n\n\nClass: SPLIT\n1.00\n1.00\n1.00\n\n\nClass: SPP2\n1.00\n1.00\n1.00\n\n\nClass: STPR\n0.60\n1.00\n0.75\n\n\nClass: THATD\n0.86\n1.00\n0.92\n\n\nClass: THRC\n1.00\n0.71\n0.83\n\n\nClass: THSC\n0.69\n1.00\n0.82\n\n\nClass: TIME\n1.00\n0.97\n0.98\n\n\nClass: TPP3P\n1.00\n1.00\n1.00\n\n\nClass: TPP3S\n1.00\n1.00\n1.00\n\n\nClass: VB\n0.94\n0.94\n0.94\n\n\nClass: VBD\n0.97\n0.99\n0.98\n\n\nClass: VBG\n0.96\n1.00\n0.98\n\n\nClass: VBN\n0.85\n0.92\n0.88\n\n\nClass: VIMP\n0.99\n0.88\n0.93\n\n\nClass: VPRT\n0.98\n0.98\n0.98\n\n\nClass: WHQU\n0.97\n1.00\n0.98\n\n\nClass: WHSC\n1.00\n0.97\n0.99\n\n\nClass: XX0\n1.00\n1.00\n1.00\n\n\nClass: YNQU\n1.00\n1.00\n1.00\n\n\nClass: OCR\nNA\n0.00\nNA",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Evaluation of the Multi-Feature Tagger of English (MFTE)</span>"
    ]
  },
  {
    "objectID": "AppendixC.html#mfte-accuracy-for-reference-corpora-or-comparable-corpora",
    "href": "AppendixC.html#mfte-accuracy-for-reference-corpora-or-comparable-corpora",
    "title": "Appendix C — Evaluation of the Multi-Feature Tagger of English (MFTE)",
    "section": "\nC.4 MFTE accuracy for reference corpora (or comparable corpora)",
    "text": "C.4 MFTE accuracy for reference corpora (or comparable corpora)\n\nC.4.1 Conversation\nThese are extracts from the Spoken BNC2014 (as entered in the study). The evaluation data for this sample excludes 7 tokens deemed unclear by at least one human annotator.\n\nCodedata &lt;- TaggerEval |&gt; \n  filter(Register == \"spoken\") |&gt; \n  filter(TagGold != \"UNCLEAR\") |&gt; \n  filter(Tag %in% c(str_extract(Tag, \"[A-Z0-9]+\"))) |&gt; # Remove all punctuation tags which are uninteresting here.\n  droplevels() |&gt; \n  mutate(Tag = factor(Tag, levels = union(levels(Tag), levels(TagGold)))) |&gt; # Ensure that the factor levels are the same for the next caret operation\n  mutate(TagGold = factor(TagGold, levels = union(levels(Tag), levels(TagGold))))\n\n# Spot gold tag corrections that are not actually errors (should return zero rows if all is well)\n# data[data$Tag==data$TagGold & data$Evaluation == FALSE,] |&gt; as.data.frame()\n\n\nThe breakdown of inaccurate vs. accurate tags in this evaluation sample is:\n\n\n   Mode   FALSE    TRUE \nlogical     224    5388 \n\n\nNote that the following accuracy metrics calculated using the caret::confusionMatrix are not very representative because they include tags, which were not entered in the study, e.g., LS and FW.\n\n\n      Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull \n          0.96           0.96           0.95           0.97           0.12 \nAccuracyPValue  McnemarPValue \n          0.00            NaN \n\n\n\nC.4.2 Fiction\nThe evaluation data for this sample excludes 0 tokens deemed unclear by at least one human annotator.\n\ndata &lt;- TaggerEval |&gt; \n  filter(Register == \"fiction\") |&gt; \n  filter(TagGold != \"UNCLEAR\") |&gt; \n  filter(Tag %in% c(str_extract(Tag, \"[A-Z0-9]+\"))) |&gt; # Remove all punctuation tags which are uninteresting here.\n  filter(Tag != \"SYM\" & Tag != \"``\") |&gt; \n  droplevels() |&gt; \n  mutate(Tag = factor(Tag, levels = union(levels(Tag), levels(TagGold)))) |&gt; # Ensure that the factor levels are the same for the next caret operation\n  mutate(TagGold = factor(TagGold, levels = union(levels(Tag), levels(TagGold))))\n\n# Spot gold tag corrections that are not actually errors (should return zero rows if all is well)\n# data[data$Tag==data$TagGold & data$Evaluation == FALSE,] |&gt; as.data.frame()\n\nThe breakdown of inaccurate vs. accurate tags in this evaluation sample is:\n\n\n   Mode   FALSE    TRUE \nlogical     168    5346 \n\n\nNote that the following accuracy metrics calculated using the caret::confusionMatrix are not very representative because they include tags, which were not entered in the study, e.g., LS and FW.\n\n\n      Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull \n          0.97           0.97           0.96           0.97           0.19 \nAccuracyPValue  McnemarPValue \n          0.00            NaN \n\n\n\nC.4.3 Informative\nThe evaluation data for this sample excludes 8 tokens deemed unclear by at least one human annotator.\n\ndata &lt;- TaggerEval |&gt; \n  filter(Register == \"news\" | FileID %in% c(\"BNCBEFor32\", \"BNCBEBl8\")) |&gt; \n  filter(TagGold != \"UNCLEAR\") |&gt; \n  filter(Tag %in% c(str_extract(Tag, \"[A-Z0-9]+\"))) |&gt; # Remove all punctuation tags which are uninteresting here.\n  filter(Tag != \"SYM\" & Tag != \"``\") |&gt; \n  droplevels() |&gt; \n  mutate(Tag = factor(Tag, levels = union(levels(Tag), levels(TagGold)))) |&gt; # Ensure that the factor levels are the same for the next caret operation\n  mutate(TagGold = factor(TagGold, levels = union(levels(Tag), levels(TagGold))))\n\n# Spot gold tag corrections that are not actually errors (should return zero rows if all is well)\n# data[data$Tag==data$TagGold & data$Evaluation == FALSE,] |&gt; as.data.frame()\n\nThe breakdown of inaccurate vs. accurate tags in this evaluation sample is:\n\n\n   Mode   FALSE    TRUE \nlogical     309    7113 \n\n\nNote that the following accuracy metrics calculated using the caret::confusionMatrix are not very representative because they include tags, which were not entered in the study, e.g., LS and FW.\n\n\n      Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull \n          0.96           0.95           0.95           0.96           0.24 \nAccuracyPValue  McnemarPValue \n          0.00            NaN",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Evaluation of the Multi-Feature Tagger of English (MFTE)</span>"
    ]
  },
  {
    "objectID": "AppendixC.html#estimating-the-overall-mfte-accuracy-for-corpora-used-in-the-study",
    "href": "AppendixC.html#estimating-the-overall-mfte-accuracy-for-corpora-used-in-the-study",
    "title": "Appendix C — Evaluation of the Multi-Feature Tagger of English (MFTE)",
    "section": "\nC.5 Estimating the overall MFTE accuracy for corpora used in the study",
    "text": "C.5 Estimating the overall MFTE accuracy for corpora used in the study\n\nCodedata &lt;- TaggerEval |&gt; \n  filter(TagGold != \"UNCLEAR\") |&gt; \n  filter(Tag %in% c(str_extract(Tag, \"[A-Z0-9]+\"))) |&gt; # Remove all punctuation tags which are uninteresting here.\n  filter(Tag != \"SYM\" & Tag != \"``\") |&gt; \n  filter(TagGold != \"SYM\" & TagGold != \"``\") |&gt; \n  droplevels() |&gt; \n  mutate(Tag = factor(Tag, levels = union(levels(Tag), levels(TagGold)))) |&gt; # Ensure that the factor levels are the same for the next caret operation\n  mutate(TagGold = factor(TagGold, levels = union(levels(Tag), levels(TagGold))))\n\n# Generate a better formatted results table for export: recall, precision and f1\nconfusion_matrix &lt;- cm$table\ntotal &lt;- sum(confusion_matrix)\nnumber_of_classes &lt;- nrow(confusion_matrix)\ncorrect &lt;- diag(confusion_matrix)\n# sum all columns\ntotal_actual_class &lt;- apply(confusion_matrix, 2, sum)\n# sum all rows\ntotal_pred_class &lt;- apply(confusion_matrix, 1, sum)\n# Precision = TP / all that were predicted as positive\nprecision &lt;- correct / total_pred_class\n# Recall = TP / all that were actually positive\nrecall &lt;- correct / total_actual_class\n# F1\nf1 &lt;- (2 * precision * recall) / (precision + recall)\n# create data frame to output results\nresults &lt;- data.frame(precision, recall, f1, total_actual_class)\n\nresults |&gt; \n  kable(digits = 2)\n\n\n\n\nprecision\nrecall\nf1\ntotal_actual_class\n\n\n\nACT\n0.92\n0.99\n0.95\n177\n\n\nAMP\n1.00\n0.94\n0.97\n16\n\n\nASPECT\n1.00\n1.00\n1.00\n23\n\n\nBEMA\n0.99\n0.99\n0.99\n111\n\n\nCAUSE\n1.00\n1.00\n1.00\n18\n\n\nCC\n1.00\n0.99\n0.99\n254\n\n\nCD\n0.99\n0.98\n0.98\n134\n\n\nCOMM\n1.00\n1.00\n1.00\n88\n\n\nCONC\n0.90\n0.82\n0.86\n11\n\n\nCOND\n1.00\n1.00\n1.00\n17\n\n\nCONT\n0.96\n1.00\n0.98\n54\n\n\nCUZ\n1.00\n0.90\n0.95\n10\n\n\nDEMO\n1.00\n0.96\n0.98\n51\n\n\nDMA\n0.50\n0.40\n0.44\n5\n\n\nDOAUX\n0.92\n0.92\n0.92\n25\n\n\nDT\n1.00\n1.00\n1.00\n490\n\n\nDWNT\n1.00\n1.00\n1.00\n5\n\n\nELAB\n1.00\n1.00\n1.00\n3\n\n\nEMPH\n0.98\n0.95\n0.96\n43\n\n\nEX\n1.00\n1.00\n1.00\n15\n\n\nEXIST\n0.96\n1.00\n0.98\n27\n\n\nFPP1P\n1.00\n1.00\n1.00\n49\n\n\nFPP1S\n1.00\n1.00\n1.00\n59\n\n\nFPUH\n1.00\n0.67\n0.80\n3\n\n\nFREQ\n1.00\n1.00\n1.00\n15\n\n\nFW\n0.29\n0.40\n0.33\n5\n\n\nGTO\n1.00\n1.00\n1.00\n4\n\n\nHDG\n1.00\n1.00\n1.00\n5\n\n\nIN\n0.99\n1.00\n0.99\n836\n\n\nJJAT\n0.94\n0.87\n0.90\n360\n\n\nJJPR\n0.92\n0.74\n0.82\n108\n\n\nLIKE\n1.00\n1.00\n1.00\n9\n\n\nMDCA\n1.00\n1.00\n1.00\n12\n\n\nMDCO\n1.00\n1.00\n1.00\n12\n\n\nMDMM\n1.00\n1.00\n1.00\n1\n\n\nMDNE\n1.00\n0.95\n0.98\n22\n\n\nMDWO\n1.00\n1.00\n1.00\n20\n\n\nMDWS\n1.00\n1.00\n1.00\n31\n\n\nMENTAL\n0.98\n1.00\n0.99\n106\n\n\nNCOMP\n0.92\n0.99\n0.96\n171\n\n\nNN\n0.96\n0.98\n0.97\n1805\n\n\nOCCUR\n1.00\n1.00\n1.00\n11\n\n\nPASS\n0.92\n0.92\n0.92\n79\n\n\nPEAS\n1.00\n0.91\n0.96\n70\n\n\nPGET\n1.00\n0.67\n0.80\n6\n\n\nPIT\n1.00\n0.96\n0.98\n78\n\n\nPLACE\n0.86\n1.00\n0.93\n19\n\n\nPOLITE\n1.00\n1.00\n1.00\n7\n\n\nPOS\n0.98\n0.96\n0.97\n46\n\n\nPROG\n0.92\n0.88\n0.90\n40\n\n\nPRP\n0.00\n0.00\nNaN\n1\n\n\nQUAN\n0.96\n1.00\n0.98\n80\n\n\nQUPR\n1.00\n1.00\n1.00\n21\n\n\nRB\n0.96\n0.95\n0.96\n137\n\n\nRP\n1.00\n0.82\n0.90\n44\n\n\nSO\n1.00\n0.89\n0.94\n9\n\n\nSPLIT\n1.00\n1.00\n1.00\n40\n\n\nSPP2\n1.00\n1.00\n1.00\n53\n\n\nSTPR\n0.50\n1.00\n0.67\n2\n\n\nTHATD\n0.85\n1.00\n0.92\n11\n\n\nTHRC\n1.00\n0.50\n0.67\n8\n\n\nTHSC\n0.85\n1.00\n0.92\n34\n\n\nTIME\n0.95\n0.98\n0.96\n40\n\n\nTPP3P\n1.00\n1.00\n1.00\n61\n\n\nTPP3S\n1.00\n1.00\n1.00\n108\n\n\nURL\n1.00\n1.00\n1.00\n1\n\n\nUSEDTO\n0.00\nNaN\nNaN\n0\n\n\nVB\n0.90\n0.93\n0.91\n258\n\n\nVBD\n0.96\n0.97\n0.97\n215\n\n\nVBG\n0.91\n0.91\n0.91\n111\n\n\nVBN\n0.42\n1.00\n0.59\n22\n\n\nVIMP\n0.71\n0.34\n0.47\n29\n\n\nVPRT\n0.95\n0.95\n0.95\n351\n\n\nWHQU\n1.00\n0.44\n0.62\n9\n\n\nWHSC\n0.95\n1.00\n0.97\n95\n\n\nXX0\n1.00\n0.97\n0.99\n76\n\n\nYNQU\n0.00\nNaN\nNaN\n0\n\n\nNULL\nNaN\n0.00\nNaN\n38\n\n\nSYM\nNaN\n0.00\nNaN\n1\n\n\n``\nNaN\n0.00\nNaN\n1\n\n\n\n\n\n\nCoderesultslong &lt;- results |&gt; \n  drop_na() %&gt;%\n  mutate(tag = row.names(.)) |&gt; \n  filter(tag != \"NULL\" & tag != \"SYM\" & tag != \"OCR\" & tag != \"FW\" & tag != \"USEDTO\") |&gt; \n  rename(n = total_actual_class) |&gt; \n  pivot_longer(cols = c(\"precision\", \"recall\", \"f1\"), names_to = \"metric\", values_to = \"value\") |&gt; \n  mutate(metric = factor(metric, levels = c(\"precision\", \"recall\", \"f1\")))\n\n# summary(resultslong$n)\n\nggplot(resultslong, aes(y = reorder(tag, desc(tag)), x = value, group = metric, colour = n)) +\n  geom_point(size = 2) +\n  ylab(\"\") +\n  xlab(\"\") +\n  facet_wrap(~ metric) +\n  scale_color_paletteer_c(\"harrypotter::harrypotter\", trans = \"log\", breaks = c(1,10, 100, 1000), labels = c(1,10, 100, 1000), name = \"# tokens \\nmanually\\nevaluated\") +\n  theme_bw() +\n  theme(panel.grid.major.y = element_line(colour = \"darkgrey\")) +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TaggerAccuracyPlot.svg\"), width = 7, height = 12)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Evaluation of the Multi-Feature Tagger of English (MFTE)</span>"
    ]
  },
  {
    "objectID": "AppendixC.html#exploring-tagger-errors",
    "href": "AppendixC.html#exploring-tagger-errors",
    "title": "Appendix C — Evaluation of the Multi-Feature Tagger of English (MFTE)",
    "section": "\nC.6 Exploring tagger errors",
    "text": "C.6 Exploring tagger errors\nTo inspect regular/systematic tagger errors, we add an error tag with the incorrectly assigned tag and underscore and then the correct “gold” label.\n\nCodeerrors &lt;- TaggerEval |&gt; \n  filter(Evaluation==\"FALSE\") |&gt; \n  filter(TagGold != \"UNCLEAR\") |&gt; \n  mutate(Error = paste(Tag, TagGold, sep = \" -&gt; \"))\n\nFreqErrors &lt;- errors |&gt; \n  #filter(Corpus %in% c(\"TEC-Fr\", \"TEC-Ger\", \"TEC-Sp\")) |&gt; \n  count(Error) |&gt; \n  arrange(desc(n))\n\n# Number of error types that only occur once\nonce &lt;- FreqErrors |&gt; \n  filter(n == 1) |&gt; \n  nrow()\n\n\nThe total number of errors is 817. Of those, 94 occur just once. In total, there are 198 different types of errors. The most frequent 10 are:\n\nCodeFreqErrors |&gt; \n  filter(n &gt; 10) |&gt; \n  kable(digits = 2)\n\n\n\nError\nn\n\n\n\nNCOMP -&gt; NULL\n37\n\n\nNN -&gt; JJAT\n35\n\n\nJJAT -&gt; NN\n27\n\n\nNN -&gt; VB\n27\n\n\nIN -&gt; RP\n25\n\n\nNN -&gt; VPRT\n24\n\n\nVB -&gt; NN\n22\n\n\nTHSC -&gt; DEMO\n19\n\n\nVB -&gt; VIMP\n19\n\n\nNN -&gt; OCR\n16\n\n\nVBN -&gt; JJAT\n16\n\n\nACT -&gt; NULL\n15\n\n\nTHATD -&gt; NULL\n15\n\n\nCD -&gt; NN\n12\n\n\nMENTAL -&gt; NULL\n12\n\n\nNN -&gt; VBG\n11\n\n\nNN -&gt; VIMP\n11\n\n\nTHSC -&gt; THRC\n11\n\n\nVBG -&gt; PROG\n11\n\n\nVBN -&gt; JJPR\n11\n\n\n\n\n\nThe code in the following chunk can be used to take a closer look at specific types of frequent errors.\n\nerrors |&gt; \n  filter(Error == \"NN -&gt; JJAT\") |&gt; \n  select(-Output, -Corpus, -Tag, -TagGold) |&gt; \n  filter(grepl(x = Token, pattern = \"[A-Z]+.\")) |&gt; \n  kable(digits = 2)\n\n\n\nFileID\nRegister\nToken\nEvaluation\nError\n\n\n\nBNCBEFor32\ninternet\nIntermediate\nFALSE\nNN -&gt; JJAT\n\n\nBNCBMass16\nnews\nFINAL\nFALSE\nNN -&gt; JJAT\n\n\nBNCBMass16\nnews\nBig\nFALSE\nNN -&gt; JJAT\n\n\nBNCBReg111\nnews\nScottish\nFALSE\nNN -&gt; JJAT\n\n\nBNCBReg111\nnews\nScottish\nFALSE\nNN -&gt; JJAT\n\n\nBNCBReg111\nnews\nMental\nFALSE\nNN -&gt; JJAT\n\n\nBNCBReg111\nnews\nScottish\nFALSE\nNN -&gt; JJAT\n\n\nBNCBReg111\nnews\nCentral\nFALSE\nNN -&gt; JJAT\n\n\nBNCBReg750\nnews\nEnglish\nFALSE\nNN -&gt; JJAT\n\n\nBNCBReg750\nnews\nNatural\nFALSE\nNN -&gt; JJAT\n\n\nBNCBReg750\nnews\nEuropean\nFALSE\nNN -&gt; JJAT\n\n\nBNCBReg750\nnews\nChristian\nFALSE\nNN -&gt; JJAT\n\n\nBNCBReg750\nnews\nSocial\nFALSE\nNN -&gt; JJAT\n\n\nBNCBReg750\nnews\nCommon\nFALSE\nNN -&gt; JJAT\n\n\nBNCBSer486\nnews\nNorthern\nFALSE\nNN -&gt; JJAT\n\n\nBNCBSer486\nnews\nNorthern\nFALSE\nNN -&gt; JJAT\n\n\nBNCBSer486\nnews\nNorthern\nFALSE\nNN -&gt; JJAT\n\n\nBNCBSer562\nnews\nUnited\nFALSE\nNN -&gt; JJAT\n\n\nBNCBSer562\nnews\nWhite\nFALSE\nNN -&gt; JJAT\n\n\nBNCBSer562\nnews\nUntold\nFALSE\nNN -&gt; JJAT\n\n\nBNCBSer562\nnews\nNew\nFALSE\nNN -&gt; JJAT\n\n\nSEL5\nspoken\nBlack\nFALSE\nNN -&gt; JJAT\n\n\n\n\nerrors |&gt; \n  filter(Error %in% c(\"NN -&gt; VB\", \"VB -&gt; NN\", \"NN -&gt; VPRT\", \"VPRT -&gt; NN\")) |&gt; \n  count(Token) |&gt; \n  arrange(desc(n)) |&gt; \n  filter(n &gt; 1) |&gt; \n  kable(digits = 2) \n\n\n\nToken\nn\n\n\n\nmince\n5\n\n\nbuild\n4\n\n\nwin\n4\n\n\nhunt\n3\n\n\nwags\n3\n\n\nthrow\n2\n\n\nlook\n2\n\n\nswamp\n2\n\n\nstop\n2\n\n\ndefeats\n2\n\n\n\n\nerrors |&gt; \n  filter(Error == \"ACT -&gt; NULL\") |&gt; \n  count(Token) |&gt; \n  arrange(desc(n)) |&gt; \n  kable(digits = 2) \n\n\n\nToken\nn\n\n\n\nwin\n3\n\n\nthrow\n2\n\n\nlost\n2\n\n\nleft\n1\n\n\nwaiting\n1\n\n\nworking\n1\n\n\nrunning\n1\n\n\ndone\n1\n\n\nfixed\n1\n\n\nPlay\n1\n\n\nreached\n1\n\n\n\n\n\nFor more information on the MFTE evaluation, see (Le Foll 2021) and https://github.com/elenlefoll/MultiFeatureTaggerEnglish.\n\n\n\n\n\n\nLe Foll, Elen. 2021. Introducing the Multi-Feature Tagger of English (MFTE). Osnabrück University. https://github.com/elenlefoll/MultiFeatureTaggerEnglish.\n\n\nLe Foll, Elen, and Muhammad Shakir. 2023. “Introducing a New Open-Source Corpus-Linguistic Tool: The Multi-Feature Tagger of English (MFTE),” May.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Evaluation of the Multi-Feature Tagger of English (MFTE)</span>"
    ]
  },
  {
    "objectID": "AppendixD.html",
    "href": "AppendixD.html",
    "title": "Appendix D — Data Preparation for the Model of Intra-Textbook Variation",
    "section": "",
    "text": "D.1 Packages required\nThe following packages must be installed and loaded to process the data.\n#renv::restore() # Restore the project's dependencies from the lockfile to ensure that same package versions are used as in the original study\n\nlibrary(caret) # For its confusion matrix function\nlibrary(DT) # To display interactive HTML tables\nlibrary(here) # For dynamic file paths\nlibrary(knitr) # Loaded to display the tables using the kable() function\nlibrary(patchwork) # Needed to put together Fig. 1\nlibrary(PerformanceAnalytics) # For the correlation plot\nlibrary(psych) # For various useful, stats function\nlibrary(tidyverse) # For data wrangling",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Data Preparation for the Model of Intra-Textbook Variation</span>"
    ]
  },
  {
    "objectID": "AppendixD.html#data-import-from-mfte-output",
    "href": "AppendixD.html#data-import-from-mfte-output",
    "title": "Appendix D — Data Preparation for the Model of Intra-Textbook Variation",
    "section": "\nD.2 Data import from MFTE output",
    "text": "D.2 Data import from MFTE output\nThe raw data used in this script is a tab-separated file that corresponds to the tabular output of mixed normalised frequencies as generated by the MFTE Perl v. 3.1 (Le Foll 2021a).\n\nCode# Read in Textbook Corpus data\nTxBcounts &lt;- read.delim(here(\"data\", \"MFTE\", \"TxB900MDA_3.1_normed_complex_counts.tsv\"), header = TRUE, stringsAsFactors = TRUE)\nTxBcounts &lt;- TxBcounts |&gt; \n  filter(Filename!=\".DS_Store\") |&gt;  \n  droplevels()\n#str(TxBcounts) # Check sanity of data\n#nrow(TxBcounts) # Should be 2014 files\ndatatable(TxBcounts,\n  filter = \"top\",\n) |&gt; \n  formatRound(2:ncol(TxBcounts), digits=2)\n\n\n\n\n\nMetadata was added on the basis of the filenames.\n\n# Adding a textbook proficiency level\nTxBLevels &lt;- read.delim(here(\"data\", \"metadata\", \"TxB900MDA_ProficiencyLevels.csv\"), sep = \",\")\nTxBcounts &lt;- full_join(TxBcounts, TxBLevels, by = \"Filename\") |&gt;  \n  mutate(Level = as.factor(Level)) |&gt;  \n  mutate(Filename = as.factor(Filename))\n\n# Check distribution and that there are no NAs\nsummary(TxBcounts$Level) |&gt; \n  kable(col.names = c(\"Textbook Level\", \"# of texts\"))\n\n\n\nTextbook Level\n# of texts\n\n\n\nA\n292\n\n\nB\n407\n\n\nC\n506\n\n\nD\n478\n\n\nE\n331\n\n\n\n\n# Check matching on random sample\n# TxBcounts |&gt;\n#   select(Filename, Level) |&gt;  \n#   sample_n(20) \n\n# Adding a register variable from the file names\nTxBcounts$Register &lt;- as.factor(stringr::str_extract(TxBcounts$Filename, \"Spoken|Narrative|Other|Personal|Informative|Instructional|Poetry\")) # Add a variable for Textbook Register\nsummary(TxBcounts$Register) |&gt; \n  kable(col.names = c(\"Textbook Register\", \"# of texts\"))\n\n\n\nTextbook Register\n# of texts\n\n\n\nInformative\n364\n\n\nInstructional\n647\n\n\nNarrative\n285\n\n\nPersonal\n88\n\n\nPoetry\n37\n\n\nSpoken\n593\n\n\n\n\nTxBcounts$Register &lt;- car::recode(TxBcounts$Register, \"'Narrative' = 'Fiction'; 'Spoken' = 'Conversation'\")\n#colnames(TxBcounts) # Check all the variables make sense\n\n# Adding a textbook series variable from the file names\nTxBcounts$Filename &lt;- stringr::str_replace(TxBcounts$Filename, \"English_In_Mind|English_in_Mind\", \"EIM\") \nTxBcounts$Filename &lt;- stringr::str_replace(TxBcounts$Filename, \"New_GreenLine\", \"NGL\") # Otherwise the regex for GreenLine will override New_GreenLine\nTxBcounts$Filename &lt;- stringr::str_replace(TxBcounts$Filename, \"Piece_of_cake\", \"POC\") # Shorten label for ease of plotting\nTxBcounts$Series &lt;- as.factor(stringr::str_extract(TxBcounts$Filename, \"Access|Achievers|EIM|GreenLine|HT|NB|NM|POC|JTT|NGL|Solutions\")) # Extract textbook series from (ammended) filenames\nsummary(TxBcounts$Series)  |&gt; \n  kable(col.names = c(\"Textbook Name\", \"# of texts\"))\n\n\n\nTextbook Name\n# of texts\n\n\n\nAccess\n315\n\n\nAchievers\n240\n\n\nEIM\n180\n\n\nGreenLine\n209\n\n\nHT\n115\n\n\nJTT\n129\n\n\nNB\n44\n\n\nNGL\n298\n\n\nNM\n59\n\n\nPOC\n98\n\n\nSolutions\n327\n\n\n\n\n# Including the French textbooks for the first year of Lycée to their corresponding publisher series from collège\nTxBcounts$Series &lt;-car::recode(TxBcounts$Series, \"c('NB', 'JTT') = 'JTT'; c('NM', 'HT') = 'HT'\") # Recode final volumes of French series (see Section 4.3.1.1 on textbook selection for details)\nsummary(TxBcounts$Series) |&gt; \n  kable(col.names = c(\"Textbook Series\", \"# of texts\"))\n\n\n\nTextbook Series\n# of texts\n\n\n\nAccess\n315\n\n\nAchievers\n240\n\n\nEIM\n180\n\n\nGreenLine\n209\n\n\nHT\n174\n\n\nJTT\n173\n\n\nNGL\n298\n\n\nPOC\n98\n\n\nSolutions\n327\n\n\n\n\n# Adding a textbook country of use variable from the series variable\nTxBcounts$Country &lt;- TxBcounts$Series\nTxBcounts$Country &lt;- car::recode(TxBcounts$Series, \"c('Access', 'GreenLine', 'NGL') = 'Germany'; c('Achievers', 'EIM', 'Solutions') = 'Spain'; c('HT', 'NB', 'NM', 'POC', 'JTT') = 'France'\")\nsummary(TxBcounts$Country) |&gt; \n  kable(col.names = c(\"Country of Use\", \"# of texts\"))\n\n\n\nCountry of Use\n# of texts\n\n\n\nFrance\n445\n\n\nGermany\n822\n\n\nSpain\n747\n\n\n\n\n# Re-order variables\n#colnames(TxBcounts)\nTxBcounts &lt;- select(TxBcounts, order(names(TxBcounts))) %&gt;%\n  select(Filename, Country, Series, Level, Register, Words, everything())\n#colnames(TxBcounts)\n\n\nD.2.1 Corpus size\nThis table provides some summary statistics about the number of words included in the TEC texts originally tagged for this study.\n\nTxBcounts  |&gt;  \n  group_by(Register) |&gt;  \n  summarise(totaltexts = n(), totalwords = sum(Words), mean = as.integer(mean(Words)), sd = as.integer(sd(Words)), TTRmean = mean(TTR)) |&gt;  \n  kable(digits = 2, format.args = list(big.mark = \",\"))\n\n\n\nRegister\ntotaltexts\ntotalwords\nmean\nsd\nTTRmean\n\n\n\nConversation\n593\n505,147\n851\n301\n0.44\n\n\nFiction\n285\n241,512\n847\n208\n0.47\n\n\nInformative\n364\n304,695\n837\n177\n0.51\n\n\nInstructional\n647\n585,049\n904\n94\n0.42\n\n\nPersonal\n88\n69,570\n790\n177\n0.48\n\n\nPoetry\n37\n26,445\n714\n192\n0.44\n\n\n\n\n#TxBcounts &lt;- saveRDS(TxBcounts, here(\"data\", \"processed\", \"TxBcounts.rds\"))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Data Preparation for the Model of Intra-Textbook Variation</span>"
    ]
  },
  {
    "objectID": "AppendixD.html#data-preparation-for-pca",
    "href": "AppendixD.html#data-preparation-for-pca",
    "title": "Appendix D — Data Preparation for the Model of Intra-Textbook Variation",
    "section": "\nD.3 Data preparation for PCA",
    "text": "D.3 Data preparation for PCA\nPoetry texts were removed for this analysis as there were too few compared to the other register categories.\n\nsummary(TxBcounts$Register) |&gt;  \n  kable(col.names = c(\"Register\", \"# texts\"))\n\n\n\nRegister\n# texts\n\n\n\nConversation\n593\n\n\nFiction\n285\n\n\nInformative\n364\n\n\nInstructional\n647\n\n\nPersonal\n88\n\n\nPoetry\n37\n\n\n\n\n\nThis led to the following distribution of texts across the five textbook English registers examined in the model of intra-textbook linguistic variation:\n\nTxBcounts &lt;- TxBcounts |&gt;  \n  filter(Register!=\"Poetry\") |&gt;  \n  droplevels()\n\nsummary(TxBcounts$Register) |&gt;  \n  kable(col.names = c(\"Register\", \"# texts\"))\n\n\n\nRegister\n# texts\n\n\n\nConversation\n593\n\n\nFiction\n285\n\n\nInformative\n364\n\n\nInstructional\n647\n\n\nPersonal\n88\n\n\n\n\n\n\nD.3.1 Feature distributions\nThe distributions of each linguistic features were examined by means of visualisation. As shown below, before transformation, many of the features displayed highly skewed distributions.\n\nCodeTxBcounts |&gt; \n  select(-Words) |&gt;  \n  keep(is.numeric) |&gt;  \n  tidyr::gather() |&gt;  # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2\n  ggplot(aes(value)) +\n    theme_bw() +\n    facet_wrap(~ key, scales = \"free\", ncol = 4) +\n    scale_x_continuous(expand=c(0,0)) +\n    geom_histogram(bins = 30, colour= \"darkred\", fill = \"darkred\", alpha = 0.5)\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TEC-HistogramPlotsAllVariablesTEC-only.svg\"), width = 20, height = 45)\n\n\n\nD.3.2 Feature removal\nA number of features were removed from the dataset as they are not linguistically interpretable. In the case of the TEC, this included the variable CD because numbers spelt out as digits were removed from the textbooks before these were tagged with the MFTE. In addition, the variables LIKE and SO because these are “bin” features included in the output of the MFTE to ensure that the counts for these polysemous words do not inflate other categories due to mistags (Le Foll 2021b).\nWhenever linguistically meaningful, very low-frequency features were merged. Finally, features absent from more than third of texts were also excluded. For the analysis intra-textbook register variation, the following linguistic features were excluded from the analysis due to low dispersion:\n\n# Removal of meaningless features:\nTxBcounts &lt;- TxBcounts |&gt;  \n  select(-c(CD, LIKE, SO))\n\n# Function to compute percentage of texts with occurrences meeting a condition\ncompute_percentage &lt;- function(data, condition, threshold) {\n  numeric_data &lt;- Filter(is.numeric, data)\n  percentage &lt;- round(colSums(condition[, sapply(numeric_data, is.numeric)])/nrow(data) * 100, 2)\n  percentage &lt;- as.data.frame(percentage)\n  colnames(percentage) &lt;- \"Percentage\"\n  percentage &lt;- percentage |&gt;  \n    filter(!is.na(Percentage)) |&gt; \n    rownames_to_column() |&gt; \n    arrange(Percentage)\n  if (!missing(threshold)) {\n    percentage &lt;- percentage |&gt;  \n      filter(Percentage &gt; threshold)\n  }\n  return(percentage)\n}\n\n# Calculate percentage of texts with 0 occurrences of each feature\nzero_features &lt;- compute_percentage(TxBcounts, TxBcounts == 0, 66.6)\n# zero_features |&gt; \n#   kable(col.names = c(\"Feature\", \"% texts with zero occurrences\"))\n\n# Combine low frequency features into meaningful groups whenever this makes linguistic sense\nTxBcounts &lt;- TxBcounts |&gt;  \n  mutate(JJPR = ABLE + JJPR, ABLE = NULL) |&gt;  \n  mutate(PASS = PGET + PASS, PGET = NULL)\n\n# Re-calculate percentage of texts with 0 occurrences of each feature\nzero_features2 &lt;- compute_percentage(TxBcounts, TxBcounts == 0, 66.6)\nzero_features2 |&gt; \n  kable(col.names = c(\"Feature\", \"% texts with zero occurrences\"))\n\n\n\nFeature\n% texts with zero occurrences\n\n\n\nGTO\n67.07\n\n\nELAB\n69.30\n\n\nMDMM\n70.81\n\n\nHGOT\n73.75\n\n\nCONC\n80.48\n\n\nDWNT\n81.44\n\n\nQUTAG\n85.99\n\n\nURL\n96.51\n\n\nEMO\n97.82\n\n\nPRP\n98.33\n\n\nHST\n99.44\n\n\n\n\n# Drop variables with low document frequency\nTxBcounts &lt;- select(TxBcounts, -one_of(zero_features2$rowname))\n#ncol(TxBcounts)-8 # Number of linguistic features remaining\n\n# List of features\n#colnames(TxBcounts)\n\nThese feature removal operations resulted in a feature set of 64 linguistic variables.\n\nD.3.3 Identifying potential outlier texts\nAll normalised frequencies were normalised to identify any potential outlier texts.\n\nTxBzcounts &lt;- TxBcounts |&gt; \n  select(-Words) |&gt;  \n  keep(is.numeric) |&gt;  \n  scale()\n\nboxplot(TxBzcounts, las = 3, main = \"z-scores\") # Slow to open!\n\n\n\n\n\n\n# If necessary, remove any outliers at this stage.\nTxBdata &lt;- cbind(TxBcounts[,1:6], as.data.frame(TxBzcounts))\n\noutliers &lt;- TxBdata |&gt;  \n  select(-c(Words, LD, TTR)) |&gt;  \n  filter(if_any(where(is.numeric), ~ .x &gt; 8)) |&gt;  \n  select(Filename)\n\nThe following outlier texts were identified and excluded in subsequent analyses.\n\nCodeoutliers\n\n                                            Filename\n1                             POC_4e_Spoken_0007.txt\n2             Solutions_Elementary_Personal_0001.txt\n3                       NGL_5_Instructional_0018.txt\n4                           Access_1_Spoken_0011.txt\n5                              EIM_1_Spoken_0012.txt\n6                              NGL_4_Spoken_0011.txt\n7      Solutions_Intermediate_Plus_Personal_0001.txt\n8           Solutions_Elementary_ELF_Spoken_0021.txt\n9                          NB_2_Informative_0009.txt\n10       Solutions_Intermediate_Plus_Spoken_0022.txt\n11     Solutions_Intermediate_Instructional_0025.txt\n12 Solutions_Pre-Intermediate_Instructional_0024.txt\n13                            POC_4e_Spoken_0010.txt\n14            Solutions_Intermediate_Spoken_0019.txt\n15                          Access_1_Spoken_0019.txt\n16    Solutions_Pre-Intermediate_ELF_Spoken_0005.txt\n\nCodeTxBcounts &lt;- TxBcounts |&gt;  \n  filter(!Filename %in% outliers$Filename)\n\n#saveRDS(TxBcounts, here(\"data\", \"processed\", \"TxBcounts3.rds\")) # Last saved 6 March 2024\n\nTxBzcounts &lt;- TxBcounts |&gt; \n  select(-Words) |&gt;  \n  keep(is.numeric) |&gt;  \n  scale()\n\n\nThis resulted in 1,961 TEC texts being included in the model of intra-textbook linguistic variation with the following standardised feature distributions.\n\nCodeTxBzcounts |&gt; \n  as.data.frame() |&gt;  \n  gather() |&gt;  # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2\n  ggplot(aes(value)) +\n    theme_bw() +\n    facet_wrap(~ key, scales = \"free\", ncol = 4) +\n    scale_x_continuous(expand=c(0,0)) +\n    geom_histogram(bins = 30, colour= \"darkred\", fill = \"darkred\", alpha = 0.5)\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TEC-zscores-HistogramsAllVariablesTEC-only.svg\"), width = 20, height = 45)\n\n\n\nD.3.4 Signed log transformation\nA signed logarithmic transformation was applied to (further) deskew the feature distributions (Diwersy, Evert, and Neumann 2014; Neumann and Evert 2021).\nThe signed log transformation function was inspired by the SignedLog function proposed in https://cran.r-project.org/web/packages/DataVisualizations/DataVisualizations.pdf\n\n# All features are signed log-transformed (note that this is also what Neumann & Evert 2021 propose)\nsigned.log &lt;- function(x) {\n  sign(x) * log(abs(x) + 1)\n  }\n\nTxBzlogcounts &lt;- signed.log(TxBzcounts) # Standardise first, then signed log transform\n\n#saveRDS(TxBzlogcounts, here(\"data\", \"processed\", \"TxBzlogcounts.rds\")) # Last saved 6 March 2024\n\nThe new feature distributions are visualised below.\n\nCodeTxBzlogcounts |&gt; \n  as.data.frame() |&gt;  \n  gather() |&gt;  # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2\n  ggplot(aes(value, after_stat(density))) +\n  theme_bw() +\n  facet_wrap(~ key, scales = \"free\", ncol = 4) +\n  scale_x_continuous(expand=c(0,0)) +\n  scale_y_continuous(limits = c(0,NA)) +\n  geom_histogram(bins = 30, colour= \"black\", fill = \"grey\") +\n  geom_density(colour = \"darkred\", weight = 2, fill=\"darkred\", alpha = .4)\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"DensityPlotsAllVariablesSignedLog-TEC-only.svg\"), width = 15, height = 49)\n\n\nThe following correlation plots serve to illustrate the effect of the variable transformations performed in the above chunks.\nExample feature distributions before transformations:\n\nCode# This is a slightly amended version of the PerformanceAnalytics::chart.Correlation() function. It simply removes the significance stars that are meaningless with this many data points (see commented out lines below)\n\nchart.Correlation.nostars &lt;- function (R, histogram = TRUE, method = c(\"pearson\", \"kendall\", \"spearman\"), ...) {\n  x = checkData(R, method = \"matrix\")\n  if (missing(method)) \n    method = method[1]\n  panel.cor &lt;- function(x, y, digits = 2, prefix = \"\", use = \"pairwise.complete.obs\", method = \"pearson\", cex.cor, ...) {\n    usr &lt;- par(\"usr\")\n    on.exit(par(usr))\n    par(usr = c(0, 1, 0, 1))\n    r &lt;- cor(x, y, use = use, method = method)\n    txt &lt;- format(c(r, 0.123456789), digits = digits)[1]\n    txt &lt;- paste(prefix, txt, sep = \"\")\n    if (missing(cex.cor)) \n      cex &lt;- 0.8/strwidth(txt)\n    test &lt;- cor.test(as.numeric(x), as.numeric(y), method = method)\n    # Signif &lt;- symnum(test$p.value, corr = FALSE, na = FALSE, \n    #                  cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1), symbols = c(\"***\", \n    #                                                                           \"**\", \"*\", \".\", \" \"))\n    text(0.5, 0.5, txt, cex = cex * (abs(r) + 0.3)/1.3)\n    # text(0.8, 0.8, Signif, cex = cex, col = 2)\n  }\n  f &lt;- function(t) {\n    dnorm(t, mean = mean(x), sd = sd.xts(x))\n  }\n  dotargs &lt;- list(...)\n  dotargs$method &lt;- NULL\n  rm(method)\n  hist.panel = function(x, ... = NULL) {\n    par(new = TRUE)\n    hist(x, col = \"light gray\", probability = TRUE, \n         axes = FALSE, main = \"\", breaks = \"FD\")\n    lines(density(x, na.rm = TRUE), col = \"red\", lwd = 1)\n    rug(x)\n  }\n  if (histogram) \n    pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor, \n          diag.panel = hist.panel)\n  else pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor)\n}\n\n# Example plot without any variable transformation\nexample1 &lt;- TxBcounts |&gt; \n  select(NN,PROG,SPLIT,ACT,FPP1S)\n\n#png(here(\"plots\", \"CorrChart-TEC-examples-normedcounts.png\"), width = 20, height = 20, units = \"cm\", res = 300)\nchart.Correlation.nostars(example1, histogram=TRUE, pch=19)\n\n\n\n\n\n\nCode#dev.off()\n\n\nExample feature distributions after transformations:\n\nCode# Example plot with transformed variables\nexample2 &lt;- TxBzlogcounts |&gt; \n  as.data.frame() |&gt;  \n  select(NN,PROG,SPLIT,ACT,FPP1S)\n\n#png(here(\"plots\", \"CorrChart-TEC-examples-zsignedlogcounts.png\"), width = 20, height = 20, units = \"cm\", res = 300)\nchart.Correlation.nostars(example2, histogram=TRUE, pch=19)\n\n\n\n\n\n\nCode#dev.off()\n\n\n\nD.3.5 Feature correlations\nThe correlations of the transformed feature frequencies can be visualised in the form of a heatmap. Negative correlations are rendered in blue, whereas positive ones are in red.\n\nCode# Simple heatmap in base R (inspired by Stephanie Evert's SIGIL code)\ncor.colours &lt;- c(\n  hsv(h=2/3, v=1, s=(10:1)/10), # blue = negative correlation \n  rgb(1,1,1), # white = no correlation \n  hsv(h=0, v=1, s=(1:10/10))) # red = positive correlation\n\n#png(here(\"plots\", \"heatmapzlogcounts-TEC-only.png\"), width = 30, height= 30, units = \"cm\", res = 300)\nheatmap(cor(TxBzlogcounts), \n        symm=TRUE, \n        zlim=c(-1,1), \n        col=cor.colours, \n        margins=c(0,0))\n\n\n\n\n\n\nCode#dev.off()\n\n# Calculate the sum of all the words in the tagged texts of the TEC\ntotalwords &lt;- TxBcounts |&gt;  \n  select(Words) |&gt; \n  sum() |&gt; \n  format(big.mark=\",\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Data Preparation for the Model of Intra-Textbook Variation</span>"
    ]
  },
  {
    "objectID": "AppendixD.html#composition-of-tec-textsfiles",
    "href": "AppendixD.html#composition-of-tec-textsfiles",
    "title": "Appendix D — Data Preparation for the Model of Intra-Textbook Variation",
    "section": "\nD.4 Composition of TEC texts/files",
    "text": "D.4 Composition of TEC texts/files\nThese figures and tables provide summary statistics on the texts/files of the TEC that were entered in the multi-dimensional model of intra-textbook linguistic variation. In total, the TEC texts entered amounted to 1,693,650 words.\n\nCodemetadata &lt;- TxBcounts |&gt;  \n  select(Filename, Country, Series, Level, Register, Words) |&gt;  \n  mutate(Volume = paste(Series, Level)) |&gt;  \n  mutate(Volume = fct_rev(Volume)) |&gt;  \n  mutate(Volume = fct_reorder(Volume, as.numeric(Level))) |&gt;  \n  group_by(Volume) |&gt;  \n  mutate(wordcount = sum(Words)) |&gt;  \n  ungroup() |&gt;  \n  distinct(Volume, .keep_all = TRUE)\n\n# Plot for book\nmetadata2 &lt;- TxBcounts |&gt;  \n  select(Country, Series, Level, Register, Words) |&gt;  \n  mutate(Volume = paste(Series, Level)) |&gt;  \n  mutate(Volume = fct_rev(Volume)) |&gt;  \n  #mutate(Volume = fct_reorder(Volume, as.numeric(Level))) |&gt;  \n  group_by(Volume, Register) |&gt;  \n  mutate(wordcount = sum(Words)) |&gt;  \n  ungroup() |&gt;  \n  distinct(Volume, Register, .keep_all = TRUE)\n\n# This is the palette created above on the basis of the suffrager pakcage (but without needed to install the package)\npalette &lt;- c(\"#BD241E\", \"#A18A33\", \"#15274D\", \"#D54E1E\", \"#EA7E1E\", \"#4C4C4C\", \"#722672\", \"#F9B921\", \"#267226\")\n\nPlotSp &lt;- metadata2 |&gt;  \n  filter(Country==\"Spain\") |&gt;  \n  #arrange(Volume) |&gt;  \n  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + \n    geom_bar(stat = \"identity\", position = \"stack\") +\n    coord_flip(expand = FALSE) + # Removes those annoying ticks before each bar label\n    theme_minimal() + theme(legend.position = \"none\") +\n    labs(x = \"Spain\", y = \"Cumulative word count\") +\n    scale_fill_manual(values = palette[c(5,4,3,2,1)], \n                      guide = guide_legend(reverse = TRUE))\n\nPlotGer &lt;- metadata2 |&gt;  \n  filter(Country==\"Germany\") |&gt;  \n  #arrange(Volume) |&gt;  \n  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + \n    geom_bar(stat = \"identity\", position = \"stack\") +\n    coord_flip(expand = FALSE) +\n    labs(x = \"Germany\", y = \"\") +\n    scale_fill_manual(values = palette[c(5,4,3,2,1)], guide = guide_legend(reverse = TRUE)) +\n    theme_minimal() + theme(legend.position = \"none\")\n\nPlotFr &lt;- metadata2 |&gt;  \n  filter(Country==\"France\") |&gt;  \n  #arrange(Volume) |&gt;  \n  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + \n    geom_bar(stat = \"identity\", position = \"stack\") +\n    coord_flip(expand = FALSE) +\n    labs(x = \"France\", y  = \"\", fill = \"Register subcorpus\") +\n    scale_fill_manual(values = palette[c(5,4,3,2,1)], guide = guide_legend(reverse = TRUE, legend.hjust = 0)) +\n    theme_minimal() + theme(legend.position = \"top\", legend.justification = \"left\")\n\nlibrary(patchwork)\n\nPlotFr /\nPlotGer /\nPlotSp\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TEC-T_wordcounts_book.svg\"), width = 8, height = 12)\n\n\nThe following table provides information about the proportion of instructional language featured in each textbook series.\n\nCodemetadataInstr &lt;- TxBcounts |&gt;  \n  select(Country, Series, Level, Register, Words) |&gt;  \n  filter(Register==\"Instructional\") |&gt;  \n  mutate(Volume = paste(Series, Register)) |&gt;  \n  mutate(Volume = fct_rev(Volume)) |&gt;  \n  mutate(Volume = fct_reorder(Volume, as.numeric(Level))) |&gt;  \n  group_by(Volume, Register) |&gt;  \n  mutate(InstrWordcount = sum(Words)) |&gt;  \n  ungroup() |&gt;  \n  distinct(Volume, .keep_all = TRUE) |&gt;  \n  select(Series, InstrWordcount)\n\nmetaWordcount &lt;- TxBcounts |&gt;  \n  select(Country, Series, Level, Register, Words) |&gt;  \n  group_by(Series) |&gt;  \n  mutate(TECwordcount = sum(Words)) |&gt;  \n  ungroup() |&gt;  \n  distinct(Series, .keep_all = TRUE) |&gt;  \n  select(Series, TECwordcount)\n\nwordcount &lt;- merge(metaWordcount, metadataInstr, by = \"Series\")\n\nwordcount |&gt;  \n  mutate(InstrucPercent = InstrWordcount/TECwordcount*100) |&gt;  \n  arrange(InstrucPercent) |&gt;  \n  mutate(InstrucPercent = round(InstrucPercent, 2)) |&gt;  \n  kable(col.names = c(\"Textbook Series\", \"Total words\", \"Instructional words\", \"% of textbook content\"), \n        digits = 2, \n        format.args = list(big.mark = \",\"))\n\n\n\n\n\n\n\n\n\nTextbook Series\nTotal words\nInstructional words\n% of textbook content\n\n\n\nAccess\n259,679\n60,938\n23.47\n\n\nNGL\n278,316\n79,312\n28.50\n\n\nGreenLine\n172,267\n54,263\n31.50\n\n\nSolutions\n270,278\n87,829\n32.50\n\n\nJTT\n137,557\n48,375\n35.17\n\n\nHT\n142,676\n51,550\n36.13\n\n\nPOC\n76,714\n30,548\n39.82\n\n\nEIM\n147,185\n59,928\n40.72\n\n\nAchievers\n208,978\n109,886\n52.58\n\n\n\n\n\n\n\n\n\n\n\nDiwersy, Sascha, Stephanie Evert, and Stella Neumann. 2014. “A Weakly Supervised Multivariate Approach to the Study of Language Variation.” In, edited by Benedikt Szmrecsanyi and Bernhard Wälchli, 174–204. Berlin: De Gruyter.\n\n\nLe Foll, Elen. 2021a. Introducing the Multi-Feature Tagger of English (MFTE). Osnabrück University. https://github.com/elenlefoll/MultiFeatureTaggerEnglish.\n\n\n———. 2021b. Introducing the Multi-Feature Tagger of English (MFTE). Osnabrück University. https://github.com/elenlefoll/MultiFeatureTaggerEnglish.\n\n\nNeumann, Stella, and Stephanie Evert. 2021. “A Register Variation Perspective on Varieties of English.” In, edited by Elena Seoane and Douglas Biber, 144178. Studies in Corpus Linguistics 103. Amsterdam: Benjamins.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Data Preparation for the Model of Intra-Textbook Variation</span>"
    ]
  },
  {
    "objectID": "AppendixE.html",
    "href": "AppendixE.html",
    "title": "Appendix E — Data Analysis for the Model of Intra-Textbook Variation",
    "section": "",
    "text": "E.1 Packages required\nThe following packages must be installed and loaded to carry out the following analyses.\n#renv::restore() # Restore the project's dependencies from the lockfile to ensure that same package versions are used as in the original study\n\nlibrary(caret) # For its confusion matrix function\nlibrary(cowplot)\nlibrary(DescTools) # For 95% CI\nlibrary(emmeans)\nlibrary(factoextra) # For circular graphs of variables\nlibrary(forcats) # For data manipulation\nlibrary(ggthemes) # For theme of factoextra plots\nlibrary(here) # For dynamic file paths\nlibrary(knitr) # Loaded to display the tables using the kable() function\nlibrary(lme4) # For linear regression modelling\nlibrary(patchwork) # To create figures with more than one plot\nlibrary(PCAtools) # For nice biplots of PCA results\nlibrary(psych) # For various useful stats function\nlibrary(sjPlot) # For model plots and tables\nlibrary(tidyverse) # For data wrangling\nlibrary(visreg) # For plots of interaction effects\n\n# From https://github.com/RainCloudPlots/RainCloudPlots:\nsource(here(\"R_rainclouds.R\")) # For geom_flat_violin rainplots",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Data Analysis for the Model of Intra-Textbook Variation</span>"
    ]
  },
  {
    "objectID": "AppendixE.html#preparing-the-data-for-pca",
    "href": "AppendixE.html#preparing-the-data-for-pca",
    "title": "Appendix E — Data Analysis for the Model of Intra-Textbook Variation",
    "section": "\nE.2 Preparing the data for PCA",
    "text": "E.2 Preparing the data for PCA\n\nE.2.1 TEC data import\n\nCodeTxBcounts &lt;- readRDS(here(\"data\", \"processed\", \"TxBcounts3.rds\"))\n# colnames(TxBcounts)\n# nrow(TxBcounts)\n\nTxBzlogcounts &lt;- readRDS(here(\"data\", \"processed\", \"TxBzlogcounts.rds\")) \n# nrow(TxBzlogcounts)\n# colnames(TxBzlogcounts)\n\nTxBdata &lt;- cbind(TxBcounts[,1:6], as.data.frame(TxBzlogcounts))\n# str(TxBdata)\n\n\nFirst, the TEC data as processed in Appendix D is imported. It comprises 1,961 texts/files, each with logged standardised normalised frequencies for 66 linguistic features.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Data Analysis for the Model of Intra-Textbook Variation</span>"
    ]
  },
  {
    "objectID": "AppendixE.html#checking-the-factorability-of-data",
    "href": "AppendixE.html#checking-the-factorability-of-data",
    "title": "Appendix E — Data Analysis for the Model of Intra-Textbook Variation",
    "section": "\nE.3 Checking the factorability of data",
    "text": "E.3 Checking the factorability of data\n\nkmo &lt;- KMO(TxBdata[,7:ncol(TxBdata)]) \n\nThe overall MSA value of the dataset is 0.86. The features have the following individual MSA values (ordered from lowest to largest):\n\nkmo$MSAi[order(kmo$MSAi)] |&gt;  round(2)\n\n  MDWO   MDWS   MDNE   MDCA    VBD   VPRT    POS    ACT   FREQ  TPP3S     LD \n  0.34   0.46   0.52   0.53   0.59   0.60   0.64   0.65   0.65   0.66   0.68 \n CAUSE   COND   MDCO   VIMP  NCOMP     DT  TPP3P   STPR     RP   SPP2 MENTAL \n  0.69   0.75   0.77   0.78   0.79   0.80   0.80   0.81   0.81   0.83   0.84 \n DOAUX   WHSC    VBG  EXIST  THATD   COMM  FPP1S     IN     NN   WHQU   JJAT \n  0.84   0.85   0.86   0.86   0.86   0.87   0.87   0.88   0.88   0.89   0.89 \n  DEMO   THRC ASPECT     CC     EX  OCCUR   PEAS    TTR   YNQU    AWL   QUAN \n  0.89   0.89   0.90   0.90   0.90   0.90   0.91   0.91   0.91   0.92   0.92 \n FPP1P   PROG    XX0   CONT   TIME   BEMA  SPLIT   PASS   JJPR    AMP   QUPR \n  0.92   0.92   0.92   0.92   0.93   0.93   0.93   0.94   0.94   0.95   0.95 \n  THSC     RB   FPUH    CUZ    VBN    PIT    DMA POLITE   EMPH    HDG  PLACE \n  0.95   0.95   0.95   0.95   0.95   0.96   0.96   0.96   0.96   0.96   0.97 \n\n\n\nE.3.1 Removal of feature with MSAs of &lt; 0.5\nWe first remove the first feature with an individual MSA &lt; 0.5, then check the MSA values again and continue removing features one by one if necessary.\n\nTxBdata &lt;- TxBdata |&gt; \n  select(-c(MDWO))\n\nkmo2 &lt;- KMO(TxBdata[,7:ncol(TxBdata)]) \n\nThe overall MSA value of the dataset is now 0.87. None of the remaining features have individual MSA values below 0.5:\n\nkmo2$MSAi[order(kmo2$MSAi)] |&gt;  round(2)\n\n  MDWS   MDNE   MDCA    VBD    POS   VPRT   FREQ    ACT  TPP3S     LD  CAUSE \n  0.55   0.58   0.61   0.63   0.64   0.65   0.65   0.66   0.66   0.69   0.70 \n  MDCO   COND     DT  TPP3P   VIMP  NCOMP     RP   STPR   SPP2  DOAUX MENTAL \n  0.77   0.80   0.80   0.80   0.81   0.81   0.81   0.82   0.83   0.84   0.85 \n   VBG   WHSC  EXIST  THATD  FPP1S   COMM     IN     NN   DEMO   WHQU   THRC \n  0.85   0.85   0.86   0.87   0.87   0.87   0.88   0.89   0.89   0.89   0.89 \n  JJAT ASPECT   PEAS     EX  OCCUR     CC    TTR   YNQU    AWL   QUAN  FPP1P \n  0.89   0.89   0.90   0.90   0.91   0.91   0.91   0.91   0.92   0.92   0.92 \n  TIME    XX0   CONT   PROG   BEMA  SPLIT   PASS   JJPR   THSC    AMP     RB \n  0.92   0.92   0.93   0.93   0.93   0.93   0.94   0.94   0.95   0.95   0.95 \n  QUPR   FPUH    PIT    VBN    DMA    CUZ POLITE   EMPH    HDG  PLACE \n  0.95   0.95   0.96   0.96   0.96   0.96   0.96   0.96   0.96   0.97 \n\n\n\nE.3.2 Choosing the number of principal components to retain\nOn the basis of this scree plot, six principal components were initially retained.\n\nCode# Plot screen plot\n#png(here(\"plots\", \"screeplot-TEC-only.png\"), width = 20, height= 12, units = \"cm\", res = 300)\nscree(TxBdata[,7:ncol(TxBdata)], factors = FALSE, pc = TRUE) # Retain six components\n\n\n\n\n\n\nCode#dev.off()\n\n# Perform PCA\npca1 &lt;- psych::principal(TxBdata[,7:ncol(TxBdata)], \n                         nfactors = 6,\n                         rotate = \"none\")\n#pca1$loadings\n\n\n\nE.3.3 Excluding features with low final communalites\nWe first check whether some feature have extremely low communalities (see https://rdrr.io/cran/FactorAssumptions/man/communalities_optimal_solution.html).\n\n\n  STPR   MDNE    HDG  CAUSE   FREQ   THRC    POS   PROG    ACT   DEMO   MDWS \n  0.09   0.17   0.17   0.19   0.22   0.22   0.24   0.26   0.26   0.27   0.28 \n   CUZ   COND   QUPR  EXIST   MDCO  NCOMP  OCCUR   TIME ASPECT  TPP3P    AMP \n  0.28   0.28   0.29   0.29   0.31   0.31   0.32   0.32   0.33   0.33   0.34 \n    RP  THATD   THSC     EX  FPP1P  PLACE    PIT    VBG   PEAS   MDCA  DOAUX \n  0.34   0.36   0.39   0.43   0.43   0.43   0.45   0.46   0.47   0.48   0.48 \n   VBN   JJPR   JJAT   WHSC  SPLIT   EMPH   QUAN MENTAL  TPP3S   PASS   YNQU \n  0.48   0.49   0.49   0.50   0.51   0.53   0.55   0.56   0.56   0.57   0.58 \nPOLITE     RB     CC    XX0     DT   COMM   WHQU    TTR  FPP1S     IN     LD \n  0.58   0.58   0.58   0.59   0.62   0.62   0.64   0.65   0.67   0.68   0.68 \n  FPUH   VPRT   SPP2   BEMA    DMA    VBD    AWL   CONT     NN   VIMP \n  0.70   0.71   0.72   0.74   0.74   0.80   0.85   0.85   0.87   0.90 \n\n\nAs we chose to exclude features with communalities of &lt; 0.2, we remove STPR, HDG, MDNE and CAUSE from the dataset to be analysed.\n\nTxBdataforPCA &lt;- TxBdata |&gt; \n  select(-c(STPR, MDNE, HDG, CAUSE))\n\nThe overall MSA value of the dataset is now 0.88. None of the remaining features have individual MSA values below 0.5:\n\nkmo3$MSAi[order(kmo3$MSAi)] |&gt;round(2)\n\n  MDWS   MDCA    POS   FREQ    VBD  TPP3S   VPRT    ACT     LD   COND     DT \n  0.54   0.64   0.64   0.65   0.65   0.66   0.66   0.67   0.69   0.78   0.79 \n  MDCO  TPP3P     RP  NCOMP   VIMP   SPP2  DOAUX MENTAL   WHSC    VBG  THATD \n  0.79   0.81   0.82   0.82   0.82   0.82   0.84   0.85   0.86   0.86   0.86 \n EXIST  FPP1S   COMM     NN     IN   WHQU   DEMO ASPECT   JJAT   THRC     EX \n  0.86   0.87   0.88   0.89   0.89   0.89   0.89   0.89   0.89   0.90   0.90 \n OCCUR   PEAS     CC   YNQU   QUAN    AWL   TIME    XX0  FPP1P    TTR   CONT \n  0.90   0.91   0.91   0.91   0.91   0.92   0.92   0.92   0.92   0.92   0.93 \n  PROG   BEMA  SPLIT   PASS   JJPR   THSC     RB   QUPR    AMP   FPUH    PIT \n  0.93   0.93   0.93   0.94   0.94   0.95   0.95   0.95   0.95   0.95   0.95 \n   VBN    DMA POLITE    CUZ   EMPH  PLACE \n  0.95   0.96   0.96   0.96   0.96   0.97 \n\n\nThe final number of linguistic features entered in the intra-textbook model of linguistic variation is 61.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Data Analysis for the Model of Intra-Textbook Variation</span>"
    ]
  },
  {
    "objectID": "AppendixE.html#testing-the-effect-of-rotating-the-components",
    "href": "AppendixE.html#testing-the-effect-of-rotating-the-components",
    "title": "Appendix E — Data Analysis for the Model of Intra-Textbook Variation",
    "section": "\nE.4 Testing the effect of rotating the components",
    "text": "E.4 Testing the effect of rotating the components\nThis chunk was used when considering whether or not to rotate the components (see methods section). Ultimately, the components were not rotated.\n\n# Comparing a rotated vs. a non-rotated solution\n\n#TxBdata &lt;- readRDS(here(\"data\", \"processed\", \"TxBdataforPCA.rds\"))\n\n# No rotation\npca2 &lt;- psych::principal(TxBdata[,7:ncol(TxBdata)], \n                         nfactors = 6, \n                         rotate = \"none\")\n\npca2$loadings\n\nbiplot.psych(pca2, \n             vars = TRUE, \n             choose=c(1,2),\n             )\n\n# Promax rotation\npca2.rotated &lt;- psych::principal(TxBdata[,7:ncol(TxBdata)], \n                         nfactors = 6, \n                         rotate = \"promax\")\n\n# This summary shows the component correlations which is particularly interesting\npca2.rotated\n\npca2.rotated$loadings\n\nbiplot.psych(pca2.rotated, vars = TRUE, choose=c(1,2))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Data Analysis for the Model of Intra-Textbook Variation</span>"
    ]
  },
  {
    "objectID": "AppendixE.html#principal-component-analysis-pca",
    "href": "AppendixE.html#principal-component-analysis-pca",
    "title": "Appendix E — Data Analysis for the Model of Intra-Textbook Variation",
    "section": "\nE.5 Principal Component Analysis (PCA)",
    "text": "E.5 Principal Component Analysis (PCA)\n\nE.5.1 Using the full dataset\nExcept outliers removed as part of the data preparation (see Appendix D).\n\n# Perform PCA on full data\nTxBdata &lt;- readRDS(here(\"data\", \"processed\", \"TxBdataforPCA.rds\"))\n\n\nE.5.2 Using random subsets of the data\nAlternatively, it is possible to conduct the PCA on random subsets of the data to test the stability of the solution. Re-running this line will generate a new subset of the TEC texts containing 2/3 of the texts randomly sampled.\n\nTxBdata &lt;- readRDS(here(\"data\", \"processed\", \"TxBdataforPCA.rds\")) |&gt;\n  slice_sample(n = round(1961*0.6), replace = FALSE)\n\nnrow(TxBdata)\nTxBdata$Filename[1:10]\nnrow(TxBdata) / (ncol(TxBdata)-6) # Check that there is enough data to conduct a PCA. This ratio should be at least 5 (see Friginal & Hardy 2014: 303–304).\n\n\nE.5.3 Using specific subsets of the data\nThe following chunk can be used to perform the PCA on a country subset of the data to test the stability of the solution. See (Le Foll, n.d.) for a detailed analysis of the subcorpus of textbooks used in Germany.\n\nTxBdata &lt;- readRDS(here(\"data\", \"processed\", \"TxBdataforPCA.rds\")) |&gt;\n  #filter(Country == \"France\")\n  #filter(Country == \"Germany\")\n  filter(Country == \"Spain\")\n\nnrow(TxBdata)\nTxBdata$Filename[1:10] # Check data\nnrow(TxBdata) / (ncol(TxBdata)-6) # Check that there is enough data to conduct a PCA. This should be &gt; 5 (see Friginal & Hardy 2014: 303–304).\n\n\nE.5.4 Performing the PCA\nWe perform the PCA using the prcomp function and print a summary of the results.\n\nCodepca &lt;- prcomp(TxBdata[,7:ncol(TxBdata)], scale.=FALSE, rank. = 6) # All quantitative variables for all TxB files except outliers\nregister  &lt;- factor(TxBdata[,\"Register\"]) # Register\nlevel &lt;- factor(TxBdata[,\"Level\"]) # Textbook proficiency level\n\n# summary(register)\n# summary(level)\nsummary(pca)\n\nImportance of first k=6 (out of 61) components:\n                          PC1    PC2     PC3     PC4     PC5     PC6\nStandard deviation     2.1693 1.7776 1.08902 1.00207 0.84288 0.76792\nProportion of Variance 0.2108 0.1416 0.05313 0.04499 0.03183 0.02642\nCumulative Proportion  0.2108 0.3524 0.40553 0.45051 0.48234 0.50876",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Data Analysis for the Model of Intra-Textbook Variation</span>"
    ]
  },
  {
    "objectID": "AppendixE.html#plotting-pca-results",
    "href": "AppendixE.html#plotting-pca-results",
    "title": "Appendix E — Data Analysis for the Model of Intra-Textbook Variation",
    "section": "\nE.6 Plotting PCA results",
    "text": "E.6 Plotting PCA results\n\nE.6.1 3D plots\nThe following chunk can be used to create projections of TEC texts on three dimensions of the model. These plots cannot be rendered in two dimensions and are therefore not generated in the present document. For more information on the pca3d library, see: https://cran.r-project.org/web/packages/pca3d/vignettes/pca3d.pdf.\n\nlibrary(pca3d) # For 3-D plots\n\ncol &lt;- palette[c(1:3,8,7)] # without poetry\nnames(col) &lt;- c(\"Conversation\", \"Fiction\", \"Informative\", \"Instructional\", \"Personal\")\nscales::show_col(col) # Check colours\n\npca3d(pca, \n      group = register,\n      components = 1:3,\n      #components = 4:6,\n      show.ellipses=FALSE, \n      ellipse.ci=0.75,\n      show.plane=FALSE,\n      col = col,\n      shape = \"sphere\",\n      radius = 1,\n      legend = \"right\")\n\nsnapshotPCA3d(here(\"plots\", \"PCA_TxB_3Dsnapshot.png\"))\n\nnames(col) &lt;- c(\"C\", \"B\", \"E\", \"A\", \"D\") # To colour the dots according to the profiency level of the textbooks\npca3d(pca, \n      components = 4:6,\n      group = level,\n      show.ellipses=FALSE, \n      ellipse.ci=0.75,\n      show.plane=FALSE,\n      col = col,\n      shape = \"sphere\",\n      radius = 0.8,\n      legend = \"right\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Data Analysis for the Model of Intra-Textbook Variation</span>"
    ]
  },
  {
    "objectID": "AppendixE.html#two-dimensional-plots-biplots",
    "href": "AppendixE.html#two-dimensional-plots-biplots",
    "title": "Appendix E — Data Analysis for the Model of Intra-Textbook Variation",
    "section": "\nE.7 Two-dimensional plots (biplots)",
    "text": "E.7 Two-dimensional plots (biplots)\nThese plots were generated using the PCAtools package, which requires the data to be formatted in a rather unconventional way so it needs to wrangled first.\n\nE.7.1 Data wrangling for PCAtools\n\nCode#TxBdata &lt;- readRDS(here(\"data\", \"processed\", \"TxBdataforPCA.rds\"))\n\nTxBdata2meta &lt;- TxBdata[,1:6]\nrownames(TxBdata2meta) &lt;- TxBdata2meta$Filename\nTxBdata2meta &lt;- TxBdata2meta |&gt; select(-Filename)\n#head(TxBdata2meta)\n\nTxBdata2 = TxBdata\nrownames(TxBdata2) &lt;- TxBdata2$Filename\nTxBdata2num &lt;- as.data.frame(base::t(TxBdata2[,7:ncol(TxBdata2)]))\n#TxBdata2num[1:12,1:3] # Check sanity of data\n\np &lt;- PCAtools::pca(TxBdata2num, \n         metadata = TxBdata2meta,\n         scale = FALSE)\n\n\n\nE.7.2 Pairs plot\nWe first produce a scatterplot matrix of all the combinations of the first six dimensions of the model of intra-textbook variation. Note that the number before the comma on each axis label shows which principal component is plotted on that axis; this is followed by the percentage of the total variance explained by that particular component. The colours correspond to the text registers.\n\nCode## Colour and shape scheme for all biplots\ncolkey = c(Conversation=\"#BD241E\", Fiction=\"#A18A33\", Informative=\"#15274D\", Instructional=\"#F9B921\", Personal=\"#722672\")\nshapekey = c(A=1, B=2, C=6, D=0, E=5)\n\n## Very slow, open in zoomed out window!\n# Add legend manually? Yes (take it from the biplot code below), sadly really the simplest solution, here. Or use Evert's mvar.pairs plot function (though that also requires manual axis annotation).\n\n# png(here(\"plots\", \"PCA_TxB_pairsplot.png\"), width = 12, height= 19, units = \"cm\", res = 300)\nPCAtools::pairsplot(p,\n                 triangle = FALSE,\n                 components = 1:6,\n                 ncol = 3,\n                 nrow = 5,\n                 pointSize = 0.8,\n                 lab = NULL, # Otherwise will try to label each data point!\n                 colby = \"Register\",\n                 colkey = colkey,\n                 shape = \"Level\",\n                 shapekey = shapekey,\n                 margingaps = unit(c(0.2, 0.2, 0.2, 0.2), \"cm\"),\n                 legendPosition = \"none\")\n\n\n\n\n\n\n\n\nE.7.3 Bi-plots\nThen, biplots of the most important dimensions are generated to examine components more carefully.\n\nCodecolkey = c(Conversation=\"#BD241E\", Fiction=\"#A18A33\", Informative=\"#15274D\", Instructional=\"#F9B921\", Personal=\"#722672\")\nshapekey = c(A=1, B=2, C=6, D=0, E=5)\n\n#png(here(\"plots\", \"PCA_TxB_Biplot_PC1_PC2.png\"), width = 40, height= 25, units = \"cm\", res = 300)\nPCAtools::biplot(p,\n                 x = \"PC1\",\n                 y = \"PC2\",\n                 lab = NULL, # Otherwise will try to label each data point!\n                 #xlim = c(min(p$rotated$PC1)-0.5, max(p$rotated$PC1)+0.5),\n                 #ylim = c(min(p$rotated$PC2)-0.5, max(p$rotated$PC2)+0.5),\n                 colby = \"Register\",\n                 pointSize = 2,\n                 colkey = colkey,\n                 shape = \"Level\",\n                 shapekey = shapekey,\n                 showLoadings = FALSE,\n                 ellipse = TRUE,\n                 axisLabSize = 22,\n                 legendPosition = 'right',\n                 legendTitleSize = 22,\n                 legendLabSize = 18, \n                 legendIconSize = 7) +\n  theme(plot.margin = unit(c(0,0,0,0.2), \"cm\"))\n\n\n\n\n\n\nCode#dev.off()\n#ggsave(here(\"plots\", \"PCA_TxB_BiplotPC1_PC2.svg\"), width = 12, height = 10)\n\n\n\nCode# Biplots to examine components more carefully\npRegisters &lt;- PCAtools::biplot(p,\n                 x = \"PC3\",\n                 y = \"PC4\",\n                 lab = NULL, # Otherwise will try to label each data point!\n                 colby = \"Register\",\n                 pointSize = 2,\n                 colkey = colkey,\n                 shape = \"Level\",\n                 shapekey = shapekey,\n                 showLoadings = FALSE,\n                 ellipse = TRUE,\n                 legendPosition = 'right',\n                 legendTitleSize = 22,\n                 legendLabSize = 18, \n                 legendIconSize = 7) +\n  theme(plot.margin = unit(c(0,0,0,0.2), \"cm\"))\n\n#ggsave(here(\"plots\", \"PCA_TxB_BiplotPC3_PC4.svg\"), width = 12, height = 10)\n\n# Biplots to examine components more carefully\npRegisters2 &lt;- PCAtools::biplot(p,\n                 x = \"PC5\",\n                 y = \"PC6\",\n                 lab = NULL, # Otherwise will try to label each data point!\n                 colby = \"Register\",\n                 pointSize = 2,\n                 colkey = colkey,\n                 shape = \"Level\",\n                 shapekey = shapekey,\n                 showLoadings = FALSE,\n                 ellipse = TRUE,\n                 legendPosition = 'right',\n                 legendTitleSize = 22,\n                 legendLabSize = 18, \n                 legendIconSize = 7) +\n  theme(plot.margin = unit(c(0,0,0,0.2), \"cm\"))\n\n#ggsave(here(\"plots\", \"PCA_TxB_BiplotPC5_PC6.svg\"), width = 12, height = 10)\n\n\nChanging the colour of the points and the ellipses to represent the texts’ target proficiency levels instead of the register allows for a different interpretation of the model.\n\nCode# Inverted keys for the biplots with ellipses for Level rather than Register\ncolkeyLevels = c(A=\"#F9B921\", B=\"#A18A33\", C=\"#BD241E\", D=\"#722672\", E=\"#15274D\")\nshapekeyLevels = c(Conversation=1, Fiction=2, Informative=6, Instructional=0, Personal=5)\n\npLevels &lt;- PCAtools::biplot(p,\n                 x = \"PC3\",\n                 y = \"PC4\",\n                 lab = NULL, # Otherwise will try to label each data point!\n                 #xlim = c(min(p$rotated$PC1)-0.5, max(p$rotated$PC1)+0.5),\n                 #ylim = c(min(p$rotated$PC2)-0.5, max(p$rotated$PC2)+0.5),\n                 colby = \"Level\",\n                 pointSize = 2,\n                 colkey = colkeyLevels,\n                 shape = \"Register\",\n                 shapekey = shapekeyLevels,\n                 showLoadings = FALSE,\n                 ellipse = TRUE,\n                 legendPosition = 'right',\n                 legendTitleSize = 22,\n                 legendLabSize = 18, \n                 legendIconSize = 7) +\n  theme(plot.margin = unit(c(0,0,0,0.2), \"cm\"))\n#ggsave(here(\"plots\", \"PCA_TxB_BiplotPC3_PC4_Level.svg\"), width = 12, height = 10)\n\npLevels2 &lt;- PCAtools::biplot(p,\n                 x = \"PC5\",\n                 y = \"PC6\",\n                 lab = NULL, # Otherwise will try to label each data point!\n                 #xlim = c(min(p$rotated$PC1)-0.5, max(p$rotated$PC1)+0.5),\n                 #ylim = c(min(p$rotated$PC2)-0.5, max(p$rotated$PC2)+0.5),\n                 colby = \"Level\",\n                 pointSize = 2,\n                 colkey = colkeyLevels,\n                 shape = \"Register\",\n                 shapekey = shapekeyLevels,\n                 showLoadings = FALSE,\n                 ellipse = TRUE,\n                 legendPosition = 'right',\n                 legendTitleSize = 22,\n                 legendLabSize = 18, \n                 legendIconSize = 7) +\n  theme(plot.margin = unit(c(0,0,0,0.2), \"cm\"))\n#ggsave(here(\"plots\", \"PCA_TxB_BiplotPC5_PC6_Level.svg\"), width = 12, height = 10)\n\n\n# Display and save the two different representations of data points on PC2 and PC3 using the {patchwork} package\npRegisters / pLevels\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"PCA_TxB_BiplotPC3_PC4_Register_vs_Level.svg\"), width = 14, height = 20)\n\n# Display and save the two different representations of data points on PC5 and PC6 using the {patchwork} package \npRegisters2 / pLevels2\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"PCA_TxB_BiplotPC5_PC6_Register_vs_Level.svg\"), width = 14, height = 20)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Data Analysis for the Model of Intra-Textbook Variation</span>"
    ]
  },
  {
    "objectID": "AppendixE.html#feature-contributions-loadings-on-each-component",
    "href": "AppendixE.html#feature-contributions-loadings-on-each-component",
    "title": "Appendix E — Data Analysis for the Model of Intra-Textbook Variation",
    "section": "\nE.8 Feature contributions (loadings) on each component",
    "text": "E.8 Feature contributions (loadings) on each component\n\nCode#TxBdata &lt;- readRDS(here(\"data\", \"processed\", \"TxBdataforPCA.rds\"))\n\npca &lt;- prcomp(TxBdata[,7:ncol(TxBdata)], scale.=FALSE) # All quantitative variables for all TEC files\n\n# The rotated data that represents the observations / samples is stored in rotated, while the variable loadings are stored in loadings\nloadings &lt;- as.data.frame(pca$rotation[,1:4])\nloadings |&gt; \n  round(2) |&gt; \n  kable()\n\n\n\n\nPC1\nPC2\nPC3\nPC4\n\n\n\nACT\n0.08\n-0.11\n0.04\n-0.10\n\n\nAMP\n-0.12\n-0.10\n-0.11\n0.01\n\n\nASPECT\n0.10\n-0.05\n0.14\n-0.01\n\n\nAWL\n0.22\n-0.16\n-0.12\n-0.13\n\n\nBEMA\n-0.22\n0.01\n-0.21\n0.02\n\n\nCC\n0.05\n-0.21\n-0.19\n0.00\n\n\nCOMM\n0.20\n0.09\n0.14\n-0.04\n\n\nCOND\n-0.01\n-0.02\n0.11\n-0.24\n\n\nCONT\n-0.25\n0.11\n-0.03\n-0.06\n\n\nCUZ\n-0.09\n-0.13\n-0.06\n-0.02\n\n\nDEMO\n-0.12\n0.08\n0.03\n-0.09\n\n\nDMA\n-0.20\n0.14\n-0.02\n0.00\n\n\nDOAUX\n-0.01\n0.20\n0.05\n-0.15\n\n\nDT\n0.12\n0.00\n0.31\n-0.02\n\n\nEMPH\n-0.19\n-0.02\n0.06\n-0.14\n\n\nEX\n-0.10\n-0.05\n-0.11\n0.05\n\n\nEXIST\n-0.02\n-0.15\n-0.09\n-0.09\n\n\nFPP1P\n-0.17\n0.01\n-0.07\n0.00\n\n\nFPP1S\n-0.23\n0.07\n0.08\n-0.01\n\n\nFPUH\n-0.16\n0.15\n-0.09\n0.07\n\n\nFREQ\n-0.03\n-0.05\n0.01\n-0.10\n\n\nIN\n0.17\n-0.18\n0.02\n-0.08\n\n\nJJAT\n-0.06\n-0.18\n0.04\n-0.21\n\n\nJJPR\n-0.17\n-0.06\n-0.11\n-0.11\n\n\nLD\n0.16\n-0.03\n-0.26\n-0.01\n\n\nMDCA\n-0.04\n0.10\n-0.18\n-0.09\n\n\nMDCO\n-0.05\n-0.10\n0.22\n0.01\n\n\nMDWS\n-0.07\n-0.01\n0.05\n-0.16\n\n\nMENTAL\n0.14\n0.13\n0.12\n-0.25\n\n\nNCOMP\n0.04\n-0.05\n-0.24\n-0.15\n\n\nNN\n0.20\n-0.09\n-0.29\n0.11\n\n\nOCCUR\n0.02\n-0.18\n0.03\n0.02\n\n\nPASS\n-0.01\n-0.22\n-0.06\n-0.05\n\n\nPEAS\n-0.06\n-0.17\n0.13\n-0.13\n\n\nPIT\n-0.19\n-0.04\n-0.06\n-0.06\n\n\nPLACE\n-0.16\n-0.01\n-0.07\n0.09\n\n\nPOLITE\n-0.14\n0.13\n-0.07\n0.02\n\n\nPOS\n-0.01\n0.03\n-0.04\n0.16\n\n\nPROG\n-0.11\n-0.02\n0.11\n0.00\n\n\nQUAN\n-0.15\n-0.03\n0.12\n-0.19\n\n\nQUPR\n-0.10\n-0.05\n0.16\n-0.11\n\n\nRB\n-0.19\n-0.08\n0.20\n0.00\n\n\nRP\n0.00\n-0.09\n0.14\n0.02\n\n\nSPLIT\n-0.11\n-0.18\n0.02\n-0.16\n\n\nSPP2\n0.10\n0.22\n-0.01\n-0.25\n\n\nTHATD\n-0.05\n0.04\n0.16\n-0.24\n\n\nTHRC\n0.02\n-0.11\n-0.02\n-0.18\n\n\nTHSC\n-0.06\n-0.17\n0.07\n-0.14\n\n\nTIME\n-0.12\n-0.08\n-0.01\n0.06\n\n\nTPP3P\n-0.01\n-0.16\n-0.09\n-0.02\n\n\nTPP3S\n-0.06\n-0.11\n0.13\n0.30\n\n\nTTR\n-0.04\n-0.26\n-0.05\n-0.01\n\n\nVBD\n-0.08\n-0.20\n0.23\n0.30\n\n\nVBG\n0.04\n-0.18\n0.00\n-0.22\n\n\nVBN\n0.03\n-0.18\n-0.07\n-0.04\n\n\nVIMP\n0.25\n0.15\n0.04\n-0.08\n\n\nVPRT\n-0.15\n0.05\n-0.32\n-0.22\n\n\nWHQU\n0.11\n0.23\n0.00\n-0.09\n\n\nWHSC\n0.11\n-0.11\n0.03\n-0.15\n\n\nXX0\n-0.22\n0.03\n0.06\n-0.06\n\n\nYNQU\n-0.03\n0.23\n0.00\n-0.08\n\n\n\n\n\nWe can go back to the normalised frequencies of the individual features to compare them across different registers and levels, e.g.:\n\nTxBcounts |&gt; \n  group_by(Register, Level) |&gt; \n  summarise(median(NCOMP), MAD(NCOMP)) |&gt; \n  select(1:4) |&gt; \n  kable(digits=2)\n\n\n\nRegister\nLevel\nmedian(NCOMP)\nMAD(NCOMP)\n\n\n\nConversation\nA\n5.69\n2.79\n\n\nConversation\nB\n5.48\n2.66\n\n\nConversation\nC\n5.32\n2.58\n\n\nConversation\nD\n6.18\n2.91\n\n\nConversation\nE\n6.21\n2.62\n\n\nFiction\nA\n4.14\n2.34\n\n\nFiction\nB\n3.96\n2.17\n\n\nFiction\nC\n4.05\n1.86\n\n\nFiction\nD\n5.05\n2.34\n\n\nFiction\nE\n5.05\n2.16\n\n\nInformative\nA\n8.07\n2.48\n\n\nInformative\nB\n7.62\n2.40\n\n\nInformative\nC\n7.49\n3.16\n\n\nInformative\nD\n7.56\n2.46\n\n\nInformative\nE\n8.77\n2.45\n\n\nInstructional\nA\n6.84\n2.54\n\n\nInstructional\nB\n6.80\n2.65\n\n\nInstructional\nC\n6.14\n2.35\n\n\nInstructional\nD\n6.22\n2.29\n\n\nInstructional\nE\n6.75\n2.69\n\n\nPersonal\nA\n6.72\n1.42\n\n\nPersonal\nB\n4.92\n2.33\n\n\nPersonal\nC\n5.75\n1.45\n\n\nPersonal\nD\n6.46\n3.19\n\n\nPersonal\nE\n8.22\n3.09\n\n\n\n\n\nGraphs of features display the features with the strongest contributions to any two dimensions of the model of intra-textbook variation. They are created using the factoextra::fviz_pca_var function.\n\nCodefactoextra::fviz_pca_var(pca,\n             axes = c(1,2),\n             select.var = list(cos2 = 0.1),\n             col.var = \"contrib\", # Colour by contributions to the PC\n             gradient.cols = c(\"#F9B921\", \"#DB241E\", \"#722672\"),\n             title = \"\",\n             repel = TRUE, # Try to avoid too much text overlapping\n             ggtheme = ggthemes::theme_few())\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"fviz_pca_var_PC1_PC2.svg\"), width = 11, height = 9)\n\nfactoextra::fviz_pca_var(pca,\n             axes = c(3,2),\n             select.var = list(contrib = 30),\n             col.var = \"contrib\", # Colour by contributions to the PC\n             gradient.cols = c(\"#F9B921\", \"#DB241E\", \"#722672\"),\n             title = \"\",\n             repel = TRUE, # Try to avoid too much text overlapping\n             ggtheme = ggthemes::theme_few())\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"fviz_pca_var_PC3_PC2.svg\"), width = 9, height = 8)\n\nfactoextra::fviz_pca_var(pca,\n             axes = c(3,4),\n             select.var = list(contrib = 30),\n             col.var = \"contrib\", # Colour by contributions to the PC\n             gradient.cols = c(\"#F9B921\", \"#DB241E\", \"#722672\"),\n             title = \"\",\n             repel = TRUE, # Try to avoid too much text overlapping\n             ggtheme = ggthemes::theme_few())\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"fviz_pca_var_PC3_PC4.svg\"), width = 9, height = 8)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Data Analysis for the Model of Intra-Textbook Variation</span>"
    ]
  },
  {
    "objectID": "AppendixE.html#exploring-the-dimensions-of-the-model",
    "href": "AppendixE.html#exploring-the-dimensions-of-the-model",
    "title": "Appendix E — Data Analysis for the Model of Intra-Textbook Variation",
    "section": "\nE.9 Exploring the dimensions of the model",
    "text": "E.9 Exploring the dimensions of the model\nWe begin with some descriptive statistics of the dimension scores.\n\nCode# http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/#pca-results-for-variables\n\n#TxBdata &lt;- readRDS(here(\"data\", \"processed\", \"TxBdataforPCA.rds\"))\n\npca &lt;- prcomp(TxBdata[,7:ncol(TxBdata)], scale.=FALSE) # All quantitative variables for all TxB files\nregister  &lt;- factor(TxBdata[,\"Register\"]) # Register\nlevel &lt;- factor(TxBdata[,\"Level\"]) # Textbook proficiency level\n\n# summary(register)\n# summary(level)\n# summary(pca)\n\n## Access to the PCA results for individual PC\n#pca$rotation[,1]\n\nres.ind &lt;- cbind(TxBdata[,1:5], as.data.frame(pca$x)[,1:6])\n\nres.ind |&gt; \n  group_by(Register) |&gt; \n  summarise_if(is.numeric, mean) |&gt; \n  kable(digits = 2)\n\n\n\nRegister\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\n\n\n\nConversation\n-2.29\n0.93\n-0.14\n-0.27\n-0.02\n0.06\n\n\nFiction\n-0.85\n-0.81\n1.02\n1.09\n0.11\n-0.10\n\n\nInformative\n0.06\n-2.45\n-0.83\n0.01\n-0.08\n0.12\n\n\nInstructional\n2.68\n0.93\n0.15\n-0.24\n0.01\n-0.07\n\n\nPersonal\n-1.92\n-0.29\n-0.05\n-0.02\n0.07\n-0.09\n\n\n\n\nCoderes.ind |&gt; \n  group_by(Register, Level) |&gt; \n  summarise_if(is.numeric, mean) |&gt; \n  kable(digits = 2)\n\n\n\nRegister\nLevel\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\n\n\n\nConversation\nA\n-2.39\n2.39\n-1.23\n0.71\n-0.45\n-0.01\n\n\nConversation\nB\n-2.54\n1.72\n-0.25\n0.04\n-0.14\n0.13\n\n\nConversation\nC\n-2.25\n0.70\n0.18\n-0.41\n0.09\n-0.02\n\n\nConversation\nD\n-2.10\n-0.08\n0.28\n-0.73\n0.17\n0.09\n\n\nConversation\nE\n-2.13\n-0.14\n0.07\n-0.98\n0.16\n0.17\n\n\nFiction\nA\n-0.95\n0.85\n-0.54\n1.48\n-0.31\n-0.46\n\n\nFiction\nB\n-0.89\n-0.14\n0.95\n1.78\n-0.06\n-0.03\n\n\nFiction\nC\n-0.98\n-0.81\n1.62\n1.23\n0.26\n-0.16\n\n\nFiction\nD\n-0.71\n-1.57\n1.27\n0.72\n0.21\n-0.01\n\n\nFiction\nE\n-0.80\n-1.45\n1.16\n0.56\n0.25\n-0.01\n\n\nInformative\nA\n-0.09\n-1.11\n-1.94\n0.87\n-0.88\n-0.15\n\n\nInformative\nB\n0.15\n-1.67\n-1.19\n0.46\n-0.38\n0.13\n\n\nInformative\nC\n-0.02\n-2.37\n-0.68\n-0.03\n-0.06\n-0.01\n\n\nInformative\nD\n0.06\n-2.89\n-0.45\n-0.19\n0.06\n0.10\n\n\nInformative\nE\n0.15\n-3.13\n-0.79\n-0.38\n0.30\n0.43\n\n\nInstructional\nA\n2.89\n1.55\n-0.20\n0.46\n-0.34\n-0.24\n\n\nInstructional\nB\n2.68\n1.27\n0.09\n0.00\n-0.12\n-0.12\n\n\nInstructional\nC\n2.59\n0.99\n0.28\n-0.32\n-0.07\n0.01\n\n\nInstructional\nD\n2.63\n0.70\n0.28\n-0.49\n0.12\n0.07\n\n\nInstructional\nE\n2.64\n0.09\n0.20\n-0.80\n0.49\n-0.16\n\n\nPersonal\nA\n-1.84\n0.53\n-1.11\n1.21\n-0.31\n0.12\n\n\nPersonal\nB\n-1.85\n0.40\n-0.58\n0.59\n0.21\n-0.07\n\n\nPersonal\nC\n-2.05\n-0.46\n0.52\n-0.17\n0.06\n-0.03\n\n\nPersonal\nD\n-1.89\n-1.05\n0.45\n-0.63\n0.39\n-0.06\n\n\nPersonal\nE\n-1.96\n-0.92\n0.21\n-1.10\n-0.19\n-0.45\n\n\n\n\n\nThe following chunk can be used to search for example texts that are located in specific areas of the biplots. For example, we can search for texts that have high scores on Dim3 and low ones on Dim2 to proceed with a qualitative comparison and analysis of these texts.\n\nres.ind |&gt; \n  filter(PC3 &gt; 2.5 & PC2 &lt; -2) |&gt; \n  select(Filename, PC2, PC3) |&gt; \n  kable(digits = 2)\n\n\n\nFilename\nPC2\nPC3\n\n\n\nAchievers_B1_plus_Narrative_0005.txt\n-3.88\n2.60\n\n\nSolutions_Intermediate_Plus_Spoken_0018.txt\n-2.08\n2.56\n\n\nJTT_3_Narrative_0005.txt\n-2.85\n2.76\n\n\nAchievers_B2_Narrative_00031.txt\n-2.61\n2.59\n\n\nAccess_4_Narrative_0006.txt\n-2.19\n3.18",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Data Analysis for the Model of Intra-Textbook Variation</span>"
    ]
  },
  {
    "objectID": "AppendixE.html#computing-mixed-effects-models-of-the-dimension-scores",
    "href": "AppendixE.html#computing-mixed-effects-models-of-the-dimension-scores",
    "title": "Appendix E — Data Analysis for the Model of Intra-Textbook Variation",
    "section": "\nE.10 Computing mixed-effects models of the dimension scores",
    "text": "E.10 Computing mixed-effects models of the dimension scores\n\nE.10.1 Dimension 1: ‘Overt instructions and explanations’\nHaving compared various models, the following model is chosen as the best-fitting one.\n\n# Models with Textbook series as random intercepts\nmd1 &lt;- lmer(PC1 ~ Register*Level + (1|Series), data = res.ind, REML = FALSE)\nmd1Register &lt;- lmer(PC1 ~ Register + (1|Series), data = res.ind, REML = FALSE)\nmd1Level &lt;- lmer(PC1 ~ Level + (1|Series), data = res.ind, REML = FALSE)\n\nanova(md1, md1Register, md1Level)\n\nData: res.ind\nModels:\nmd1Register: PC1 ~ Register + (1 | Series)\nmd1Level: PC1 ~ Level + (1 | Series)\nmd1: PC1 ~ Register * Level + (1 | Series)\n            npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)    \nmd1Register    7 4080.4 4119.4 -2033.2   4066.4                         \nmd1Level       7 8533.0 8572.0 -4259.5   8519.0    0.0  0               \nmd1           27 4068.3 4219.0 -2007.2   4014.3 4504.6 20  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ntab_model(md1, wrap.labels = 300) # Marginal R2 = 0.890\n\n\n\n \nPC 1\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-2.37\n-2.59 – -2.15\n&lt;0.001\n\n\nRegister [Fiction]\n1.61\n1.36 – 1.87\n&lt;0.001\n\n\nRegister [Informative]\n2.23\n1.96 – 2.50\n&lt;0.001\n\n\nRegister [Instructional]\n5.29\n5.10 – 5.47\n&lt;0.001\n\n\nRegister [Personal]\n0.48\n0.08 – 0.88\n0.019\n\n\nLevel [B]\n-0.12\n-0.30 – 0.05\n0.167\n\n\nLevel [C]\n0.12\n-0.05 – 0.29\n0.159\n\n\nLevel [D]\n0.23\n0.06 – 0.41\n0.010\n\n\nLevel [E]\n0.27\n0.07 – 0.48\n0.010\n\n\nRegister [Fiction] × Level [B]\n0.18\n-0.15 – 0.51\n0.284\n\n\nRegister [Informative] × Level [B]\n0.36\n0.02 – 0.70\n0.038\n\n\nRegister [Instructional] × Level [B]\n-0.10\n-0.35 – 0.15\n0.434\n\n\nRegister [Personal] × Level [B]\n0.11\n-0.39 – 0.61\n0.671\n\n\nRegister [Fiction] × Level [C]\n-0.25\n-0.58 – 0.07\n0.130\n\n\nRegister [Informative] × Level [C]\n-0.00\n-0.32 – 0.32\n0.993\n\n\nRegister [Instructional] × Level [C]\n-0.39\n-0.62 – -0.15\n0.001\n\n\nRegister [Personal] × Level [C]\n-0.22\n-0.72 – 0.28\n0.381\n\n\nRegister [Fiction] × Level [D]\n-0.05\n-0.38 – 0.27\n0.739\n\n\nRegister [Informative] × Level [D]\n-0.01\n-0.33 – 0.31\n0.946\n\n\nRegister [Instructional] × Level [D]\n-0.47\n-0.72 – -0.23\n&lt;0.001\n\n\nRegister [Personal] × Level [D]\n-0.07\n-0.60 – 0.46\n0.800\n\n\nRegister [Fiction] × Level [E]\n-0.24\n-0.58 – 0.10\n0.173\n\n\nRegister [Informative] × Level [E]\n0.06\n-0.29 – 0.40\n0.747\n\n\nRegister [Instructional] × Level [E]\n-0.50\n-0.77 – -0.22\n&lt;0.001\n\n\nRegister [Personal] × Level [E]\n-0.18\n-0.74 – 0.38\n0.527\n\n\nRandom Effects\n\n\nσ2\n\n0.45\n\n\nτ00Series\n\n0.07\n\n\n\nICC\n0.14\n\n\n\nN Series\n\n9\n\n\nObservations\n1961\n\n\nMarginal R2 / Conditional R2\n\n0.890 / 0.906\n\n\n\n\nIts estimated coefficients are visualised in the plot below.\n\nCode# Plot of fixed effects:\nplot_model(md1Register, \n           type = \"est\",\n           show.intercept = TRUE,\n           show.values=TRUE, \n           show.p=TRUE,\n           value.offset = .4,\n           value.size = 3.5,\n           colors = palette[c(1:3,8,7)],\n           group.terms = c(1:5), \n           title = \"\",\n           wrap.labels = 40,\n           axis.title = \"PC1 estimated coefficients\") +\n  theme_sjplot2() \n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TxB_PCA1_lmer_fixedeffects_Register.svg\"), height = 3, width = 8)\n\n\nThe emmeans and pairs functions are used to compare the estimated Dim1 scores for each register and to compare these to one another.\n\nCodeRegister_results &lt;- emmeans(md1Register, \"Register\")\nsummary(Register_results)\n\n Register       emmean    SE   df lower.CL upper.CL\n Conversation  -2.2793 0.102 11.6   -2.502   -2.056\n Fiction       -0.7267 0.106 13.9   -0.955   -0.498\n Informative    0.0603 0.104 12.7   -0.165    0.286\n Instructional  2.7141 0.101 11.3    2.492    2.937\n Personal      -1.8734 0.122 25.5   -2.125   -1.622\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodecomparisons &lt;- pairs(Register_results, adjust = \"tukey\")\ncomparisons\n\n contrast                     estimate     SE   df  t.ratio p.value\n Conversation - Fiction         -1.553 0.0508 1963  -30.535  &lt;.0001\n Conversation - Informative     -2.340 0.0465 1961  -50.341  &lt;.0001\n Conversation - Instructional   -4.993 0.0399 1961 -125.141  &lt;.0001\n Conversation - Personal        -0.406 0.0791 1958   -5.134  &lt;.0001\n Fiction - Informative          -0.787 0.0557 1962  -14.135  &lt;.0001\n Fiction - Instructional        -3.441 0.0497 1962  -69.168  &lt;.0001\n Fiction - Personal              1.147 0.0840 1958   13.645  &lt;.0001\n Informative - Instructional    -2.654 0.0447 1957  -59.399  &lt;.0001\n Informative - Personal          1.934 0.0816 1957   23.692  &lt;.0001\n Instructional - Personal        4.587 0.0780 1957   58.820  &lt;.0001\n\nDegrees-of-freedom method: kenward-roger \nP value adjustment: tukey method for comparing a family of 5 estimates \n\nCode#write_last_clip()\nconfint(comparisons)\n\n contrast                     estimate     SE   df lower.CL upper.CL\n Conversation - Fiction         -1.553 0.0508 1963   -1.691   -1.414\n Conversation - Informative     -2.340 0.0465 1961   -2.466   -2.213\n Conversation - Instructional   -4.993 0.0399 1961   -5.102   -4.884\n Conversation - Personal        -0.406 0.0791 1958   -0.622   -0.190\n Fiction - Informative          -0.787 0.0557 1962   -0.939   -0.635\n Fiction - Instructional        -3.441 0.0497 1962   -3.577   -3.305\n Fiction - Personal              1.147 0.0840 1958    0.917    1.376\n Informative - Instructional    -2.654 0.0447 1957   -2.776   -2.532\n Informative - Personal          1.934 0.0816 1957    1.711    2.156\n Instructional - Personal        4.587 0.0780 1957    4.374    4.800\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \nConf-level adjustment: tukey method for comparing a family of 5 estimates \n\nCode#write_last_clip()\n\n\nWe can also visualise the estimated coefficients for the textbook series, which is modelled here as a random effect.\n\nCodeplot_model(md1, \n           type = \"re\", # Option to visualise random effects\n           show.values=TRUE, \n           show.p=TRUE,\n           value.offset = .4,\n           value.size = 3.5,\n           colors = \"bw\",\n           wrap.labels = 40,\n           axis.title = \"PC1 estimated coefficients\") +\n  theme_sjplot2()\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TxB_PCA1_lmer_randomeffects.svg\"), height = 3, width = 8)\n\n\n\nE.10.2 Dimension 2: ‘Involved vs. Informational Production’\n\nCodemd2 &lt;- lmer(PC2 ~ Register*Level + (1|Series), data = res.ind, REML = FALSE)\nmd2Register &lt;- lmer(PC2 ~ Register + (1|Series), data = res.ind, REML = FALSE)\nmd2Level &lt;- lmer(PC2 ~ Level + (1|Series), data = res.ind, REML = FALSE)\nanova(md2, md2Register, md2Level)\n\nData: res.ind\nModels:\nmd2Register: PC2 ~ Register + (1 | Series)\nmd2Level: PC2 ~ Level + (1 | Series)\nmd2: PC2 ~ Register * Level + (1 | Series)\n            npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)    \nmd2Register    7 6155.2 6194.3 -3070.6   6141.2                         \nmd2Level       7 7290.1 7329.2 -3638.1   7276.1    0.0  0               \nmd2           27 5200.9 5351.6 -2573.4   5146.9 2129.2 20  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCodetab_model(md2) # Marginal R2 = 0.723\n\n\n\n \nPC 2\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n2.46\n2.20 – 2.73\n&lt;0.001\n\n\nRegister [Fiction]\n-1.80\n-2.14 – -1.46\n&lt;0.001\n\n\nRegister [Informative]\n-3.44\n-3.79 – -3.08\n&lt;0.001\n\n\nRegister [Instructional]\n-0.87\n-1.12 – -0.62\n&lt;0.001\n\n\nRegister [Personal]\n-1.89\n-2.42 – -1.35\n&lt;0.001\n\n\nLevel [B]\n-0.72\n-0.96 – -0.49\n&lt;0.001\n\n\nLevel [C]\n-1.71\n-1.94 – -1.49\n&lt;0.001\n\n\nLevel [D]\n-2.37\n-2.61 – -2.14\n&lt;0.001\n\n\nLevel [E]\n-2.65\n-2.93 – -2.37\n&lt;0.001\n\n\nRegister [Fiction] ×Level [B]\n-0.28\n-0.72 – 0.17\n0.223\n\n\nRegister [Informative] ×Level [B]\n0.13\n-0.32 – 0.58\n0.565\n\n\nRegister [Instructional]× Level [B]\n0.44\n0.11 – 0.77\n0.009\n\n\nRegister [Personal] ×Level [B]\n0.53\n-0.14 – 1.20\n0.120\n\n\nRegister [Fiction] ×Level [C]\n0.08\n-0.35 – 0.52\n0.705\n\n\nRegister [Informative] ×Level [C]\n0.42\n-0.01 – 0.84\n0.053\n\n\nRegister [Instructional]× Level [C]\n1.11\n0.80 – 1.43\n&lt;0.001\n\n\nRegister [Personal] ×Level [C]\n0.59\n-0.07 – 1.26\n0.081\n\n\nRegister [Fiction] ×Level [D]\n-0.01\n-0.44 – 0.42\n0.972\n\n\nRegister [Informative] ×Level [D]\n0.49\n0.07 – 0.91\n0.024\n\n\nRegister [Instructional]× Level [D]\n1.49\n1.16 – 1.81\n&lt;0.001\n\n\nRegister [Personal] ×Level [D]\n0.59\n-0.12 – 1.30\n0.104\n\n\nRegister [Fiction] ×Level [E]\n0.42\n-0.04 – 0.87\n0.072\n\n\nRegister [Informative] ×Level [E]\n0.39\n-0.07 – 0.86\n0.099\n\n\nRegister [Instructional]× Level [E]\n1.05\n0.68 – 1.42\n&lt;0.001\n\n\nRegister [Personal] ×Level [E]\n1.01\n0.27 – 1.76\n0.008\n\n\nRandom Effects\n\n\nσ2\n\n0.80\n\n\nτ00Series\n\n0.09\n\n\n\nICC\n0.10\n\n\n\nN Series\n\n9\n\n\nObservations\n1961\n\n\nMarginal R2 / Conditional R2\n\n0.723 / 0.752\n\n\n\nCode# tab_model(md2Register) # Marginal R2 = 0.558\n# tab_model(md2Level) # Marginal R2 = 0.228\n\n\nEstimated coefficients of fixed effects on Dim2 scores:\n\nCodeplot_model(md2, \n           type = \"est\",\n           show.intercept = TRUE,\n           show.values=TRUE, \n           show.p=TRUE,\n           value.offset = .4,\n           value.size = 3.5,\n           colors = palette[c(1:3,8,7)],\n           group.terms = c(1:5,1,1,1,1,2:5,2:5,2:5,2:5), \n           title = \"\",\n           wrap.labels = 40,\n           axis.title = \"PC2 estimated coefficients\") +\n  theme_sjplot2() \n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TxB_PCA2_lmer_fixedeffects.svg\"), height = 8, width = 8)\n\n\nEstimated coefficients of random effects on Dim2 scores:\n\nCode## Random intercepts\nplot_model(md2, \n           type = \"re\", # Option to visualise random effects\n           show.values=TRUE, \n           show.p=TRUE,\n           value.offset = .4,\n           value.size = 3.5,\n           colors = \"bw\",\n           wrap.labels = 40,\n           axis.title = \"PC2 estimated coefficients\") +\n  theme_sjplot2()\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TxB_PCA2_lmer_randomeffects.svg\"), height = 3, width = 8)\n\n# Textbook Country as a random effect variable\nmd2country &lt;- lmer(PC2 ~ Register*Level + (1|Country), data = res.ind, REML = FALSE)\n\nplot_model(md2country, \n           type = \"re\", # Option to visualise random effects\n           show.values=TRUE, \n           show.p=TRUE,\n           value.offset = .4,\n           value.size = 3.5,\n           colors = \"bw\",\n           wrap.labels = 40,\n           axis.title = \"PC2 estimated coefficients\") +\n  theme_sjplot2()\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TxB_PCA2_lmer_randomeffects_country.svg\"), height = 3, width = 8)\n\n\nThe visreg function is used to visualise the distributions of the modelled Dim2 scores:\n\nCode# svg(here(\"plots\", \"TxB_predicted_PC2_scores_interactions.svg\"), height = 5, width = 8)\nvisreg(md2, xvar = \"Level\", by=\"Register\", type = \"conditional\",\n       line=list(col=\"darkred\"), \n       xlab = \"Textbook Level\", ylab = \"PC2\"\n       #,gg = TRUE\n       ,layout=c(5,1)\n)\n\n\n\n\n\n\nCode# dev.off()\n\n# Textbook Series-Register interactions\nvisreg::visreg(md2, \"Register\", by=\"Series\", re.form=~(1|Series),\n               ylab=\"PC2\", line=list(col=\"darkred\"))\n\n\n\n\n\n\nCodevisreg(md2, xvar = \"Series\", by=\"Level\", type = \"conditional\", re.form=~(1|Series), \n       line=list(col=\"darkred\"), xlab = \"Textbook Series\", ylab = \"PC2\",\n       layout=c(1,5))\n\n\n\n\n\n\nCode# Textbook Series-Register interactions\n# svg(here(\"plots\", \"TxB_PCA2_lmer_randomeffects_country_register.svg\"), height = 5, width = 8)\nvisreg::visreg(md2country, \"Country\", by=\"Register\", re.form=~(1|Country),\n               ylab=\"PC2\", line=list(col=\"darkred\"))\n\n\n\n\n\n\nCode# dev.off()\n\n\n\nE.10.3 Dimension 3: ‘Narrative vs. factual discourse’\n\nCodemd3 &lt;- lmer(PC3 ~ Register*Level + (1|Series), data = res.ind, REML = FALSE)\nmd3Register &lt;- lmer(PC3 ~ Register + (1|Series), data = res.ind, REML = FALSE)\nmd3Level &lt;- lmer(PC3 ~ Level + (1|Series), data = res.ind, REML = FALSE)\n\nanova(md3, md3Register, md3Level)\n\nData: res.ind\nModels:\nmd3Register: PC3 ~ Register + (1 | Series)\nmd3Level: PC3 ~ Level + (1 | Series)\nmd3: PC3 ~ Register * Level + (1 | Series)\n            npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)    \nmd3Register    7 5139.9 5179.0 -2563.0   5125.9                         \nmd3Level       7 5528.8 5567.9 -2757.4   5514.8   0.00  0               \nmd3           27 4582.6 4733.3 -2264.3   4528.6 986.21 20  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCodetab_model(md3) # Marginal R2 = 0.436\n\n\n\n \nPC 3\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-1.33\n-1.62 – -1.04\n&lt;0.001\n\n\nRegister [Fiction]\n0.79\n0.50 – 1.08\n&lt;0.001\n\n\nRegister [Informative]\n-0.79\n-1.10 – -0.49\n&lt;0.001\n\n\nRegister [Instructional]\n1.03\n0.82 – 1.24\n&lt;0.001\n\n\nRegister [Personal]\n0.19\n-0.27 – 0.65\n0.411\n\n\nLevel [B]\n0.97\n0.77 – 1.17\n&lt;0.001\n\n\nLevel [C]\n1.39\n1.20 – 1.58\n&lt;0.001\n\n\nLevel [D]\n1.50\n1.30 – 1.70\n&lt;0.001\n\n\nLevel [E]\n1.60\n1.36 – 1.83\n&lt;0.001\n\n\nRegister [Fiction] ×Level [B]\n0.55\n0.17 – 0.93\n0.004\n\n\nRegister [Informative] ×Level [B]\n-0.13\n-0.52 – 0.25\n0.504\n\n\nRegister [Instructional]× Level [B]\n-0.67\n-0.95 – -0.39\n&lt;0.001\n\n\nRegister [Personal] ×Level [B]\n-0.34\n-0.91 – 0.23\n0.242\n\n\nRegister [Fiction] ×Level [C]\n0.78\n0.41 – 1.15\n&lt;0.001\n\n\nRegister [Informative] ×Level [C]\n-0.17\n-0.53 – 0.19\n0.362\n\n\nRegister [Instructional]× Level [C]\n-0.94\n-1.21 – -0.68\n&lt;0.001\n\n\nRegister [Personal] ×Level [C]\n0.17\n-0.40 – 0.74\n0.568\n\n\nRegister [Fiction] ×Level [D]\n0.34\n-0.03 – 0.70\n0.073\n\n\nRegister [Informative] ×Level [D]\n0.01\n-0.35 – 0.37\n0.953\n\n\nRegister [Instructional]× Level [D]\n-1.01\n-1.29 – -0.73\n&lt;0.001\n\n\nRegister [Personal] ×Level [D]\n0.02\n-0.58 – 0.63\n0.939\n\n\nRegister [Fiction] ×Level [E]\n0.23\n-0.15 – 0.62\n0.238\n\n\nRegister [Informative] ×Level [E]\n-0.18\n-0.58 – 0.21\n0.364\n\n\nRegister [Instructional]× Level [E]\n-1.01\n-1.32 – -0.70\n&lt;0.001\n\n\nRegister [Personal] ×Level [E]\n-0.21\n-0.84 – 0.43\n0.520\n\n\nRandom Effects\n\n\nσ2\n\n0.58\n\n\nτ00Series\n\n0.14\n\n\n\nICC\n0.19\n\n\n\nN Series\n\n9\n\n\nObservations\n1961\n\n\nMarginal R2 / Conditional R2\n\n0.436 / 0.543\n\n\n\nCode# tab_model(md3Register) # Marginal R2 = 0.272\n# tab_model(md3Level) # Marginal R2 = 0.119\n\n# Plot of fixed effects:\nplot_model(md3, \n           type = \"est\",\n           show.intercept = TRUE,\n           show.values=TRUE, \n           show.p=TRUE,\n           value.offset = .4,\n           value.size = 3.5,\n           colors = palette[c(1:3,8,7)],\n           group.terms = c(1:5,1,1,1,1,2:5,2:5,2:5,2:5), \n           title = \"\",\n           wrap.labels = 40,\n           axis.title = \"PC3 estimated coefficients\") +\n  theme_sjplot2() \n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TxB_PCA3_lmer_fixedeffects.svg\"), height = 8, width = 8)\n\n\n\n# Plot of random effects:\nplot_model(md3, \n           type = \"re\", # Option to visualise random effects\n           show.values=TRUE, \n           show.p=TRUE,\n           value.offset = .4,\n           value.size = 3.5,\n           color = \"bw\",\n           wrap.labels = 40,\n           axis.title = \"PC3 estimated coefficients\") +\n  theme_sjplot2()\n\n\n\n\n\n\n#ggsave(here(\"plots\", \"TxB_PCA3_lmer_randomeffects.svg\"), height = 3, width = 8)\n\n\nCode# svg(here(\"plots\", \"TxB_predicted_PC3_scores_interactions.svg\"), height = 5, width = 8)\nvisreg(md3, xvar = \"Level\", by=\"Register\", type = \"conditional\",\n       line=list(col=\"darkred\"), \n       xlab = \"Textbook Level\", ylab = \"PC3\"\n       #,gg = TRUE\n       ,layout=c(5,1)\n)\n\n\n\n\n\n\nCode# dev.off()\n\n# Textbook Series-Register interactions\nvisreg::visreg(md3, \"Series\", by=\"Register\", re.form=~(1|Series),\n               ylab=\"PC3\", line=list(col=\"darkred\"))\n\n\n\n\n\n\nCodevisreg(md3, xvar = \"Level\", by=\"Series\", type = \"conditional\", re.form=~(1|Series), \n       line=list(col=\"darkred\"), xlab = \"Textbook Series\", ylab = \"PC3\")\n\n\n\n\n\n\n\n\nE.10.4 Dimension 4: ‘Informational compression vs. elaboration’\n\nCodemd4 &lt;- lmer(PC4 ~ Register*Level + (1|Series), data = res.ind, REML = FALSE)\nmd4Register &lt;- lmer(PC4 ~ Register + (1|Series), data = res.ind, REML = FALSE)\nmd4Level &lt;- lmer(PC4 ~ Level + (1|Series), data = res.ind, REML = FALSE)\n\nanova(md4, md4Register, md4Level)\n\nData: res.ind\nModels:\nmd4Register: PC4 ~ Register + (1 | Series)\nmd4Level: PC4 ~ Level + (1 | Series)\nmd4: PC4 ~ Register * Level + (1 | Series)\n            npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)    \nmd4Register    7 5034.0 5073.0 -2510.0   5020.0                         \nmd4Level       7 5043.6 5082.7 -2514.8   5029.6   0.00  0               \nmd4           27 4372.1 4522.8 -2159.1   4318.1 711.52 20  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCodetab_model(md4) # Marginal R2 = 0.426\n\n\n\n \nPC 4\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n0.76\n0.56 – 0.96\n&lt;0.001\n\n\nRegister [Fiction]\n0.72\n0.44 – 1.00\n&lt;0.001\n\n\nRegister [Informative]\n0.20\n-0.09 – 0.49\n0.176\n\n\nRegister [Instructional]\n-0.23\n-0.43 – -0.03\n0.023\n\n\nRegister [Personal]\n0.49\n0.06 – 0.93\n0.026\n\n\nLevel [B]\n-0.67\n-0.87 – -0.48\n&lt;0.001\n\n\nLevel [C]\n-1.14\n-1.32 – -0.96\n&lt;0.001\n\n\nLevel [D]\n-1.42\n-1.61 – -1.23\n&lt;0.001\n\n\nLevel [E]\n-1.76\n-1.99 – -1.54\n&lt;0.001\n\n\nRegister [Fiction] ×Level [B]\n0.97\n0.61 – 1.33\n&lt;0.001\n\n\nRegister [Informative] ×Level [B]\n0.22\n-0.14 – 0.59\n0.230\n\n\nRegister [Instructional]× Level [B]\n0.21\n-0.06 – 0.47\n0.130\n\n\nRegister [Personal] ×Level [B]\n-0.02\n-0.57 – 0.52\n0.930\n\n\nRegister [Fiction] ×Level [C]\n0.85\n0.50 – 1.20\n&lt;0.001\n\n\nRegister [Informative] ×Level [C]\n0.25\n-0.09 – 0.59\n0.156\n\n\nRegister [Instructional]× Level [C]\n0.37\n0.11 – 0.62\n0.005\n\n\nRegister [Personal] ×Level [C]\n-0.26\n-0.80 – 0.28\n0.343\n\n\nRegister [Fiction] ×Level [D]\n0.65\n0.30 – 1.00\n&lt;0.001\n\n\nRegister [Informative] ×Level [D]\n0.34\n-0.00 – 0.68\n0.053\n\n\nRegister [Instructional]× Level [D]\n0.47\n0.20 – 0.73\n0.001\n\n\nRegister [Personal] ×Level [D]\n-0.43\n-1.01 – 0.14\n0.140\n\n\nRegister [Fiction] ×Level [E]\n0.82\n0.45 – 1.19\n&lt;0.001\n\n\nRegister [Informative] ×Level [E]\n0.42\n0.05 – 0.80\n0.027\n\n\nRegister [Instructional]× Level [E]\n0.44\n0.14 – 0.74\n0.004\n\n\nRegister [Personal] ×Level [E]\n-0.55\n-1.15 – 0.05\n0.072\n\n\nRandom Effects\n\n\nσ2\n\n0.52\n\n\nτ00Series\n\n0.05\n\n\n\nICC\n0.08\n\n\n\nN Series\n\n9\n\n\nObservations\n1961\n\n\nMarginal R2 / Conditional R2\n\n0.426 / 0.472\n\n\n\nCode# tab_model(md4Register) # Marginal R2 = 0.203\n# tab_model(md4Level) # Marginal R2 = 0.187\n\n# Plot of fixed effects:\nplot_model(md4, \n           type = \"est\",\n           show.intercept = TRUE,\n           show.values=TRUE, \n           show.p=TRUE,\n           value.offset = .4,\n           value.size = 3.5,\n           colors = palette[c(1:3,8,7)],\n           group.terms = c(1:5,1,1,1,1,2:5,2:5,2:5,2:5), \n           title = \"\",\n           wrap.labels = 40,\n           axis.title = \"PC4 estimated coefficients\") +\n  theme_sjplot2() \n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TxB_PCA4_lmer_fixedeffects.svg\"), height = 8, width = 8)\n\n\n\n# Plot of random effects:\nplot_model(md4, \n           type = \"re\", # Option to visualise random effects\n           show.values=TRUE, \n           show.p=TRUE,\n           value.offset = .4,\n           value.size = 3.5,\n           color = \"bw\",\n           wrap.labels = 40,\n           axis.title = \"PC4 estimated coefficients\") +\n  theme_sjplot2()\n\n\n\n\n\n\n#ggsave(here(\"plots\", \"TxB_PCA4_lmer_randomeffects.svg\"), height = 3, width = 8)\n\n\n# svg(here(\"plots\", \"TxB_predicted_PC4_scores_interactions.svg\"), height = 5, width = 8)\nvisreg(md4, xvar = \"Level\", by=\"Register\", type = \"conditional\",\n       line=list(col=\"darkred\"), \n       xlab = \"Textbook Level\", ylab = \"PC4\"\n       #,gg = TRUE\n       ,layout=c(5,1)\n)\n\n\n\n\n\n\n# dev.off()\n\n\nE.10.5 Testing model assumptions\nThis chunk can be used to check the assumptions of all of the models computed above. In the following example, we examine the final model selected to predict Dim2 scores.\n\nmodel2test &lt;- md2\n\n# check distribution of residuals\nplot(model2test)\n\n\n\n\n\n\n# scale-location plot\nplot(model2test,\n     sqrt(abs(resid(.)))~fitted(.),\n     type=c(\"p\",\"smooth\"), col.line=1)\n\n\n\n\n\n\n# Q-Q plot\nlattice::qqmath(model2test)\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe Foll, Elen. n.d. “Schulenglisch: A Multi-Dimensional Model of the Variety of English Taught in German Secondary Schools.” AAA: Arbeiten Aus Anglistik Und Amerikanistik 49.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Data Analysis for the Model of Intra-Textbook Variation</span>"
    ]
  },
  {
    "objectID": "AppendixF.html",
    "href": "AppendixF.html",
    "title": "Appendix F — Data Preparation for the Model of Textbook English vs. ‘real-world’ English",
    "section": "",
    "text": "F.1 Packages required\nThe following packages must be installed and loaded to process the data.\n#renv::restore() # Restore the project's dependencies from the lockfile to ensure that same package versions are used as in the original thesis.\n\nlibrary(broom.mixed) # For checking singularity issues \nlibrary(car) # For recoding data\nlibrary(corrplot) # For the feature correlation matrix\nlibrary(cowplot) # For nice plots\nlibrary(emmeans) # Comparing group means of predicted values\nlibrary(GGally) # For ggpairs\nlibrary(gridExtra) # For making large faceted plots\nlibrary(here) # For ease of sharing\nlibrary(knitr) # Loaded to display the tables using the kable() function\nlibrary(lme4) # For mixed effects modelling\nlibrary(psych) # For various useful stats function, including KMO()\nlibrary(scales) # For working with colours\nlibrary(sjPlot) # For nice tabular display of regression models\nlibrary(tidyverse) # For data wrangling and plotting\nlibrary(visreg) # For nice visualisations of model results\nselect &lt;- dplyr::select\nfilter &lt;- dplyr::filter",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Data Preparation for the Model of Textbook English vs. ‘real-world’ English</span>"
    ]
  },
  {
    "objectID": "AppendixF.html#data-import-from-mfte-outputs",
    "href": "AppendixF.html#data-import-from-mfte-outputs",
    "title": "Appendix F — Data Preparation for the Model of Textbook English vs. ‘real-world’ English",
    "section": "\nF.2 Data import from MFTE outputs",
    "text": "F.2 Data import from MFTE outputs\nThe raw data used in this script comes from the matrices of mixed normalised frequencies as output by the MFTE Perl v. 3.1 (Le Foll 2021a).\n\nF.2.1 Spoken BNC2014\n\nCodeSpokenBNC2014 &lt;- read.delim(here(\"data\", \"MFTE\", \"SpokenBNC2014_3.1_normed_complex_counts.tsv\"), header = TRUE, stringsAsFactors = TRUE)\n\nSpokenBNC2014$Series &lt;- \"Spoken BNC2014\"\nSpokenBNC2014$Level &lt;- \"Ref.\"\nSpokenBNC2014$Country &lt;- \"Spoken BNC2014\"\nSpokenBNC2014$Register &lt;- \"Spoken BNC2014\"\n\n\nThese normalised frequencies were computed on the basis of my own “John and Jill in Ivybridge” version of the Spoken BNC2014 with added full stops at speaker turns (see Appendix B for details). This corpus comprises of 1,251 texts, all of which were used in the following analyses.\n\nF.2.2 Youth Fiction corpus\n\nCodeYouthFiction &lt;- read.delim(here(\"data\", \"MFTE\", \"YF_sampled_500_3.1_normed_complex_counts.tsv\"), header = TRUE, stringsAsFactors = TRUE)\n\nYouthFiction$Series &lt;- \"Youth Fiction\"\nYouthFiction$Level &lt;- \"Ref.\"\nYouthFiction$Country &lt;- \"Youth Fiction\"\nYouthFiction$Register &lt;- \"Youth Fiction\"\n\n\nThese normalised frequencies were computed on the basis of the random samples of approximately 5,000 words of the books of the Youth Fiction corpus (for details of the works included in this corpus, see Appendix B). The sampling procedure is described in Section 4.3.2.4 of the book. This dataset consists of 1,191 files.\n\nF.2.3 Informative Texts for Teens (InfoTeens) corpus\n\nCodeInfoTeen &lt;- read.delim(here(\"data\", \"MFTE\", \"InfoTeen_3.1_normed_complex_counts.tsv\"), header = TRUE, stringsAsFactors = TRUE)\n\n# Removes three outlier files which should not have been included in the corpus as they contain exam papers only\nInfoTeen &lt;- InfoTeen |&gt; \n  filter(Filename!=\".DS_Store\" & Filename!=\"Revision_World_GCSE_10529068_wjec-level-law-past-papers.txt\" & Filename!=\"Revision_World_GCSE_10528474_wjec-level-history-past-papers.txt\" & Filename!=\"Revision_World_GCSE_10528472_edexcel-level-history-past-papers.txt\")\n\nInfoTeen$Series &lt;- \"Info Teens\"\nInfoTeen$Level &lt;- \"Ref.\"\nInfoTeen$Country &lt;- \"Info Teens\"\nInfoTeen$Register &lt;- \"Info Teens\"\n\n\nDetails of the composition of the Info Teens corpus can be found in Section 4.3.2.5 of the book. The version used in the present study comprises 1,411 texts.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Data Preparation for the Model of Textbook English vs. ‘real-world’ English</span>"
    ]
  },
  {
    "objectID": "AppendixF.html#merging-tec-and-reference-corpora-data",
    "href": "AppendixF.html#merging-tec-and-reference-corpora-data",
    "title": "Appendix F — Data Preparation for the Model of Textbook English vs. ‘real-world’ English",
    "section": "\nF.3 Merging TEC and reference corpora data",
    "text": "F.3 Merging TEC and reference corpora data\n\nF.3.1 Corpus size\nThese tables provide some summary statistics about the texts/files whose normalised feature frequencies were entered in the model of Textbook English vs. real-world English described in Chapter 7.\n\nCodesummary(ncounts$Subcorpus) |&gt; \n  kable(col.names = c(\"(Sub)corpus\", \"# texts\"),\n        format.args = list(big.mark = \",\"))\n\n\n\n(Sub)corpus\n# texts\n\n\n\nTextbook Conversation\n593\n\n\nTextbook Fiction\n285\n\n\nInfo Teens Ref.\n1,411\n\n\nTextbook Informative\n364\n\n\nSpoken BNC2014 Ref.\n1,251\n\n\nYouth Fiction Ref.\n1,191\n\n\n\n\nCodencounts  |&gt;  \n  group_by(Register) |&gt;  \n  summarise(totaltexts = n(), \n            totalwords = sum(Words), \n            mean = as.integer(mean(Words)), \n            sd = as.integer(sd(Words)), \n            TTRmean = mean(TTR)) |&gt;  \n  kable(digits = 2, \n        format.args = list(big.mark = \",\"),\n        col.names = c(\"Register\", \"# texts/files\", \"# words\", \"mean # words per text\", \"SD\", \"mean TTR\"))\n\n\n\n\n\n\n\n\n\n\n\nRegister\n# texts/files\n# words\nmean # words per text\nSD\nmean TTR\n\n\n\nConversation\n1,844\n13,804,196\n7,486\n8,690\n0.40\n\n\nFiction\n1,476\n7,321,747\n4,960\n2,022\n0.49\n\n\nInformative\n1,775\n1,436,732\n809\n188\n0.51",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Data Preparation for the Model of Textbook English vs. ‘real-world’ English</span>"
    ]
  },
  {
    "objectID": "AppendixF.html#data-preparation-for-pca",
    "href": "AppendixF.html#data-preparation-for-pca",
    "title": "Appendix F — Data Preparation for the Model of Textbook English vs. ‘real-world’ English",
    "section": "\nF.4 Data preparation for PCA",
    "text": "F.4 Data preparation for PCA\n\nF.4.1 Feature distributions\nThe distributions of each linguistic features were examined by means of visualisation. As shown below, before transformation, many of the features displayed highly skewed distributions.\n\nCode#ncounts &lt;- readRDS(here(\"data\", \"processed\", \"counts3Reg.rds\"))\n\nncounts |&gt;\n  select(-Words) |&gt; \n  keep(is.numeric) |&gt; \n  gather() |&gt; # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2\n  ggplot(aes(value, after_stat(density))) +\n    theme_bw() +\n    facet_wrap(~ key, scales = \"free\", ncol = 4) +\n    scale_x_continuous(expand=c(0,0)) +\n    scale_y_continuous(limits = c(0,NA)) +\n    geom_histogram(bins = 30, colour= \"black\", fill = \"grey\") +\n    geom_density(colour = \"darkred\", weight = 2, fill=\"darkred\", alpha = .4)\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"DensityPlotsAllVariables.svg\"), width = 15, height = 49)\n\n\n\nF.4.2 Feature removal\nA number of features were removed from the dataset as they are not linguistically interpretable. In the case of the TEC, this included the variable CD because numbers spelt out as digits were removed from the textbooks before these were tagged with the MFTE. In addition, the variables LIKE and SO because these are “bin” features included in the output of the MFTE to ensure that the counts for these polysemous words do not inflate other categories due to mistags (Le Foll 2021b).\nWhenever linguistically meaningful, very low-frequency features, features with low MSA or communalities (see chunks below) were merged. Finally, features absent from more than third of texts were also excluded. For the comparative analysis of TEC and the reference corpora, the following linguistic features were excluded from the analysis due to low dispersion:\n\nCode# Removal of meaningless feature: CD because numbers as digits were mostly removed from the textbooks, LIKE and SO because they are dustbin categories\nncounts &lt;- ncounts |&gt; \n  select(-c(CD, LIKE, SO))\n\n# Combine problematic features into meaningful groups whenever this makes linguistic sense\nncounts &lt;- ncounts |&gt; \n  mutate(JJPR = JJPR + ABLE, ABLE = NULL) |&gt; \n  mutate(PASS = PGET + PASS, PGET = NULL) |&gt; \n  mutate(TPP3 = TPP3S + TPP3P, TPP3P = NULL, TPP3S = NULL) |&gt; # Merged due to TTP3P having an individual MSA &lt; 0.5\n  mutate(FQTI = FREQ + TIME, FREQ = NULL, TIME = NULL) # Merged due to TIME communality &lt; 0.2 (see below)\n\n# Function to compute percentage of texts with occurrences meeting a condition\ncompute_percentage &lt;- function(data, condition, threshold) {\n  numeric_data &lt;- Filter(is.numeric, data)\n  percentage &lt;- round(colSums(condition[, sapply(numeric_data, is.numeric)])/nrow(data) * 100, 2)\n  percentage &lt;- as.data.frame(percentage)\n  colnames(percentage) &lt;- \"Percentage\"\n  percentage &lt;- percentage |&gt; \n    filter(!is.na(Percentage)) |&gt;\n    rownames_to_column() |&gt;\n    arrange(Percentage)\n  if (!missing(threshold)) {\n    percentage &lt;- percentage |&gt; \n      filter(Percentage &gt; threshold)\n  }\n  return(percentage)\n}\n\n# Calculate percentage of texts with 0 occurrences of each feature\nzero_features &lt;- compute_percentage(ncounts, ncounts == 0, 66.6)\nzero_features |&gt; \n  kable(col.names = c(\"Feature\", \"% texts with zero occurrences\"))\n\n\n\nFeature\n% texts with zero occurrences\n\n\n\nPRP\n85.34\n\n\nURL\n93.03\n\n\nEMO\n98.98\n\n\nHST\n99.55\n\n\n\n\nCode# Drop variables with low document frequency\nncounts2 &lt;- select(ncounts, -one_of(zero_features$rowname))\n\n\nThese feature removal operations resulted in a feature set of 71 linguistic variables.\n\nF.4.3 Identifying outlier texts\nAll normalised frequencies were normalised to identify any potential outlier texts.\n\n# First scale the normalised counts (z-standardisation) to be able to compare the various features\nzcounts &lt;- ncounts2 |&gt;\n  select(-Words) |&gt; \n  keep(is.numeric) |&gt; \n  scale()\n\n# If necessary, remove any outliers at this stage.\ndata &lt;- cbind(ncounts2[,1:8], as.data.frame(zcounts))\noutliers &lt;- data |&gt; \n filter(if_any(where(is.numeric) & !Words,  .fns = function(x){x &gt; 8}))  |&gt;\n  select(Filename, Corpus, Register, Words) \n\nThe following outlier texts were identified according to the above conditions and excluded in subsequent analyses.\n\nCode# These are potential outlier texts :\noutliers |&gt; \n  kable(col.names = c(\"Filename\", \"Corpus\", \"Register\", \"# words\"))\n\n\n\n\n\n\n\n\n\nFilename\nCorpus\nRegister\n# words\n\n\n\nPOC_4e_Spoken_0007.txt\nTextbook.English\nConversation\n750\n\n\nSolutions_Elementary_ELF_Spoken_0013.txt\nTextbook.English\nConversation\n931\n\n\nEIM_Starter_Informative_0004.txt\nTextbook.English\nInformative\n534\n\n\nGreenLine_1_Spoken_0003.txt\nTextbook.English\nConversation\n970\n\n\nAccess_1_Spoken_0011.txt\nTextbook.English\nConversation\n784\n\n\nAchievers_B1_Informative_0003.txt\nTextbook.English\nInformative\n926\n\n\nEIM_Starter_Spoken_0002.txt\nTextbook.English\nConversation\n824\n\n\nGreenLine_1_Spoken_0008.txt\nTextbook.English\nConversation\n876\n\n\nJTT_3_Informative_0003.txt\nTextbook.English\nInformative\n699\n\n\nGreenLine_1_Spoken_0010.txt\nTextbook.English\nConversation\n701\n\n\nEIM_1_Spoken_0012.txt\nTextbook.English\nConversation\n640\n\n\nNGL_1_Spoken_0013.txt\nTextbook.English\nConversation\n940\n\n\nNGL_3_Spoken_0018.txt\nTextbook.English\nConversation\n751\n\n\nSolutions_Intermediate_Spoken_0029.txt\nTextbook.English\nConversation\n672\n\n\nNGL_1_Spoken_0012.txt\nTextbook.English\nConversation\n910\n\n\nGreenLine_1_Spoken_0006.txt\nTextbook.English\nConversation\n622\n\n\nGreenLine_2_Spoken_0004.txt\nTextbook.English\nConversation\n1102\n\n\nAccess_2_Spoken_0023.txt\nTextbook.English\nConversation\n875\n\n\nHT_4_Informative_0006.txt\nTextbook.English\nInformative\n513\n\n\nSolutions_Intermediate_Informative_0017.txt\nTextbook.English\nInformative\n816\n\n\nEIM_1_Spoken_0013.txt\nTextbook.English\nConversation\n967\n\n\nSolutions_Elementary_ELF_Spoken_0021.txt\nTextbook.English\nConversation\n846\n\n\nSolutions_Intermediate_Plus_Spoken_0022.txt\nTextbook.English\nConversation\n596\n\n\nAccess_2_Spoken_0028.txt\nTextbook.English\nConversation\n813\n\n\nNGL_1_Spoken_0005.txt\nTextbook.English\nConversation\n1020\n\n\nSolutions_Elementary_ELF_Spoken_0016.txt\nTextbook.English\nConversation\n871\n\n\nSolutions_Pre-Intermediate_ELF_Spoken_0007.txt\nTextbook.English\nConversation\n630\n\n\nSolutions_Intermediate_Informative_0013.txt\nTextbook.English\nInformative\n770\n\n\nGreenLine_2_Spoken_0003.txt\nTextbook.English\nConversation\n850\n\n\nHT_4_Spoken_0010.txt\nTextbook.English\nConversation\n727\n\n\nSolutions_Elementary_Informative_0003.txt\nTextbook.English\nInformative\n1051\n\n\nAccess_2_Informative_0001.txt\nTextbook.English\nInformative\n655\n\n\nSolutions_Elementary_Informative_0010.txt\nTextbook.English\nInformative\n708\n\n\nGreenLine_1_Informative_0001.txt\nTextbook.English\nInformative\n731\n\n\nAccess_2_Spoken_0002.txt\nTextbook.English\nConversation\n572\n\n\nSolutions_Intermediate_Spoken_0019.txt\nTextbook.English\nConversation\n1024\n\n\nAccess_3_Informative_0003.txt\nTextbook.English\nInformative\n1000\n\n\nAccess_1_Spoken_0019.txt\nTextbook.English\nConversation\n701\n\n\nAccess_2_Spoken_0013.txt\nTextbook.English\nConversation\n981\n\n\nSolutions_Intermediate_Plus_Informative_0014.txt\nTextbook.English\nInformative\n537\n\n\nRevision_World_GCSE_10525362_literary-terms.txt\nInformative.Teens\nInformative\n790\n\n\nRevision_World_GCSE_10528697_p6-physics-radioactive-materials.txt\nInformative.Teens\nInformative\n1015\n\n\nScience_Tech_Kinds_NZ_10382383_math.txt\nInformative.Teens\nInformative\n522\n\n\nScience_for_students_10064820_scientists-say-metabolism.txt\nInformative.Teens\nInformative\n895\n\n\nScience_Tech_Kinds_NZ_10382388_recycling.txt\nInformative.Teens\nInformative\n666\n\n\nHistory_Kids_BBC_10404337_go_furthers.txt\nInformative.Teens\nInformative\n620\n\n\nScience_Tech_Kinds_NZ_10382391_sports.txt\nInformative.Teens\nInformative\n657\n\n\nTeen_Kids_News_10402607_so-you-want-to-be-an-archivist.txt\nInformative.Teens\nInformative\n763\n\n\nScience_Tech_Kinds_NZ_10382234_biology.txt\nInformative.Teens\nInformative\n843\n\n\nScience_Tech_Kinds_NZ_10382372_astronomy.txt\nInformative.Teens\nInformative\n900\n\n\nDogo_News_file10060404_banana-plant-extract-may-be-the-key-to-slower-melting-ice-cream.txt\nInformative.Teens\nInformative\n611\n\n\nScience_Tech_Kinds_NZ_10382667_countries.txt\nInformative.Teens\nInformative\n717\n\n\nQuatr_us_file10390777_quick-summary-geological-erashtm.txt\nInformative.Teens\nInformative\n643\n\n\nScience_Tech_Kinds_NZ_10382873_physics.txt\nInformative.Teens\nInformative\n722\n\n\nScience_Tech_Kinds_NZ_10382382_light.txt\nInformative.Teens\nInformative\n639\n\n\nFactmonster_10053687_august-13.txt\nInformative.Teens\nInformative\n523\n\n\nRevision_World_GCSE_10526703_limited-companies.txt\nInformative.Teens\nInformative\n714\n\n\nRevision_World_GCSE_10529637_transition-metals.txt\nInformative.Teens\nInformative\n787\n\n\nQuatr_us_10390856_early-african-historyhtm.txt\nInformative.Teens\nInformative\n1136\n\n\nHistory_Kids_BBC_10401873_ff6_sicilylandingss.txt\nInformative.Teens\nInformative\n813\n\n\nQuatr_us_10394250_harappan.txt\nInformative.Teens\nInformative\n651\n\n\nDucksters_10398301_iraqphp.txt\nInformative.Teens\nInformative\n657\n\n\nHistory_Kids_BBC_10403171_death_sakkara_gallery_04s.txt\nInformative.Teens\nInformative\n844\n\n\nRevision_World_GCSE_10528246_agricultural-change.txt\nInformative.Teens\nInformative\n789\n\n\nRevision_World_GCSE_10528086_uk-government-judiciary.txt\nInformative.Teens\nInformative\n1019\n\n\nRevision_World_GCSE_10529794_definitions.txt\nInformative.Teens\nInformative\n904\n\n\nEncyclopedia_Kinds_au_10085347_Nobel_Prize_in_Chemistry.txt\nInformative.Teens\nInformative\n598\n\n\nScience_for_students_10064875_questions-big-melt-earths-ice-sheets-are-under-attack.txt\nInformative.Teens\nInformative\n685\n\n\nTeen_Kids_News_10403301_golden-globe-winners-2019-the-complete-list.txt\nInformative.Teens\nInformative\n800\n\n\nScience_Tech_Kinds_NZ_10382201_projects.txt\nInformative.Teens\nInformative\n947\n\n\nRevision_World_GCSE_10529753_probability.txt\nInformative.Teens\nInformative\n816\n\n\nEncyclopedia_Kinds_au_10085531_Complex_analysis.txt\nInformative.Teens\nInformative\n735\n\n\nHistory_Kids_BBC_10401890_ff7_ddays.txt\nInformative.Teens\nInformative\n759\n\n\nHistory_Kids_BBC_10403434s.txt\nInformative.Teens\nInformative\n732\n\n\nHistory_Kids_BBC_10401872_ff6_italys.txt\nInformative.Teens\nInformative\n786\n\n\nScience_Tech_Kinds_NZ_10382371_amazing.txt\nInformative.Teens\nInformative\n629\n\n\nQuatr_us_10391129_athabascan.txt\nInformative.Teens\nInformative\n637\n\n\nEncyclopedia_Kinds_au_10085355_20th_century.txt\nInformative.Teens\nInformative\n864\n\n\nDogo_News_10060755_luxury-space-hotel-promises-guests-a-truly-out-of-this-world-vacation.txt\nInformative.Teens\nInformative\n722\n\n\nRevision_World_GCSE_10528072_nationalism-practice.txt\nInformative.Teens\nInformative\n776\n\n\nQuatr_us_10390861_quatr-us-privacy-policyhtm.txt\nInformative.Teens\nInformative\n960\n\n\nHistory_Kids_BBC_10401909_ff7_bulges.txt\nInformative.Teens\nInformative\n732\n\n\nHistory_kids_10381259_timeline-of-mesopotamia.txt\nInformative.Teens\nInformative\n768\n\n\nRevision_World_GCSE_10528123_gender-written-textual-analysis-framework.txt\nInformative.Teens\nInformative\n905\n\n\nScience_Tech_Kinds_NZ_10386406_floods.txt\nInformative.Teens\nInformative\n580\n\n\nRevision_World_GCSE_10529693_advantages.txt\nInformative.Teens\nInformative\n782\n\n\nScience_Tech_Kinds_NZ_10382378_geography.txt\nInformative.Teens\nInformative\n761\n\n\nScience_Tech_Kinds_NZ_10382374_earth.txt\nInformative.Teens\nInformative\n726\n\n\nScience_for_students_10066286_watering-plants-wastewater-can-spread-germs.txt\nInformative.Teens\nInformative\n836\n\n\nScience_Tech_Kinds_NZ_10382393_water.txt\nInformative.Teens\nInformative\n856\n\n\nWorld_Dteen_10406069_website_policies.txt\nInformative.Teens\nInformative\n995\n\n\nScience_Tech_Kinds_NZ_10382384_metals.txt\nInformative.Teens\nInformative\n669\n\n\nDogo_News_10062028_puppy-bowl-14-promises-viewers-a-paw-some-time-on-super-bowl-sunday.txt\nInformative.Teens\nInformative\n581\n\n\nHistory_Kids_BBC_10404730_go_furthers.txt\nInformative.Teens\nInformative\n611\n\n\nScience_Tech_Kinds_NZ_10382385_nature.txt\nInformative.Teens\nInformative\n722\n\n\nScience_for_students_10065015_scientists-say-dna-sequencing.txt\nInformative.Teens\nInformative\n953\n\n\nQuatr_us_file10390817_conifers-pine-trees-gymnospermshtm.txt\nInformative.Teens\nInformative\n533\n\n\nTweenTribute_10051509_it-true-elephants-cant-jump.txt\nInformative.Teens\nInformative\n790\n\n\nRevision_World_GCSE_10528494_application-software.txt\nInformative.Teens\nInformative\n855\n\n\nRevision_World_GCSE_10529581_different-types-questions-examinations.txt\nInformative.Teens\nInformative\n742\n\n\nDogo_News_10061669_the-chinese-city-of-chengdu-may-soon-be-home-to-multiple-moons.txt\nInformative.Teens\nInformative\n614\n\n\nDucksters_10398306_geography_of_ancient_chinaphp.txt\nInformative.Teens\nInformative\n638\n\n\nScience_for_students_10065144_scientists-say-multiverse.txt\nInformative.Teens\nInformative\n712\n\n\nScience_Tech_Kinds_NZ_10382211_images.txt\nInformative.Teens\nInformative\n793\n\n\nFactmonster_10053754_may-18.txt\nInformative.Teens\nInformative\n497\n\n\nWorld_Dteen_10406047_AboutWORLDteen.txt\nInformative.Teens\nInformative\n1053\n\n\nDucksters_10398078_first_new_dealphp.txt\nInformative.Teens\nInformative\n649\n\n\nRevision_World_GCSE_10526926_economies-scale.txt\nInformative.Teens\nInformative\n621\n\n\nFactmonster_10053201_september-03.txt\nInformative.Teens\nInformative\n445\n\n\nScience_Tech_Kinds_NZ_10387183_calciumcarbonates.txt\nInformative.Teens\nInformative\n804\n\n\nScience_Tech_Kinds_NZ_10382380_health.txt\nInformative.Teens\nInformative\n694\n\n\nRevision_World_GCSE_10529587_sources-finance.txt\nInformative.Teens\nInformative\n665\n\n\nQuatr_us_10393444_fishing.txt\nInformative.Teens\nInformative\n656\n\n\nDucksters_10398315_glossary_and_termsphp.txt\nInformative.Teens\nInformative\n684\n\n\nS5AA.txt\nSpoken.BNC2014\nConversation\n1869\n\n\n\n\n\nWe check that that outlier texts are not particularly long or short texts by looking at the distribution of text/file length of the outliers.\n\nCodesummary(outliers$Words)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  445.0   655.5   751.0   773.6   860.0  1869.0 \n\nCodehist(outliers$Words, breaks = 30)\n\n\n\n\n\n\n\nWe also check the distribution of outlier texts across the four corpora. The majority come from the Info Teens corpus, though quite a few are also from the TEC.\n\nCodesummary(outliers$Corpus) |&gt; \n  kable(col.names = c(\"(Sub)corpus\", \"# outlier texts\"))\n\n\n\n(Sub)corpus\n# outlier texts\n\n\n\nTextbook.English\n40\n\n\nInformative.Teens\n74\n\n\nSpoken.BNC2014\n1\n\n\nYouth.Fiction\n0\n\n\n\n\nCode# Report on the manual check of a sample of these outliers:\n\n# Encyclopedia_Kinds_au_10085347_Nobel_Prize_in_Chemistry.txt is essentially a list of Nobel prize winners but with some additional information. In other words, not a bad representative of the type of texts of the Info Teen corpus.\n# Solutions_Elementary_ELF_Spoken_0013 --&gt; Has a lot of \"going to\" constructions because they are learnt in this chapter but is otherwise a well-formed text.\n# Teen_Kids_News_10403972_a-brief-history-of-white-house-weddings --&gt; No issues\n# Teen_Kids_News_10403301_golden-globe-winners-2019-the-complete-list --&gt; Similar to the Nobel prize laureates text.\n# Revision_World_GCSE_10528123_gender-written-textual-analysis-framework --&gt; Text includes bullet points tokenised as the letter \"o\" but otherwise a fairly typical informative text.\n\n# Removing the outliers at the request of the reviewers (but comparisons of models including the outliers showed that the results are very similar):\nncounts3 &lt;- ncounts2 |&gt; \n  filter(!Filename %in% outliers$Filename)\n\n#saveRDS(ncounts3, here(\"data\", \"processed\", \"ncounts3_3Reg.rds\")) # Last saved 6 March 2024\n\n\nThis resulted in 4,980 texts/files being included in the comparative model of Textbook English vs. ‘real-world’ English. These standardised feature frequencies were distributed as follows:\n\nCodezcounts3 &lt;- ncounts3 |&gt;\n  select(-Words) |&gt; \n  keep(is.numeric) |&gt; \n  scale()\n\nboxplot(zcounts3, las = 3, main = \"z-scores\") # Slow\n\n\n\n\n\n\n\n\nF.4.4 Signed log transformation\nA signed logarithmic transformation was applied to (further) deskew the feature distributions (see Diwersy, Evert, and Neumann 2014; Neumann and Evert 2021).\nThe signed log transformation function was inspired by the SignedLog function proposed in https://cran.r-project.org/web/packages/DataVisualizations/DataVisualizations.pdf.\n\nsigned.log &lt;- function(x) {sign(x)*log(abs(x)+1)}\n\n# Standardise first, then sign log transform\nzlogcounts &lt;- signed.log(zcounts3) \n\nThe new feature distributions are visualised below.\n\nCodezlogcounts |&gt;\n  as.data.frame() |&gt; \n  gather() |&gt; # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2\n  ggplot(aes(value, after_stat(density))) +\n  theme_bw() +\n  facet_wrap(~ key, scales = \"free\", ncol = 4) +\n  scale_x_continuous(expand=c(0,0)) +\n  scale_y_continuous(limits = c(0,NA)) +\n  geom_histogram(bins = 30, colour= \"black\", fill = \"grey\") +\n  geom_density(colour = \"darkred\", weight = 2, fill=\"darkred\", alpha = .4)\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"DensityPlotsAllVariablesSignedLog.svg\"), width = 15, height = 49)\n\n\n\nF.4.5 Merging of data for MDA\n\nCodezlogcounts &lt;- readRDS(here(\"data\", \"processed\", \"zlogcounts_3Reg.rds\")) \n#nrow(zlogcounts)\n#colnames(zlogcounts)\n\nncounts3 &lt;- readRDS(here(\"data\", \"processed\", \"ncounts3_3Reg.rds\"))\n#nrow(ncounts3)\n#colnames(ncounts3)\n\ndata &lt;- cbind(ncounts3[,1:8], as.data.frame(zlogcounts))\n#saveRDS(data, here(\"data\", \"processed\", \"datazlogcounts_3Reg.rds\")) # Last saved 16 March 2024\n\n\nThe final dataset comprises of 4,980 texts/files, divided as follows:\n\n\n\n\n(Sub)corpus\n# texts/files\n\n\n\nTextbook Conversation\n565\n\n\nTextbook Fiction\n285\n\n\nInfo Teens Ref.\n1337\n\n\nTextbook Informative\n352\n\n\nSpoken BNC2014 Ref.\n1250\n\n\nYouth Fiction Ref.\n1191",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Data Preparation for the Model of Textbook English vs. ‘real-world’ English</span>"
    ]
  },
  {
    "objectID": "AppendixF.html#testing-factorability-of-data",
    "href": "AppendixF.html#testing-factorability-of-data",
    "title": "Appendix F — Data Preparation for the Model of Textbook English vs. ‘real-world’ English",
    "section": "\nF.5 Testing factorability of data",
    "text": "F.5 Testing factorability of data\n\nF.5.1 Visualisation of feature correlations\nWe begin by visualising the correlations of the transformed feature frequencies using the heatmap function of the stats library. Negative correlations are rendered in blue; positive ones are in red.\n\nCode# Simple heatmap in base R (inspired by Stephanie Evert's SIGIL code)\ncor.colours &lt;- c(\n  hsv(h=2/3, v=1, s=(10:1)/10), # blue = negative correlation \n  rgb(1,1,1), # white = no correlation \n  hsv(h=0, v=1, s=(1:10/10))) # red = positive correlation\n\n#png(here(\"plots\", \"heatmapzlogcounts.png\"), width = 30, height= 30, units = \"cm\", res = 300)\nheatmap(cor(zlogcounts), \n        symm=TRUE, \n        zlim=c(-1,1), \n        col=cor.colours, \n        margins=c(7,7))\n\n\n\n\n\n\nCode#dev.off()\n\n\n\nF.5.2 Collinearity\nAs a result of the normalisation unit of finite verb phrases for verb-based features, the present tense (VPRT) and past tense (VBD) variables are correlated to a very high degree:\n\ncor(data$VPRT, data$VBD) |&gt; round(2)\n\n[1] -0.97\n\n\nWe therefore remove the least marked of the pair of collinear variables: VPRT.\n\ndata &lt;- data |&gt; \n  select(-c(VPRT))\n\n\nF.5.3 MSA\n\nkmo &lt;- KMO(data[,9:ncol(data)]) # The first eight columns contain metadata.\n\nThe overall MSA value of the dataset is 0.95. The features have the following individual MSA values (ordered from lowest to largest):\n\nCodekmo$MSAi[order(kmo$MSAi)] |&gt;  round(2)\n\n   AMP   COMM    POS   TPP3   JJPR  PLACE  SPLIT     DT   JJAT   VIMP   MDCO \n  0.67   0.69   0.70   0.74   0.76   0.82   0.83   0.83   0.84   0.84   0.85 \n    RP     EX   THSC     LD  NCOMP   BEMA   MDWS   FQTI  FPP1P   MDCA    ACT \n  0.85   0.85   0.86   0.87   0.88   0.88   0.89   0.89   0.89   0.89   0.89 \nMENTAL    VBD  FPP1S   MDMM   PEAS   CONC   MDWO   THRC     NN   COND   PROG \n  0.91   0.91   0.91   0.91   0.91   0.93   0.93   0.94   0.94   0.95   0.95 \n    CC   SPP2     RB   DWNT   MDNE   WHSC   CONT   QUPR    XX0  CAUSE   WHQU \n  0.95   0.95   0.95   0.95   0.95   0.96   0.96   0.96   0.96   0.96   0.96 \n   VBG    AWL POLITE   PASS    PIT  DOAUX   ELAB ASPECT    DMA   DEMO    HDG \n  0.96   0.96   0.96   0.96   0.97   0.97   0.97   0.97   0.97   0.97   0.97 \n    IN   FPUH  OCCUR    CUZ   EMPH   YNQU   QUAN    TTR  QUTAG  THATD    VBN \n  0.97   0.97   0.97   0.97   0.98   0.98   0.98   0.98   0.98   0.98   0.98 \n EXIST   STPR    GTO   HGOT \n  0.98   0.99   0.99   0.99 \n\n\nWe aim to remove features with an individual MSA &lt; 0.5. All features have individual MSAs of &gt; 0.5 (but only because TPP3P was merged into a broader category in an earlier chunk).\n\nF.5.4 Scree plot\nSix components were originally retained on the basis of the following screeplot, though only the first four were found to be interpretable and were therefore included in the model.\n\nCode# png(here(\"plots\", \"screeplot-TEC-Ref_3Reg.png\"), width = 20, height= 12, units = \"cm\", res = 300)\nscree(data[,9:ncol(data)], factors = FALSE, pc = TRUE) # \n\n\n\n\n\n\nCode# dev.off()\n\n# Perform PCA\npca1 &lt;- psych::principal(data[9:ncol(data)], \n                         nfactors = 6)\n\n\n\nF.5.5 Communalities\nIf features with final communalities of &lt; 0.2 are removed, TIME would have to be removed. TIME was therefore merged with FREQ in an earlier chunk so that now all features have final communalities of &gt; 0.2 (note that this is a very generous threshold!).\n\nCodepca1$communality |&gt; sort() |&gt; round(2)\n\n  DWNT   STPR   CONC   FQTI    POS ASPECT   MDNE  FPP1P   PROG   MDCO   MDMM \n  0.22   0.23   0.23   0.23   0.24   0.25   0.27   0.28   0.29   0.32   0.32 \n  MDWO  SPLIT   MDWS   PEAS   QUPR    AMP  PLACE    HDG   COMM  CAUSE     EX \n  0.32   0.33   0.34   0.35   0.35   0.35   0.37   0.38   0.38   0.38   0.38 \n  THSC  OCCUR   WHSC   THRC   JJAT   COND MENTAL    ACT   VIMP   ELAB  EXIST \n  0.40   0.40   0.42   0.43   0.44   0.44   0.45   0.45   0.46   0.46   0.46 \n  JJPR  NCOMP     RP    GTO   DEMO   MDCA POLITE    CUZ     CC   WHQU   TPP3 \n  0.46   0.48   0.49   0.50   0.50   0.52   0.52   0.53   0.57   0.58   0.58 \n   VBG  THATD    PIT   BEMA  FPP1S     DT   HGOT     RB    VBN  QUTAG   EMPH \n  0.60   0.60   0.61   0.61   0.61   0.61   0.62   0.62   0.64   0.64   0.64 \n  PASS    XX0   QUAN   SPP2  DOAUX    TTR   YNQU    VBD     LD   FPUH     IN \n  0.65   0.65   0.67   0.68   0.69   0.71   0.74   0.78   0.81   0.83   0.86 \n  CONT    DMA    AWL     NN \n  0.89   0.89   0.91   0.93 \n\nCode#saveRDS(data, here(\"data\", \"processed\", \"dataforPCA.rds\")) # Last saved on 6 March 2024\n\n\nThe final dataset entered in the analysis described in Chapter 7 therefore comprises 4,980 texts/files, each with logged standardised normalised frequencies for 70 linguistic features.\n\n\n\n\n\n\nDiwersy, Sascha, Stephanie Evert, and Stella Neumann. 2014. “A Weakly Supervised Multivariate Approach to the Study of Language Variation.” In, edited by Benedikt Szmrecsanyi and Bernhard Wälchli, 174–204. Berlin: De Gruyter.\n\n\nLe Foll, Elen. 2021a. Introducing the Multi-Feature Tagger of English (MFTE). Osnabrück University. https://github.com/elenlefoll/MultiFeatureTaggerEnglish.\n\n\n———. 2021b. Introducing the Multi-Feature Tagger of English (MFTE). Osnabrück University. https://github.com/elenlefoll/MultiFeatureTaggerEnglish.\n\n\nNeumann, Stella, and Stephanie Evert. 2021. “A Register Variation Perspective on Varieties of English.” In, edited by Elena Seoane and Douglas Biber, 144178. Studies in Corpus Linguistics 103. Amsterdam: Benjamins.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Data Preparation for the Model of Textbook English vs. ‘real-world’ English</span>"
    ]
  },
  {
    "objectID": "AppendixG.html",
    "href": "AppendixG.html",
    "title": "Appendix G — Data Analysis for the Model of Textbook English vs. ‘real-world’ English",
    "section": "",
    "text": "G.1 Packages required\nThe following packages must be installed and loaded to process the data.\n#renv::restore() # Restore the project's dependencies from the lockfile to ensure that same package versions are used as in the original thesis.\n\nlibrary(caret) # For its confusion matrix function\nlibrary(cowplot) # For its plot themes\nlibrary(DescTools) # For 95% CI\nlibrary(emmeans) # For the emmeans function\nlibrary(factoextra) # For circular graphs of variables\nlibrary(gtsummary) # For nice table of summary statistics (optional)\nlibrary(gridExtra) # For Fig. 35\nlibrary(here) # For dynamic file paths\nlibrary(ggthemes) # For theme of factoextra plots\nlibrary(knitr) # Loaded to display the tables using the kable() function\nlibrary(lme4) # For linear regression modelling\nlibrary(patchwork) # To create figures with more than one plot\n#library(pca3d) # For 3-D plots (not rendered in exports)\nlibrary(PCAtools) # For nice biplots of PCA results\nlibrary(psych) # For various useful stats function\nlibrary(sjPlot) # For model plots and tables\nlibrary(tidyverse) # For data wrangling\nlibrary(visreg) # For plots of interaction effects\n\nsource(here(\"R_rainclouds.R\")) # For geom_flat_violin rainplots",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Analysis for the Model of Textbook English vs. ‘real-world’ English</span>"
    ]
  },
  {
    "objectID": "AppendixG.html#conducting-the-pca",
    "href": "AppendixG.html#conducting-the-pca",
    "title": "Appendix G — Data Analysis for the Model of Textbook English vs. ‘real-world’ English",
    "section": "\nG.2 Conducting the PCA",
    "text": "G.2 Conducting the PCA\nWe first import the full dataset (see Appendix F for data preparation steps).\nThe following chunks can be used to perform the MDA on various subsets of the data (see also Section 10.1.1 in the book).\n\nSubset of the data that excludes the lower-level textbooks:\n\n\ndata &lt;- readRDS(here(\"processed_data\", \"dataforPCA.rds\")) |&gt;\n  filter(Level !=\"A\" & Level != \"B\") |&gt;\n  droplevels()\nsummary(data$Level)\n\n\nSubset of the data that includes only one Country` subcorpus of the TEC (note that a detailed analysis of the German subcorpus can be found in (Le Foll, n.d.)):\n\n\ndata &lt;- readRDS(here(\"processed_data\", \"dataforPCA.rds\")) |&gt;\n  #filter(Country != \"France\" & Country != \"Germany\") |&gt; # Spain only\n  #filter(Country != \"France\" & Country != \"Spain\") |&gt; # Germany only\n  filter(Country != \"Spain\" & Country != \"Germany\") |&gt; # France only\n  droplevels()\nsummary(data$Country)\n\n\nRandom subsets of the data to test the stability of the model proposed in Chapter 7. Re-running this line will generate a new subset of 2/3 of the texts randomly sampled. set.seed(13) was used for the analyses reported on in Section 10.1.1.\n\n\nset.seed(13) \ndata &lt;- readRDS(here(\"processed_data\", \"dataforPCA.rds\")) |&gt;\n  slice_sample(n = 4980*0.6, replace = FALSE)\nnrow(data)\ndata$Filename[1:4]\n#Using the set.seed(13), these should be:\n#[1] HT_4_Spoken_0009.txt                       \n#[2] Solutions_Intermediate_Plus_Spoken_0020.txt\n#[3] 141_PRATCHETT1992DW13GODS_4.txt            \n#[4] Achievers_B2_Informative_0004.txt",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Analysis for the Model of Textbook English vs. ‘real-world’ English</span>"
    ]
  },
  {
    "objectID": "AppendixG.html#plotting-pca-results",
    "href": "AppendixG.html#plotting-pca-results",
    "title": "Appendix G — Data Analysis for the Model of Textbook English vs. ‘real-world’ English",
    "section": "\nG.3 Plotting PCA results",
    "text": "G.3 Plotting PCA results\n\nG.3.1 3D plots\nThe following chunk can be used to create projections of TEC texts on three dimensions of the model. These plots cannot be rendered in two dimensions and are therefore not generated in the present document. For more information on the pca3d library, see: https://cran.r-project.org/web/packages/pca3d/vignettes/pca3d.pdf.\n\n# Data preparation for 3D plots\ncolnames(data) # Checking that the features start at the 9th column\npca &lt;- prcomp(data[,9:ncol(data)], scale.=FALSE) # All quantitative variables that contribute to the model\nregister &lt;- factor(data[,\"Register\"]) \ncorpus &lt;- factor(data[,\"Corpus\"])\nsubcorpus &lt;- factor(data[,\"Subcorpus\"])\n\nlibrary(pca3d)\n\npca3d(pca, group = subcorpus,\n       components = 1:3,\n       components = 4:6,\n       show.plane=FALSE,\n       col = col6,\n       shape = shapes6,\n       radius = 0.7,\n       legend = \"right\")\n\nsnapshotPCA3d(here(\"plots\", \"PCA_TxB_3Ref_3Dsnapshot.png\"))\n\n# Alternative visualisation, looking at all three Textbook English registers in one colour\n\npca3d(pca, group = corpus, \n      show.plane=FALSE,\n      components = 1:3,\n      col = col4,\n      shape = shapes4,\n      radius = 0.7,\n      legend = \"right\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Analysis for the Model of Textbook English vs. ‘real-world’ English</span>"
    ]
  },
  {
    "objectID": "AppendixG.html#two-dimensional-plots-biplots",
    "href": "AppendixG.html#two-dimensional-plots-biplots",
    "title": "Appendix G — Data Analysis for the Model of Textbook English vs. ‘real-world’ English",
    "section": "\nG.4 Two-dimensional plots (biplots)",
    "text": "G.4 Two-dimensional plots (biplots)\nThese plots were generated using the PCAtools package, which requires the data to be formatted in a rather unconventional way so it needs to wrangled first.\n\nG.4.1 Data wrangling for PCAtools\n\nCode# Data wrangling\ndata2 &lt;- data |&gt; \n  mutate(Source = recode_factor(Corpus, Textbook.English = \"Textbook English (TEC)\", Informative.Teens = \"Reference corpora\", Spoken.BNC2014 = \"Reference corpora\", Youth.Fiction = \"Reference corpora\")) |&gt; \n  mutate(Corpus = fct_relevel(Subcorpus, \"Info Teens Ref.\", after = 9)) |&gt;\n  relocate(Source, .after = \"Corpus\") |&gt; \n  droplevels()\n\n# colnames(data2)\ndata2meta &lt;- data2[,1:9]\nrownames(data2meta) &lt;- data2meta$Filename\ndata2meta &lt;- data2meta |&gt; select(-Filename)\n# head(data2meta)\nrownames(data2) &lt;- data2$Filename\ndata2num &lt;- as.data.frame(base::t(data2[,10:ncol(data2)]))\n# data2num[1:5,1:5] # Check data frame format is correct by comparing to output of head(data2meta) above\n\np &lt;- PCAtools::pca(data2num, \n         metadata = data2meta,\n         scale = FALSE)\n\n\nThe cumulative proportion of variance in the dataset explained the first four components explain 47.15%.\n\nG.4.2 Pairs plot\nThis chunk produces a scatterplot matrix of combinations of the four dimensions of the model of Textbook English vs. ‘real-world’ English. Note that the number before the comma on each axis label shows which principal component is plotted on that axis; this is followed by the percentage of the total variance explained by that particular component. The colours correspond to the text registers.\n\nCode# For five TEC registers\n# colkey = c(`Spoken BNC2014 Ref.`=\"#BD241E\", `Info Teens Ref.`=\"#15274D\", `Youth Fiction Ref.`=\"#267226\", `Textbook Fiction`=\"#A18A33\", `Textbook Conversation`=\"#F9B921\", `Textbook Informative` = \"#722672\", `Textbook Instructional` = \"grey\", `Textbook Personal` = \"black\")\n\n# For three TEC registers\n# summary(data2$Corpus)\ncolkey = c(`Spoken BNC2014 Ref.`=\"#BD241E\", `Info Teens Ref.`=\"#15274D\", `Youth Fiction Ref.`=\"#267226\", `Textbook Fiction`=\"#A18A33\", `Textbook Conversation`=\"#F9B921\", `Textbook Informative` = \"#722672\")\n\n#summary(data2$Source)\n#shapekey = c(`Textbook English (TEC)`=6, `Reference corpora`=1)\n\n# summary(data2$Level)\nshapekey = c(A=1, B=2, C=6, D=0, E=5, `Ref.`=4)\n\n## Warning: this can be very slow! Open in extra zoomed out window!\n\n#png(here(\"plots\", \"PCA_3Ref_pairsplot.png\"), width = 12, height= 19, units = \"cm\", res = 300)\nPCAtools::pairsplot(p,\n                 triangle = FALSE,\n                 components = 1:4,\n                 ncol = 2,\n                 nrow = 3,\n                 pointSize = 0.6,\n                 shape = \"Level\",\n                 shapekey = shapekey,\n                 lab = NULL, # Otherwise will try to label each data point!\n                 colby = \"Corpus\",\n                 legendPosition = \"none\",\n                 margingaps = unit(c(0.2, 0.2, 0.8, 0.2), \"cm\"),\n                 colkey = colkey)\n\n\n\n\n\n\nCode#dev.off()\n#ggsave(here(\"plots\", \"PCA_TxB_pairsplot.svg\"), width = 6, height = 10)\n# Note that the legend has to be added manually (it was taken from the biplot code below).\n\n\n\nG.4.3 Bi-plots\nBiplots are used to more closely examine the position of texts on just two dimensions.\n\nCode# These settings (with legendPosition = \"top\") were used to generate the legend for the scatterplot matrix above:\n#png(here(\"plots\", \"PCA_3Ref_Biplot_PC1_PC2test.png\"), width = 40, height= 25, units = \"cm\", res = 300) \n\nPCAtools::biplot(p,\n                 x = \"PC1\",\n                 y = \"PC2\",\n                 lab = NULL, # Otherwise will try to label each data point!\n                 colby = \"Corpus\",\n                 pointSize = 1.3,\n                 colkey = colkey,\n                 shape = \"Level\",\n                 shapekey = shapekey,\n                 xlim = c(min(p$rotated[, \"PC1\"]), max(p$rotated[, \"PC1\"])),\n                 ylim = c(min(p$rotated[, \"PC2\"]), max(p$rotated[, \"PC2\"])),\n                 showLoadings = FALSE,\n                 ellipse = TRUE,\n                 axisLabSize = 18,\n                 legendPosition = 'right',\n                 legendTitleSize = 18,\n                 legendLabSize = 14, \n                 legendIconSize = 5) +\n  theme(plot.margin = unit(c(0,0,0,0.2), \"cm\"))\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"PCA_Ref3TxB_BiplotPC1_PC2.svg\"), width = 12, height = 8)\n\n# Biplots to examine components more carefully\nPCAtools::biplot(p,\n                 x = \"PC3\",\n                 y = \"PC4\",\n                 lab = NULL, # Otherwise will try to label each data point!\n                 colby = \"Corpus\",\n                 pointSize = 1.2,\n                 colkey = colkey,\n                 shape = \"Level\",\n                 shapekey = shapekey,\n                 xlim = c(min(p$rotated[, \"PC3\"]), max(p$rotated[, \"PC3\"])),\n                 ylim = c(min(p$rotated[, \"PC4\"]), max(p$rotated[, \"PC4\"])),\n                 showLoadings = FALSE,\n                 ellipse = TRUE,\n                 axisLabSize = 18,\n                 legendPosition = 'right',\n                 legendTitleSize = 18,\n                 legendLabSize = 14, \n                 legendIconSize = 5) +\n  theme(plot.margin = unit(c(0,0,0,0.2), \"cm\"))\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"PCA_Ref3TxB_BiplotPC3_PC4.svg\"), width = 12, height = 8)\n\n\nThe colours and corresponding ellipses can be used to visualise different clusters and patterns. In the following, we change the colour of the points and the ellipses to represent the texts’ target proficiency levels instead of the register, allowing for a different interpretation of the model.\n\nCode# Biplot with ellipses for Level rather than Register\ncolkeyLevels = c(A=\"#F9B921\", B=\"#A18A33\", C=\"#BD241E\", D=\"#722672\", E=\"#15274D\", `Ref. data`= \"darkgrey\")\nshapekeyLevels = c(`Spoken BNC2014 Ref.`=16, `Info Teens Ref.`=17, `Youth Fiction Ref.`=15, `Textbook Fiction`=0, `Textbook Conversation`=1, `Textbook Informative`=2)\n\nPCAtools::biplot(p,\n                 x = \"PC3\",\n                 y = \"PC4\",\n                 lab = NULL, # Otherwise will try to label each data point!\n                 colby = \"Level\",\n                 pointSize = 1.3,\n                 colkey = colkeyLevels,\n                 shape = \"Corpus\",\n                 shapekey = shapekeyLevels,\n                 xlim = c(min(p$rotated[, \"PC3\"]), max(p$rotated[, \"PC3\"])),\n                 ylim = c(min(p$rotated[, \"PC4\"]), max(p$rotated[, \"PC4\"])),\n                 showLoadings = FALSE,\n                 ellipse = TRUE,\n                 axisLabSize = 18,\n                 legendPosition = 'right',\n                 legendTitleSize = 18,\n                 legendLabSize = 14, \n                 legendIconSize = 5) +\n  theme(plot.margin = unit(c(0,0,0,0.2), \"cm\"))\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"PCA_Ref3TxB_BiplotPC3_PC4_levels.svg\"), width = 12, height = 8)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Analysis for the Model of Textbook English vs. ‘real-world’ English</span>"
    ]
  },
  {
    "objectID": "AppendixG.html#feature-contributions-loadings-on-each-component",
    "href": "AppendixG.html#feature-contributions-loadings-on-each-component",
    "title": "Appendix G — Data Analysis for the Model of Textbook English vs. ‘real-world’ English",
    "section": "\nG.5 Feature contributions (loadings) on each component",
    "text": "G.5 Feature contributions (loadings) on each component\n\nCode#data &lt;- readRDS(here(\"processed_data\", \"dataforPCA.rds\")) \npca &lt;- prcomp(data[,9:ncol(data)], scale.=FALSE) # All quantitative variables to be included in the model\n\n# The rotated data that represents the observations / samples is stored in rotated, while the variable loadings are stored in loadings\nloadings &lt;- as.data.frame(pca$rotation[,1:4])\n\n# Table of loadings with no minimum threshold applied\nloadings |&gt; \n  round(2) |&gt; \n  kable()\n\n\n\n\nPC1\nPC2\nPC3\nPC4\n\n\n\nACT\n-0.10\n0.01\n-0.01\n0.12\n\n\nAMP\n0.00\n-0.05\n-0.05\n0.09\n\n\nASPECT\n-0.08\n0.10\n-0.01\n0.00\n\n\nAWL\n-0.21\n-0.13\n-0.06\n-0.01\n\n\nBEMA\n0.08\n-0.24\n0.06\n-0.02\n\n\nCAUSE\n-0.08\n-0.12\n0.06\n0.14\n\n\nCC\n-0.14\n-0.13\n-0.09\n-0.09\n\n\nCOMM\n-0.03\n0.19\n0.03\n0.20\n\n\nCONC\n-0.03\n-0.03\n-0.18\n-0.06\n\n\nCOND\n0.08\n-0.02\n-0.17\n0.23\n\n\nCONT\n0.22\n-0.05\n0.03\n0.00\n\n\nCUZ\n0.10\n-0.14\n-0.20\n-0.18\n\n\nDEMO\n0.15\n-0.12\n-0.08\n-0.04\n\n\nDMA\n0.20\n-0.09\n-0.04\n-0.16\n\n\nDOAUX\n0.18\n-0.02\n0.06\n0.00\n\n\nDT\n0.08\n0.16\n-0.24\n-0.03\n\n\nDWNT\n-0.04\n0.10\n-0.12\n0.09\n\n\nELAB\n-0.07\n-0.17\n-0.04\n0.07\n\n\nEMPH\n0.17\n-0.07\n-0.09\n-0.03\n\n\nEX\n0.06\n-0.02\n-0.04\n0.00\n\n\nEXIST\n-0.13\n-0.09\n-0.05\n0.00\n\n\nFPP1P\n0.08\n-0.02\n0.09\n0.19\n\n\nFPP1S\n0.17\n0.06\n0.06\n0.08\n\n\nFPUH\n0.18\n-0.10\n-0.05\n-0.19\n\n\nGTO\n0.14\n0.00\n-0.04\n0.00\n\n\nHDG\n0.10\n-0.07\n-0.14\n-0.12\n\n\nHGOT\n0.16\n-0.05\n-0.01\n-0.11\n\n\nIN\n-0.21\n-0.01\n-0.08\n0.01\n\n\nJJAT\n-0.05\n-0.13\n-0.25\n0.07\n\n\nJJPR\n-0.03\n-0.17\n-0.03\n0.21\n\n\nLD\n-0.17\n-0.16\n0.13\n-0.04\n\n\nMDCA\n0.05\n-0.21\n0.11\n0.22\n\n\nMDCO\n0.00\n0.19\n-0.15\n0.10\n\n\nMDMM\n-0.02\n-0.10\n-0.14\n0.17\n\n\nMDNE\n0.06\n-0.02\n-0.06\n0.22\n\n\nMDWO\n0.07\n0.11\n-0.18\n0.04\n\n\nMDWS\n0.06\n-0.02\n-0.01\n0.25\n\n\nMENTAL\n0.11\n-0.02\n-0.05\n0.16\n\n\nNCOMP\n0.00\n-0.27\n-0.05\n0.03\n\n\nNN\n-0.21\n-0.10\n0.10\n-0.07\n\n\nOCCUR\n-0.13\n-0.02\n-0.05\n-0.06\n\n\nPASS\n-0.14\n-0.13\n-0.09\n-0.10\n\n\nPEAS\n-0.06\n0.12\n-0.19\n0.12\n\n\nPIT\n0.15\n-0.10\n-0.15\n-0.07\n\n\nPLACE\n0.02\n0.09\n0.09\n0.07\n\n\nPOLITE\n0.09\n0.00\n0.20\n0.11\n\n\nPOS\n0.02\n0.09\n0.04\n-0.05\n\n\nPROG\n0.09\n0.08\n-0.04\n0.15\n\n\nQUAN\n0.17\n-0.01\n-0.16\n0.01\n\n\nQUPR\n0.08\n0.11\n-0.12\n0.21\n\n\nQUTAG\n0.15\n-0.04\n-0.07\n-0.15\n\n\nRB\n0.14\n0.15\n-0.18\n0.07\n\n\nRP\n-0.01\n0.22\n-0.09\n0.15\n\n\nSPLIT\n-0.03\n-0.11\n-0.21\n0.08\n\n\nSPP2\n0.17\n-0.07\n0.10\n0.16\n\n\nSTPR\n0.10\n0.01\n0.01\n-0.04\n\n\nTHATD\n0.16\n-0.01\n-0.14\n-0.02\n\n\nTHRC\n-0.05\n-0.17\n-0.15\n-0.02\n\n\nTHSC\n-0.02\n-0.08\n-0.27\n0.07\n\n\nTTR\n-0.19\n0.07\n-0.02\n0.16\n\n\nVBD\n-0.10\n0.35\n-0.05\n-0.20\n\n\nVBG\n-0.14\n-0.02\n-0.14\n0.12\n\n\nVBN\n-0.14\n-0.10\n-0.08\n-0.07\n\n\nVIMP\n0.01\n-0.07\n0.21\n0.21\n\n\nWHQU\n0.13\n-0.02\n0.20\n0.07\n\n\nWHSC\n-0.09\n-0.10\n-0.20\n0.05\n\n\nXX0\n0.18\n0.01\n-0.06\n0.06\n\n\nYNQU\n0.18\n-0.03\n0.14\n-0.02\n\n\nTPP3\n-0.05\n0.30\n-0.04\n-0.15\n\n\nFQTI\n-0.07\n0.03\n0.01\n0.14\n\n\n\n\nCode#clipr::write_last_clip()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Analysis for the Model of Textbook English vs. ‘real-world’ English</span>"
    ]
  },
  {
    "objectID": "AppendixG.html#graphs-of-features-of-that-contribute-most-to-each-componentdimension",
    "href": "AppendixG.html#graphs-of-features-of-that-contribute-most-to-each-componentdimension",
    "title": "Appendix G — Data Analysis for the Model of Textbook English vs. ‘real-world’ English",
    "section": "\nG.6 Graphs of features of that contribute most to each component/dimension",
    "text": "G.6 Graphs of features of that contribute most to each component/dimension\nGraphs of features display the features with the strongest contributions to any two dimensions of the model of intra-textbook variation. They are created using the factoextra::fviz_pca_var function.\n\nCodefactoextra::fviz_pca_var(pca,\n             axes = c(1,2),\n             select.var = list(contrib = 25),\n             col.var = \"contrib\", # Colour by contributions to the PC\n             gradient.cols = c(\"#F9B921\", \"#DB241E\", \"#722672\"),\n             title = \"\",\n             repel = TRUE, # Try to avoid too much text overlapping\n             ggtheme = ggthemes::theme_few())\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"fviz_pca_var_PC1_PC2_Ref3Reg.svg\"), width = 9, height = 7)\n\nfactoextra::fviz_pca_var(pca,\n             axes = c(3,2),\n             select.var = list(contrib = 30),\n             col.var = \"contrib\", # Colour by contributions to the PC\n             gradient.cols = c(\"#F9B921\", \"#DB241E\", \"#722672\"),\n             title = \"\",\n             repel = TRUE, # Try to avoid too much text overlapping\n             ggtheme = ggthemes::theme_few())\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"fviz_pca_var_PC3_PC2_Ref3Reg.svg\"), width = 9, height = 8)\n\nfactoextra::fviz_pca_var(pca,\n             axes = c(3,4),\n             select.var = list(contrib = 30),\n             col.var = \"contrib\", # Colour by contributions to the PC\n             gradient.cols = c(\"#F9B921\", \"#DB241E\", \"#722672\"),\n             title = \"\",\n             repel = TRUE, # Try to avoid too much text overlapping\n             ggtheme = ggthemes::theme_few())\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"fviz_pca_var_PC3_PC4_Ref3Reg.svg\"), width = 9, height = 8)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Analysis for the Model of Textbook English vs. ‘real-world’ English</span>"
    ]
  },
  {
    "objectID": "AppendixG.html#exploring-feature-contributions-in-terms-of-normalised-frequencies",
    "href": "AppendixG.html#exploring-feature-contributions-in-terms-of-normalised-frequencies",
    "title": "Appendix G — Data Analysis for the Model of Textbook English vs. ‘real-world’ English",
    "section": "\nG.7 Exploring feature contributions in terms of normalised frequencies",
    "text": "G.7 Exploring feature contributions in terms of normalised frequencies\nWe can go back to the normalised frequencies of the individual features to compare them across different registers and levels, e.g.,:\n\nCodencounts &lt;- readRDS(here(\"data\", \"processed\", \"ncounts3_3Reg.rds\"))\n\nncounts |&gt; \n  filter(Register==\"Informative\") |&gt; \n  #filter(Level %in% c(\"C\", \"D\", \"E\")) |&gt; \n  select(Level, VBD, PEAS) |&gt; \n  group_by(Level) |&gt; \n  summarise_if(is.numeric, mean) |&gt; \n  kable(digits=2)\n\n\n\nLevel\nVBD\nPEAS\n\n\n\nA\n28.11\n0.21\n\n\nB\n34.11\n2.49\n\n\nC\n35.21\n5.36\n\n\nD\n39.83\n7.63\n\n\nE\n35.17\n6.91\n\n\nRef.\n39.73\n4.94\n\n\n\n\n\nThe following chunk produces Figure 35 which shows normalised counts of selected features with salient loadings on PC1 in the Textbook Informative subcorpus (Levels A to E) and the reference Info Teens corpus (Ref.). This plots visualises the observed normalised frequencies as they were extracted using the MFTE Perl (see Appendices C and F).\n\nCodecols = c(\"#F9B921\", \"#A18A33\", \"#BD241E\", \"#722672\", \"#15274D\", \"darkgrey\")\n\nboxfeature &lt;- ncounts |&gt; \n  filter(Register==\"Informative\") |&gt; \n  #filter(Level %in% c(\"C\", \"D\", \"E\")) |&gt; \n  select(Level, FPP1S, SPP2, CONT, EXIST, AWL, XX0, PASS, VBN) |&gt; \n  ggplot(aes(x = Level, y = CONT, colour = Level, fill = Level)) +\n  geom_jitter(size=0.7, alpha=.7) +\n  geom_boxplot(outlier.shape = NA, fatten = 2, fill = \"white\", alpha = 0.3) +\n  scale_colour_manual(values = cols) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  xlab(\"\")\n\nCONT = boxfeature\nSPP2 &lt;- boxfeature + aes(y = SPP2) \nEXIST &lt;- boxfeature + aes(y = EXIST) + ylim(c(0,25)) # These y-axis limits remove individual outliers that overextend the scales and make the existing differences invisible to the naked eye. They can be removed to visualise all data points.\nFFP1 &lt;- boxfeature + aes(y = FPP1S) + ylim(c(0,60))  \nAWL &lt;- boxfeature + aes(y = AWL)\nXX0 &lt;- boxfeature + aes(y = XX0) + ylim(c(0,25))\nPASS &lt;- boxfeature + aes(y = PASS)\nVBN &lt;- boxfeature + aes(y = VBN) + ylim(c(0,40))\n\nboxplots &lt;- gridExtra::grid.arrange(PASS, VBN, AWL, EXIST, FFP1, SPP2, CONT, XX0, ncol=2, nrow=4)\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"BoxplotsInformativeFeatures.svg\"), plot = boxplots, dpi = 300, width = 9, height = 11)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Analysis for the Model of Textbook English vs. ‘real-world’ English</span>"
    ]
  },
  {
    "objectID": "AppendixG.html#exploring-the-dimensions-of-the-model",
    "href": "AppendixG.html#exploring-the-dimensions-of-the-model",
    "title": "Appendix G — Data Analysis for the Model of Textbook English vs. ‘real-world’ English",
    "section": "\nG.8 Exploring the dimensions of the model",
    "text": "G.8 Exploring the dimensions of the model\nWe begin with some descriptive statistics of the dimension scores.\n\nCode#data &lt;- readRDS(here(\"data\", \"processed\", \"dataforPCA.rds\")) \n#colnames(data)\npca &lt;- prcomp(data[,9:ncol(data)], scale.=FALSE) # All quantitative variables\n\n## Access to the PCA results\n#colnames(data)\nres.ind &lt;- cbind(data[,1:8], as.data.frame(pca$x)[,1:4])\n\n## Summary statistics\nres.ind |&gt; \n  group_by(Subcorpus, Level) |&gt; \n  summarise_if(is.numeric, c(mean = mean, sd = sd)) |&gt; \n  kable(digits = 2)  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubcorpus\nLevel\nWords_mean\nPC1_mean\nPC2_mean\nPC3_mean\nPC4_mean\nWords_sd\nPC1_sd\nPC2_sd\nPC3_sd\nPC4_sd\n\n\n\nTextbook Conversation\nA\n813.02\n1.91\n-1.02\n3.49\n-0.17\n154.57\n0.89\n0.60\n1.03\n0.73\n\n\nTextbook Conversation\nB\n819.48\n2.11\n-0.64\n2.33\n0.39\n218.88\n0.91\n0.69\n1.04\n0.79\n\n\nTextbook Conversation\nC\n823.69\n1.59\n-0.38\n1.39\n0.75\n279.14\n1.20\n0.73\n1.04\n0.79\n\n\nTextbook Conversation\nD\n797.85\n1.09\n-0.43\n0.79\n0.91\n187.98\n1.45\n0.72\n1.26\n0.79\n\n\nTextbook Conversation\nE\n1132.79\n1.03\n-0.61\n0.86\n1.09\n569.85\n1.30\n0.78\n0.88\n0.62\n\n\nTextbook Fiction\nA\n886.00\n0.13\n0.22\n2.74\n-0.27\n242.78\n0.97\n0.69\n0.90\n0.81\n\n\nTextbook Fiction\nB\n864.16\n-0.21\n1.34\n1.73\n-0.29\n222.44\n0.79\n0.62\n0.73\n0.53\n\n\nTextbook Fiction\nC\n854.21\n-0.27\n1.63\n0.76\n0.12\n198.21\n0.89\n0.78\n0.96\n0.63\n\n\nTextbook Fiction\nD\n801.78\n-0.84\n1.38\n0.26\n0.15\n196.33\n1.15\n0.79\n0.83\n0.59\n\n\nTextbook Fiction\nE\n853.26\n-0.65\n1.27\n0.26\n0.20\n195.97\n1.10\n0.75\n0.92\n0.57\n\n\nTextbook Informative\nA\n851.71\n-1.46\n-0.96\n1.86\n-0.70\n199.77\n0.82\n0.86\n0.69\n0.85\n\n\nTextbook Informative\nB\n844.03\n-1.63\n-0.50\n1.23\n-0.27\n182.83\n0.92\n1.03\n0.80\n1.08\n\n\nTextbook Informative\nC\n838.90\n-1.87\n-0.54\n0.25\n0.14\n160.21\n1.09\n0.98\n0.93\n1.06\n\n\nTextbook Informative\nD\n847.65\n-2.24\n-0.32\n-0.15\n0.13\n179.25\n0.95\n0.92\n0.95\n0.83\n\n\nTextbook Informative\nE\n823.23\n-2.57\n-0.66\n-0.28\n0.37\n180.39\n0.99\n0.90\n0.79\n0.82\n\n\nSpoken BNC2014 Ref.\nRef.\n10637.74\n3.71\n-0.52\n-0.64\n-0.53\n8974.14\n0.49\n0.47\n0.76\n0.52\n\n\nYouth Fiction Ref.\nRef.\n5944.78\n-0.49\n1.75\n-0.21\n0.41\n198.64\n0.90\n0.52\n0.88\n0.52\n\n\nInfo Teens Ref.\nRef.\n805.41\n-3.06\n-0.96\n-0.23\n-0.15\n193.31\n1.09\n1.19\n0.88\n1.19\n\n\n\n\nCoderes.ind &lt;- res.ind |&gt; \n  mutate(Subsubcorpus = paste(Corpus, Register, sep = \"_\")) |&gt; \n  mutate(Subsubcorpus = as.factor(Subsubcorpus))\n  \nres.ind |&gt; \n  select(PC1, PC2, PC3, PC4, Subsubcorpus) |&gt; \n  tbl_summary(by = Subsubcorpus,\n              digits = list(all_continuous() ~ c(2, 2)),\n              statistic = all_continuous() ~  \"{mean} ({sd})\")\n\n\n\n\n\nCharacteristic\n\nInformative.Teens_Informative, N = 1,3371\n\n\nSpoken.BNC2014_Conversation, N = 1,2501\n\n\nTextbook.English_Conversation, N = 5651\n\n\nTextbook.English_Fiction, N = 2851\n\n\nTextbook.English_Informative, N = 3521\n\n\nYouth.Fiction_Fiction, N = 1,1911\n\n\n\n\nPC1\n-3.06 (1.09)\n3.71 (0.49)\n1.56 (1.25)\n-0.43 (1.05)\n-2.05 (1.04)\n-0.49 (0.90)\n\n\nPC2\n-0.96 (1.19)\n-0.52 (0.47)\n-0.57 (0.74)\n1.25 (0.84)\n-0.52 (0.96)\n1.75 (0.52)\n\n\nPC3\n-0.23 (0.88)\n-0.64 (0.76)\n1.71 (1.43)\n0.96 (1.22)\n0.31 (1.10)\n-0.21 (0.88)\n\n\nPC4\n-0.15 (1.19)\n-0.53 (0.52)\n0.61 (0.86)\n0.02 (0.64)\n0.05 (0.98)\n0.41 (0.52)\n\n\n\n\n1 Mean (SD)\n\n\n\n\nCoderes.ind |&gt; \n  select(Register, Level, PC4) |&gt; \n  group_by(Register, Level) |&gt; \n  summarise_if(is.numeric, c(Median = median, MAD = mad)) |&gt; \n  kable(digits = 2)  \n\n\n\nRegister\nLevel\nMedian\nMAD\n\n\n\nConversation\nA\n-0.07\n0.68\n\n\nConversation\nB\n0.42\n0.80\n\n\nConversation\nC\n0.73\n0.80\n\n\nConversation\nD\n0.86\n0.70\n\n\nConversation\nE\n1.09\n0.61\n\n\nConversation\nRef.\n-0.54\n0.52\n\n\nFiction\nA\n-0.38\n0.75\n\n\nFiction\nB\n-0.39\n0.53\n\n\nFiction\nC\n0.13\n0.67\n\n\nFiction\nD\n0.01\n0.54\n\n\nFiction\nE\n0.12\n0.67\n\n\nFiction\nRef.\n0.40\n0.50\n\n\nInformative\nA\n-0.71\n0.82\n\n\nInformative\nB\n-0.37\n1.10\n\n\nInformative\nC\n0.07\n1.09\n\n\nInformative\nD\n0.04\n0.79\n\n\nInformative\nE\n0.37\n0.57\n\n\nInformative\nRef.\n-0.08\n1.21\n\n\n\n\n\nThe following chunk can be used to search for example texts that are located in specific areas of the biplots. For example, we can search for texts that have high scores on Dim3 and low ones on Dim2 to proceed with a qualitative comparison and analysis of these texts.\n\nCode# Search for example texts to illustrate results\nres.ind |&gt; \n  filter(PC3 &gt; 2 & PC2 &lt; -2) |&gt; \n  #filter(Register==\"Conversation\") |&gt; \n  #filter(Level == \"B\") |&gt; \n  #filter(PC1 &gt; 4.7) |&gt; \n  select(Filename, PC1, PC2, PC3) |&gt; \n  kable(digits=2)\n\n\n\nFilename\nPC1\nPC2\nPC3\n\n\n\nNGL_1_Spoken_0002.txt\n2.41\n-2.27\n4.36\n\n\nHT_5_ELF_Spoken_0003.txt\n1.94\n-2.30\n3.49\n\n\nHT_6_Informative_0001.txt\n-2.19\n-2.36\n3.11\n\n\nScience_Tech_Kinds_NZ_10383721_typesofrobots.txt\n-3.09\n-2.62\n2.24\n\n\nHistory_Kids_BBC_10402894_go_furthers.txt\n-4.26\n-2.07\n2.08",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Analysis for the Model of Textbook English vs. ‘real-world’ English</span>"
    ]
  },
  {
    "objectID": "AppendixG.html#raincloud-plots-visualising-dimension-scores",
    "href": "AppendixG.html#raincloud-plots-visualising-dimension-scores",
    "title": "Appendix G — Data Analysis for the Model of Textbook English vs. ‘real-world’ English",
    "section": "\nG.9 Raincloud plots visualising dimension scores",
    "text": "G.9 Raincloud plots visualising dimension scores\n\nCoderes.ind$Subcorpus &lt;- fct_relevel(res.ind$Subcorpus, \"Spoken BNC2014 Ref.\", \"Textbook Conversation\", \"Youth Fiction Ref.\", \"Textbook Fiction\", \"Info Teens Ref.\", \"Textbook Informative\")\n\n# colours &lt;- suf_palette(name = \"london\", n = 6, type = \"continuous\")\n# colours2 &lt;- suf_palette(name = \"classic\", n = 5, type = \"continuous\")\n# colours &lt;- c(colours, colours2[c(2:4)]) # Nine colours range\n# palette &lt;- colours[c(1,5,6,2,3,8,7,4,9)] # Good order for PCA\n# colours &lt;- palette[c(1,8,9,2,7,3)]\n\n# This translates as:\npalette &lt;- c(\"#BD241E\", \"#A18A33\", \"#15274D\", \"#D54E1E\", \"#EA7E1E\", \"#4C4C4C\", \"#722672\", \"#F9B921\", \"#267226\")\ncolours &lt;- c(\"#BD241E\", \"#F9B921\", \"#267226\", \"#A18A33\", \"#722672\",\"#15274D\")\n\nggplot(res.ind, aes(x=Subcorpus,y=PC1, fill = Subcorpus, colour = Subcorpus))+ # Or leave out \"colour = Register\" to keep the dots in black\n  geom_flat_violin(position = position_nudge(x = .25, y = 0),adjust = 2, trim = FALSE)+\n  geom_point(position = position_jitter(width = .15), size = .25)+\n# note that here we need to set the x-variable to a numeric variable and bump it to get the boxplots to line up with the rainclouds. \n  geom_boxplot(aes(x = as.numeric(Subcorpus)+0.25, y = PC1), outlier.shape = NA, alpha = 0.3, width = .15, colour = \"BLACK\") +\n  ylab('PC1')+ \n  theme_cowplot()+\n  theme(axis.title.x=element_blank())+\n  guides(fill = \"none\", colour = \"none\") +\n  scale_colour_manual(values = colours)+\n  scale_fill_manual(values = colours) +\n  annotate(geom = \"text\", x = 1.5, y = -7, label = \"Conversation\", size = 5) +\n  annotate(geom = \"segment\", x = 0.7, xend = 2.5, y = -6.5, yend = -6.5) +\n  annotate(geom = \"text\", x = 3.5, y = -7, label = \"Fiction\", size = 5) +\n  annotate(geom = \"segment\", x = 2.7, xend = 4.5, y = -6.5, yend = -6.5) +\n  annotate(geom = \"text\", x = 5.7, y = -7, label = \"Informative\", size = 5) +\n  annotate(geom = \"segment\", x = 4.7, xend = 6.5, y = -6.5, yend = -6.5) +\n  scale_x_discrete(labels=rep(c(\"Reference\", \"Textbook\"), 3))+\n  scale_y_continuous(sec.axis = dup_axis(name=NULL), breaks = seq(from = -6, to = 5, by = 1))\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"PC1_3RegComparison.svg\"), width = 13, height = 8)\n#ggsave(here(\"plots\", \"PC1_3RegComparison.png\"), width = 20, height = 15, units = \"cm\", dpi = 300)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Analysis for the Model of Textbook English vs. ‘real-world’ English</span>"
    ]
  },
  {
    "objectID": "AppendixG.html#computing-mixed-effects-models-of-the-dimension-scores",
    "href": "AppendixG.html#computing-mixed-effects-models-of-the-dimension-scores",
    "title": "Appendix G — Data Analysis for the Model of Textbook English vs. ‘real-world’ English",
    "section": "\nG.10 Computing mixed-effects models of the dimension scores",
    "text": "G.10 Computing mixed-effects models of the dimension scores\n\nG.10.1 Data preparation\nIn this chunk, we add a Source variable to be used as a random effect variable in the following mixed-effects models (see 5.3.8 for details).\n\nCoderes.ind &lt;- res.ind |&gt; \n  mutate(Source = case_when(\n  Corpus==\"Youth.Fiction\" ~ paste(\"Book\", str_extract(Filename, \"[0-9]{1,3}\"), sep = \"\"),\n  Corpus==\"Spoken.BNC2014\" ~ \"Spoken.BNC2014\",\n  Corpus==\"Textbook.English\" ~ as.character(Series),\n  Corpus==\"Informative.Teens\" ~ str_extract(Filename, \"BBC|Science_Tech\"),\n  TRUE ~ \"NA\")) |&gt; \n  mutate(Source = case_when(\n  Corpus==\"Informative.Teens\" & is.na(Source) ~ str_remove(Filename, \"_.*\"),\n  TRUE ~ as.character(Source))) |&gt; \n  mutate(Source = as.factor(Source)) |&gt; \n  mutate(Corpus = case_when(\n    Corpus==\"Textbook.English\" ~ \"Textbook\",\n    Corpus==\"Informative.Teens\" ~ \"Reference\",\n    Corpus==\"Spoken.BNC2014\" ~ \"Reference\",\n    Corpus==\"Youth.Fiction\" ~ \"Reference\"\n  )) |&gt; \n  mutate(Corpus = as.factor(Corpus))\n\n# Change the reference levels to theoretically more meaningful levels and one that is better populated (see, e.g., https://stats.stackexchange.com/questions/430770/in-a-multilevel-linear-regression-how-does-the-reference-level-affect-other-lev)\n# summary(res.ind$Corpus)\nres.ind$Corpus &lt;- relevel(res.ind$Corpus, \"Reference\")\n\n# summary(res.ind$Subcorpus)\nres.ind$Subcorpus &lt;- factor(res.ind$Subcorpus, levels = c(\"Spoken BNC2014 Ref.\", \"Textbook Conversation\", \"Youth Fiction Ref.\", \"Textbook Fiction\", \"Info Teens Ref.\", \"Textbook Informative\"))\n\n# summary(res.ind$Level)\nres.ind$Level &lt;- relevel(res.ind$Level, \"Ref.\")\n\n\n\nG.10.2 Dimension 1: ‘Spontaneous interactional vs. Edited informational’\nWe first compare various models and then present a tabular summary of the best-fitting one.\n\nmd_source &lt;- lmer(PC1 ~ 1 + (Register|Source), res.ind, REML = FALSE) \nmd_corpus &lt;- update(md_source, .~. + Level) # Failed to converge\nmd_register &lt;- update(md_source, . ~ . + Register)\nmd_both &lt;- update(md_corpus, .~. + Register)\nmd_interaction &lt;- update(md_both, . ~ . + Level:Register)\n\nanova(md_source, md_corpus, md_register, md_both, md_interaction)\n\nData: res.ind\nModels:\nmd_source: PC1 ~ 1 + (Register | Source)\nmd_register: PC1 ~ (Register | Source) + Register\nmd_corpus: PC1 ~ (Register | Source) + Level\nmd_both: PC1 ~ (Register | Source) + Level + Register\nmd_interaction: PC1 ~ (Register | Source) + Level + Register + Level:Register\n               npar   AIC   BIC  logLik deviance   Chisq Df Pr(&gt;Chisq)    \nmd_source         8 12421 12473 -6202.5    12405                          \nmd_register      10 12357 12422 -6168.4    12337  68.106  2  1.625e-15 ***\nmd_corpus        13 12181 12266 -6077.5    12155 181.996  3  &lt; 2.2e-16 ***\nmd_both          15 12117 12215 -6043.5    12087  67.843  2  1.854e-15 ***\nmd_interaction   25 12098 12261 -6024.0    12048  39.104 10  2.435e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmd_interaction &lt;- lmer(PC1 ~ Level + Register + Level*Register + (Register|Source), res.ind, REML = FALSE) \n\ntab_model(md_interaction, wrap.labels = 200) # R2 = 0.870 / 0.923\n\n\n\n \nPC 1\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n3.71\n2.46 – 4.96\n&lt;0.001\n\n\nLevel [A]\n-1.73\n-3.06 – -0.40\n0.011\n\n\nLevel [B]\n-1.65\n-2.97 – -0.32\n0.015\n\n\nLevel [C]\n-2.08\n-3.40 – -0.76\n0.002\n\n\nLevel [D]\n-2.47\n-3.80 – -1.15\n&lt;0.001\n\n\nLevel [E]\n-2.73\n-4.06 – -1.40\n&lt;0.001\n\n\nRegister [Fiction]\n-4.20\n-5.45 – -2.95\n&lt;0.001\n\n\nRegister [Informative]\n-6.75\n-8.03 – -5.47\n&lt;0.001\n\n\nLevel [A] × Register [Fiction]\n2.18\n0.86 – 3.49\n0.001\n\n\nLevel [B] × Register [Fiction]\n1.71\n0.41 – 3.01\n0.010\n\n\nLevel [C] × Register [Fiction]\n2.12\n0.83 – 3.42\n0.001\n\n\nLevel [D] × Register [Fiction]\n1.93\n0.64 – 3.23\n0.003\n\n\nLevel [E] × Register [Fiction]\n2.41\n1.10 – 3.71\n&lt;0.001\n\n\nLevel [A] × Register [Informative]\n3.37\n2.01 – 4.73\n&lt;0.001\n\n\nLevel [B] × Register [Informative]\n3.17\n1.83 – 4.51\n&lt;0.001\n\n\nLevel [C] × Register [Informative]\n3.24\n1.90 – 4.57\n&lt;0.001\n\n\nLevel [D] × Register [Informative]\n3.20\n1.87 – 4.53\n&lt;0.001\n\n\nLevel [E] × Register [Informative]\n3.14\n1.80 – 4.48\n&lt;0.001\n\n\nRandom Effects\n\n\nσ2\n\n0.59\n\n\nτ00Source\n\n0.41\n\n\n\nτ11Source.RegisterFiction\n\n0.12\n\n\n\nτ11Source.RegisterInformative\n\n0.20\n\n\n\nρ01\n\n-0.05\n\n\n\n\n-0.48\n\n\n\nICC\n0.41\n\n\n\nN Source\n\n325\n\n\nObservations\n4980\n\n\nMarginal R2 / Conditional R2\n\n0.870 / 0.923\n\n\n\n\nIts estimated coefficients are visualised in the plot below.\n\nCode# Tweak plot aesthetics with: https://cran.r-project.org/web/packages/sjPlot/vignettes/custplot.html\n# Colour customisation trick from: https://stackoverflow.com/questions/55598920/different-line-colors-in-forest-plot-output-from-sjplot-r-package\n\nplot_model(md_interaction, \n           #type = \"re\", # Option to visualise random effects \n           show.intercept = TRUE,\n           show.values=TRUE, \n           show.p=TRUE,\n           value.offset = .4,\n           value.size = 3.5,\n           colors = palette[c(1:3,7:9)],\n           group.terms = c(1,5,5,5,5,5,6,4,2,2,2,2,2,3,3,3,3,3), \n           title=\"Fixed effects\",\n           wrap.labels = 40,\n           axis.title = \"PC1 estimated coefficients\") +\n  theme_sjplot2() \n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TxBRef3Reg_PC1_lmer_fixed.svg\"), height = 6, width = 9)\n\n\nThe visreg function is used to visualise the distributions of the modelled Dim1 scores:\n\nCode# svg(here(\"plots\", \"TxBReg3Reg_predicted_PC1_scores_interactions.svg\"), height = 8, width = 9)\nvisreg(md_interaction, xvar = \"Level\", by=\"Register\", \n       #type = \"contrast\",\n       type = \"conditional\",\n       line=list(col=\"darkred\"), \n       points=list(cex=0.3),\n       xlab = \"Ref. corpora and textbook level (A to E)\", ylab = \"PC1\",\n       layout=c(3,1)\n)\n\n\n\n\n\n\nCode# dev.off()\n\n\nFor PC2 to PC4, the models with random intercepts and slopes failed to converge, which is why only slopes are included in the following models.\n\nCode# Function to avoid repeating model fitting and comparison process for each PC.\nrun_anova &lt;- function(response_var, data) {\n  # Fit the initial model\n  md_source &lt;- lmer(formula = paste(response_var, \"~ 1 + (1|Source)\"), data = data, REML = FALSE)\n  \n  # Update models\n  md_corpus &lt;- update(md_source, . ~ . + Level)\n  md_register &lt;- update(md_source, . ~ . + Register)\n  md_both &lt;- update(md_corpus, . ~ . + Register)\n  md_interaction &lt;- update(md_both, . ~ . + Level:Register)\n  \n  # Perform ANOVA\n  anova_results &lt;- anova(md_source, md_corpus, md_register, md_both, md_interaction)\n  \n  # Print ANOVA results\n  print(anova_results)\n  \n  # Save model object with appropriate name\n  pc_number &lt;- gsub(\"PC\", \"\", response_var)\n  assign(paste(\"md_interaction_PC\", pc_number, sep = \"\"), md_interaction, envir = .GlobalEnv)\n  \n  # Return tabulated model\n  return(md_interaction)\n}\n\n\n\nG.10.3 Dimension 2: ‘Narrative vs. Non-narrative’\nWe first compare various models and then present a tabular summary of the best-fitting one.\n\nPC2_models &lt;- run_anova(\"PC2\", res.ind)\n\nData: data\nModels:\nmd_source: PC2 ~ 1 + (1 | Source)\nmd_register: PC2 ~ (1 | Source) + Register\nmd_corpus: PC2 ~ (1 | Source) + Level\nmd_both: PC2 ~ (1 | Source) + Level + Register\nmd_interaction: PC2 ~ (1 | Source) + Level + Register + Level:Register\n               npar   AIC   BIC  logLik deviance    Chisq Df Pr(&gt;Chisq)    \nmd_source         3 12281 12301 -6137.6    12275                           \nmd_register       5 10761 10794 -5375.7    10751 1523.882  2  &lt; 2.2e-16 ***\nmd_corpus         8 12138 12190 -6061.1    12122    0.000  3          1    \nmd_both          10 10616 10681 -5298.1    10596 1525.963  2  &lt; 2.2e-16 ***\nmd_interaction   20 10550 10680 -5254.9    10510   86.436 10  2.718e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ntab_model(md_interaction_PC2) # R2 = 0.671 / 0.753\n\n\n\n \nPC 2\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-0.52\n-1.27 – 0.24\n0.183\n\n\nLevel [A]\n-0.54\n-1.35 – 0.28\n0.195\n\n\nLevel [B]\n-0.15\n-0.96 – 0.66\n0.721\n\n\nLevel [C]\n0.08\n-0.72 – 0.89\n0.842\n\n\nLevel [D]\n0.04\n-0.76 – 0.85\n0.915\n\n\nLevel [E]\n0.07\n-0.75 – 0.89\n0.866\n\n\nRegister [Fiction]\n2.27\n1.51 – 3.03\n&lt;0.001\n\n\nRegister [Informative]\n-0.46\n-1.24 – 0.32\n0.250\n\n\nLevel [A] × Register[Fiction]\n-1.01\n-1.82 – -0.21\n0.014\n\n\nLevel [B] × Register[Fiction]\n-0.25\n-1.04 – 0.54\n0.532\n\n\nLevel [C] × Register[Fiction]\n-0.21\n-1.00 – 0.58\n0.602\n\n\nLevel [D] × Register[Fiction]\n-0.41\n-1.20 – 0.38\n0.308\n\n\nLevel [E] × Register[Fiction]\n-0.47\n-1.26 – 0.32\n0.246\n\n\nLevel [A] × Register[Informative]\n0.49\n-0.34 – 1.33\n0.246\n\n\nLevel [B] × Register[Informative]\n0.59\n-0.22 – 1.40\n0.154\n\n\nLevel [C] × Register[Informative]\n0.26\n-0.54 – 1.06\n0.524\n\n\nLevel [D] × Register[Informative]\n0.55\n-0.26 – 1.35\n0.183\n\n\nLevel [E] × Register[Informative]\n0.33\n-0.49 – 1.14\n0.431\n\n\nRandom Effects\n\n\nσ2\n\n0.45\n\n\nτ00Source\n\n0.15\n\n\n\nICC\n0.25\n\n\n\nN Source\n\n325\n\n\nObservations\n4980\n\n\nMarginal R2 / Conditional R2\n\n0.671 / 0.753\n\n\n\n\nVisualisation of the coefficient estimates of the fixed effects:\n\nCodeplot_model(md_interaction_PC2, \n           #type = \"re\", # Option to visualise random effects \n           show.intercept = TRUE,\n           show.values=TRUE, \n           show.p=TRUE,\n           value.offset = .4,\n           value.size = 3.5,\n           colors = palette[c(1:3,7:9)],\n           group.terms = c(1,5,5,5,5,5,6,4,2,2,2,2,2,3,3,3,3,3), \n           title=\"Fixed effects\",\n           wrap.labels = 40,\n           axis.title = \"PC2 estimated coefficients\") +\n  theme_sjplot2() \n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TxBRef3Reg_PC2_lmer_fixed.svg\"), height = 6, width = 9)\n\n\nVisualisation of the predicted Dim2 scores:\n\nCode# svg(here(\"plots\", \"TxBReg3Reg_predicted_PC2_scores_interactions.svg\"), height = 8, width = 9)\nvisreg(md_interaction_PC2, xvar = \"Level\", by=\"Register\", \n       #type = \"contrast\",\n       type = \"conditional\",\n       line=list(col=\"darkred\"), \n       points=list(cex=0.3),\n       xlab = \"Ref. corpora and textbook level (A to E)\", ylab = \"PC2\",\n       layout=c(3,1)\n)\n\n\n\n\n\n\nCode# dev.off()\n\n\nWe can also explore the random effect structure.\n\n# Random effects\nranef &lt;- as.data.frame(ranef(md_interaction_PC2))\n\n# Exploring the random effects of the sources of the Info Teens corpus\nranef |&gt; \n  filter(grp %in% c(\"TeenVogue\", \"BBC\", \"Dogo\", \"Ducksters\", \"Encyclopedia\", \"Factmonster\", \"History\", \"Quatr\", \"Revision\", \"Science\", \"Science_Tech\", \"Teen\", \"TweenTribute\", \"WhyFiles\", \"World\")) |&gt; \n  ggplot(aes(x = grp, y = condval)) +\n  geom_point() +\n  coord_flip()\n\n\n\n\n\n\n# Exploring the random effects associated with textbook series\nranef |&gt; \n  filter(grp %in% levels(data$Series)) |&gt; \n  ggplot(aes(x = grp, y = condval)) +\n  geom_point() +\n  coord_flip()\n\n\n\n\n\n\n\n\nG.10.4 Dimension 3: ‘Pedagogically adapted vs. Natural’\nWe first compare various models and then present a tabular summary of the best-fitting one.\n\nPC3_models &lt;- run_anova(\"PC3\", res.ind)\n\nData: data\nModels:\nmd_source: PC3 ~ 1 + (1 | Source)\nmd_register: PC3 ~ (1 | Source) + Register\nmd_corpus: PC3 ~ (1 | Source) + Level\nmd_both: PC3 ~ (1 | Source) + Level + Register\nmd_interaction: PC3 ~ (1 | Source) + Level + Register + Level:Register\n               npar   AIC   BIC  logLik deviance    Chisq Df Pr(&gt;Chisq)    \nmd_source         3 13523 13542 -6758.3    13517                           \nmd_register       5 12988 13020 -6489.0    12978  538.750  2  &lt; 2.2e-16 ***\nmd_corpus         8 11928 11981 -5956.2    11912 1065.455  3  &lt; 2.2e-16 ***\nmd_both          10 11466 11531 -5722.8    11446  466.870  2  &lt; 2.2e-16 ***\nmd_interaction   20 11461 11592 -5710.7    11421   24.264 10   0.006929 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ntab_model(md_interaction_PC3) # R2 = 0.425 / 0.700\n\n\n\n \nPC 3\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-0.64\n-1.99 – 0.71\n0.354\n\n\nLevel [A]\n4.25\n2.82 – 5.69\n&lt;0.001\n\n\nLevel [B]\n3.09\n1.66 – 4.52\n&lt;0.001\n\n\nLevel [C]\n2.12\n0.69 – 3.55\n0.004\n\n\nLevel [D]\n1.64\n0.21 – 3.07\n0.024\n\n\nLevel [E]\n1.29\n-0.15 – 2.73\n0.078\n\n\nRegister [Fiction]\n0.43\n-0.92 – 1.79\n0.533\n\n\nRegister [Informative]\n0.43\n-0.96 – 1.83\n0.544\n\n\nLevel [A] × Register[Fiction]\n-1.50\n-2.89 – -0.12\n0.033\n\n\nLevel [B] × Register[Fiction]\n-1.39\n-2.76 – -0.01\n0.048\n\n\nLevel [C] × Register[Fiction]\n-1.34\n-2.71 – 0.03\n0.056\n\n\nLevel [D] × Register[Fiction]\n-1.35\n-2.72 – 0.03\n0.055\n\n\nLevel [E] × Register[Fiction]\n-1.03\n-2.41 – 0.35\n0.142\n\n\nLevel [A] × Register[Informative]\n-1.92\n-3.35 – -0.49\n0.009\n\n\nLevel [B] × Register[Informative]\n-1.45\n-2.86 – -0.03\n0.045\n\n\nLevel [C] × Register[Informative]\n-1.36\n-2.77 – 0.05\n0.058\n\n\nLevel [D] × Register[Informative]\n-1.43\n-2.84 – -0.02\n0.047\n\n\nLevel [E] × Register[Informative]\n-1.53\n-2.95 – -0.11\n0.034\n\n\nRandom Effects\n\n\nσ2\n\n0.52\n\n\nτ00Source\n\n0.48\n\n\n\nICC\n0.48\n\n\n\nN Source\n\n325\n\n\nObservations\n4980\n\n\nMarginal R2 / Conditional R2\n\n0.425 / 0.700\n\n\n\n\nVisualisation of the coefficient estimates of the fixed effects:\n\nCodeplot_model(md_interaction_PC3, \n           #type = \"re\", # Option to visualise random effects \n           show.intercept = TRUE,\n           show.values=TRUE, \n           show.p=TRUE,\n           value.offset = .4,\n           value.size = 3.5,\n           colors = palette[c(1:3,7:9)],\n           group.terms = c(1,5,5,5,5,5,6,4,2,2,2,2,2,3,3,3,3,3), \n           title=\"Fixed effects\",\n           wrap.labels = 40,\n           axis.title = \"PC3 estimated coefficients\") +\n  theme_sjplot2() \n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TxBRef3Reg_PC3_lmer_fixed.svg\"), height = 6, width = 9)\n\n\nVisualisation of the predicted Dim3 scores:\n\nCode# svg(here(\"plots\", \"TxBReg3Reg_predicted_PC3_scores_interactions.svg\"), height = 8, width = 9)\nvisreg(md_interaction_PC3, xvar = \"Level\", by=\"Register\", \n       #type = \"contrast\",\n       type = \"conditional\",\n       line=list(col=\"darkred\"), \n       points=list(cex=0.3),\n       xlab = \"Ref. corpora and textbook level (A to E)\", ylab = \"PC3\",\n       layout=c(3,1)\n)\n\n\n\n\n\n\nCode# dev.off()\n\n\nWe can also explore the random effect structure.\n\n# Random effects\nranef &lt;- as.data.frame(ranef(md_interaction_PC3))\n\n# Exploring the random effects of the sources of the Info Teens corpus\nranef |&gt; \n  filter(grp %in% c(\"TeenVogue\", \"BBC\", \"Dogo\", \"Ducksters\", \"Encyclopedia\", \"Factmonster\", \"History\", \"Quatr\", \"Revision\", \"Science\", \"Science_Tech\", \"Teen\", \"TweenTribute\", \"WhyFiles\", \"World\")) |&gt; \n  ggplot(aes(x = grp, y = condval)) +\n  geom_point() +\n  coord_flip()\n\n\n\n\n\n\n# Exploring the random effects associated with textbook series\nranef |&gt; \n  filter(grp %in% levels(data$Series)) |&gt; \n  ggplot(aes(x = grp, y = condval)) +\n  geom_point() +\n  coord_flip()\n\n\n\n\n\n\n\n\nG.10.5 Dimension 4: ‘Factual vs. Speculative’ / ‘Simple vs. complex verb forms’?\nWe first compare various models and then present a tabular summary of the best-fitting one.\n\nPC4_models &lt;- run_anova(\"PC4\", res.ind)\n\nData: data\nModels:\nmd_source: PC4 ~ 1 + (1 | Source)\nmd_register: PC4 ~ (1 | Source) + Register\nmd_corpus: PC4 ~ (1 | Source) + Level\nmd_both: PC4 ~ (1 | Source) + Level + Register\nmd_interaction: PC4 ~ (1 | Source) + Level + Register + Level:Register\n               npar   AIC   BIC  logLik deviance    Chisq Df Pr(&gt;Chisq)    \nmd_source         3 11019 11039 -5506.5    11013                           \nmd_register       5 10825 10857 -5407.4    10815 198.2593  2    &lt; 2e-16 ***\nmd_corpus         8 10827 10879 -5405.3    10811   4.0956  3     0.2513    \nmd_both          10 10563 10628 -5271.6    10543 267.5805  2    &lt; 2e-16 ***\nmd_interaction   20 10527 10657 -5243.3    10487  56.4370 10    1.7e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ntab_model(md_interaction_PC4) # R2 = 0.234 / 0.434\n\n\n\n \nPC 4\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-0.53\n-1.31 – 0.25\n0.187\n\n\nLevel [A]\n0.36\n-0.47 – 1.20\n0.392\n\n\nLevel [B]\n0.91\n0.08 – 1.74\n0.033\n\n\nLevel [C]\n1.28\n0.45 – 2.11\n0.003\n\n\nLevel [D]\n1.45\n0.62 – 2.28\n0.001\n\n\nLevel [E]\n1.64\n0.80 – 2.47\n&lt;0.001\n\n\nRegister [Fiction]\n0.93\n0.15 – 1.71\n0.019\n\n\nRegister [Informative]\n0.38\n-0.43 – 1.18\n0.358\n\n\nLevel [A] × Register[Fiction]\n-1.11\n-1.94 – -0.29\n0.008\n\n\nLevel [B] × Register[Fiction]\n-1.67\n-2.48 – -0.85\n&lt;0.001\n\n\nLevel [C] × Register[Fiction]\n-1.59\n-2.40 – -0.78\n&lt;0.001\n\n\nLevel [D] × Register[Fiction]\n-1.75\n-2.55 – -0.94\n&lt;0.001\n\n\nLevel [E] × Register[Fiction]\n-1.86\n-2.67 – -1.05\n&lt;0.001\n\n\nLevel [A] × Register[Informative]\n-0.92\n-1.78 – -0.07\n0.034\n\n\nLevel [B] × Register[Informative]\n-1.02\n-1.85 – -0.19\n0.016\n\n\nLevel [C] × Register[Informative]\n-1.00\n-1.83 – -0.18\n0.017\n\n\nLevel [D] × Register[Informative]\n-1.20\n-2.03 – -0.38\n0.004\n\n\nLevel [E] × Register[Informative]\n-1.16\n-1.99 – -0.32\n0.007\n\n\nRandom Effects\n\n\nσ2\n\n0.45\n\n\nτ00Source\n\n0.16\n\n\n\nICC\n0.26\n\n\n\nN Source\n\n325\n\n\nObservations\n4980\n\n\nMarginal R2 / Conditional R2\n\n0.234 / 0.434\n\n\n\n\nVisualisation of the coefficient estimates of the fixed effects:\n\nCodeplot_model(md_interaction_PC4, \n           #type = \"re\", # Option to visualise random effects \n           show.intercept = TRUE,\n           show.values=TRUE, \n           show.p=TRUE,\n           value.offset = .4,\n           value.size = 3.5,\n           colors = palette[c(1:3,7:9)],\n           group.terms = c(1,5,5,5,5,5,6,4,2,2,2,2,2,3,3,3,3,3), \n           title=\"Fixed effects\",\n           wrap.labels = 40,\n           axis.title = \"PC4 estimated coefficients\") +\n  theme_sjplot2() \n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TxBRef3Reg_PC4_lmer_fixed.svg\"), height = 6, width = 9)\n\n\nVisualisation of the predicted Dim4 scores:\n\nCode# svg(here(\"plots\", \"TxBReg3Reg_predicted_PC4_scores_interactions.svg\"), height = 8, width = 9)\nvisreg(md_interaction_PC4, xvar = \"Level\", by=\"Register\", \n       #type = \"contrast\",\n       type = \"conditional\",\n       line=list(col=\"darkred\"), \n       points=list(cex=0.3),\n       xlab = \"Ref. corpora and textbook level (A to E)\", ylab = \"PC4\",\n       layout=c(3,1)\n)\n\n\n\n\n\n\nCode# dev.off()\n\n\nWe can also explore the random effect structure.\n\n# Random effects\nranef &lt;- as.data.frame(ranef(md_interaction_PC4))\n\n# Exploring the random effects of the sources of the Info Teens corpus\nranef |&gt; \n  filter(grp %in% c(\"TeenVogue\", \"BBC\", \"Dogo\", \"Ducksters\", \"Encyclopedia\", \"Factmonster\", \"History\", \"Quatr\", \"Revision\", \"Science\", \"Science_Tech\", \"Teen\", \"TweenTribute\", \"WhyFiles\", \"World\")) |&gt; \n  ggplot(aes(x = grp, y = condval)) +\n  geom_point() +\n  coord_flip()\n\n\n\n\n\n\n# Exploring the random effects associated with textbook series\nranef |&gt; \n  filter(grp %in% levels(data$Series)) |&gt; \n  ggplot(aes(x = grp, y = condval)) +\n  geom_point() +\n  coord_flip()\n\n\n\n\n\n\n\n\nG.10.6 Testing model assumptions\nThis chunk can be used to check the assumptions of all of the models computed above. In the following example, we examine the final model selected to predict Dim2 scores.\n\nmodel2test &lt;- md_interaction_PC2\n\n# check distribution of residuals\nplot(model2test)\n\n\n\n\n\n\n# scale-location plot\nplot(model2test,\n     sqrt(abs(resid(.)))~fitted(.),\n     type=c(\"p\",\"smooth\"), col.line=1)\n\n\n\n\n\n\n# Q-Q plot\nlattice::qqmath(model2test)\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe Foll, Elen. n.d. “Schulenglisch: A Multi-Dimensional Model of the Variety of English Taught in German Secondary Schools.” AAA: Arbeiten Aus Anglistik Und Amerikanistik 49.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Analysis for the Model of Textbook English vs. ‘real-world’ English</span>"
    ]
  }
]