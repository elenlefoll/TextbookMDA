[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Textbook English: A Multi-Dimensional Approach",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "5a_TEC_data_prep.html",
    "href": "5a_TEC_data_prep.html",
    "title": "\n2  Data import from MFTE output\n",
    "section": "",
    "text": "2.1 Summary statistics\nCodeTxBcounts %&gt;% \n  group_by(Register) %&gt;% \n  summarise(totaltexts = n(), totalwords = sum(Words), mean = as.integer(mean(Words)), sd = as.integer(sd(Words)), TTRmean = mean(TTR))\n\n# A tibble: 6 × 6\n  Register      totaltexts totalwords  mean    sd TTRmean\n  &lt;fct&gt;              &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;\n1 Conversation         593     505147   851   301   0.438\n2 Fiction              285     241512   847   208   0.472\n3 Informative          364     304695   837   177   0.514\n4 Instructional        647     585049   904    94   0.421\n5 Personal              88      69570   790   177   0.477\n6 Poetry                37      26445   714   192   0.436\n\nCode#TxBcounts &lt;- saveRDS(TxBcounts, here(\"processed_data\", \"TxBcounts.rds\"))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data import from MFTE output</span>"
    ]
  },
  {
    "objectID": "5a_TEC_data_prep.html#removal-of-poetry-texts",
    "href": "5a_TEC_data_prep.html#removal-of-poetry-texts",
    "title": "\n2  Data import from MFTE output\n",
    "section": "\n3.1 Removal of Poetry texts",
    "text": "3.1 Removal of Poetry texts\n\nCodenrow(TxBcounts)\n\n[1] 2014\n\nCodeTxBcounts &lt;- TxBcounts %&gt;% \n  filter(Register!=\"Poetry\") %&gt;% \n  droplevels(.)\n\nnrow(TxBcounts)\n\n[1] 1977\n\nCodesummary(TxBcounts$Register)\n\n Conversation       Fiction   Informative Instructional      Personal \n          593           285           364           647            88",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data import from MFTE output</span>"
    ]
  },
  {
    "objectID": "5a_TEC_data_prep.html#feature-distributions",
    "href": "5a_TEC_data_prep.html#feature-distributions",
    "title": "\n2  Data import from MFTE output\n",
    "section": "\n3.2 Feature distributions",
    "text": "3.2 Feature distributions\n\nCodeTxBcounts %&gt;%\n  select(-Words) %&gt;% \n  keep(is.numeric) %&gt;% \n  tidyr::gather() %&gt;% # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2\n  ggplot(aes(value)) +\n    theme_bw() +\n    facet_wrap(~ key, scales = \"free\", ncol = 4) +\n    scale_x_continuous(expand=c(0,0)) +\n    geom_histogram(bins = 30, colour= \"darkred\", fill = \"darkred\", alpha = 0.5)\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TEC-HistogramPlotsAllVariablesTEC-only.svg\"), width = 20, height = 45)\n\n\n\n3.2.1 Feature removal I\n\nCode# Removal of meaningless features:\n# CD because numbers as digits were mostly removed from the textbooks\n# LIKE and SO because they are \"bin\" features designed to ensure that the counts for these two words don't inflate other categories due to mistags.\nTxBcounts &lt;- TxBcounts %&gt;% \n  select(-c(CD, LIKE, SO))\n\n# Function to compute percentage of texts with occurrences meeting a condition\ncompute_percentage &lt;- function(data, condition, threshold) {\n  numeric_data &lt;- Filter(is.numeric, data)\n  percentage &lt;- round(colSums(condition[, sapply(numeric_data, is.numeric)])/nrow(data) * 100, 2)\n  percentage &lt;- as.data.frame(percentage)\n  colnames(percentage) &lt;- \"Percentage\"\n  percentage &lt;- percentage %&gt;% \n    filter(!is.na(Percentage)) %&gt;%\n    rownames_to_column() %&gt;%\n    arrange(Percentage)\n  if (!missing(threshold)) {\n    percentage &lt;- percentage %&gt;% \n      filter(Percentage &gt; threshold)\n  }\n  return(percentage)\n}\n\n# Calculate percentage of texts with 0 occurrences of each feature\nzero_features &lt;- compute_percentage(TxBcounts, TxBcounts == 0, 66.6)\nprint(zero_features)\n\n   rowname Percentage\n1      GTO      67.07\n2     ELAB      69.30\n3     MDMM      70.81\n4     HGOT      73.75\n5     CONC      80.48\n6     DWNT      81.44\n7    QUTAG      85.99\n8     PGET      87.35\n9     ABLE      88.87\n10     URL      96.51\n11     EMO      97.82\n12     PRP      98.33\n13     HST      99.44\n\nCode# Combine low frequency features into meaningful groups whenever this makes linguistic sense\nTxBcounts &lt;- TxBcounts %&gt;% \n  mutate(JJPR = ABLE + JJPR, ABLE = NULL) %&gt;% \n  mutate(PASS = PGET + PASS, PGET = NULL)\n\n# Re-calculate percentage of texts with 0 occurrences of each feature\nzero_features &lt;- compute_percentage(TxBcounts, TxBcounts == 0, 66.6)\nprint(zero_features)\n\n   rowname Percentage\n1      GTO      67.07\n2     ELAB      69.30\n3     MDMM      70.81\n4     HGOT      73.75\n5     CONC      80.48\n6     DWNT      81.44\n7    QUTAG      85.99\n8      URL      96.51\n9      EMO      97.82\n10     PRP      98.33\n11     HST      99.44\n\nCode# Drop variables with low document frequency\nTxBcounts &lt;- select(TxBcounts, -one_of(zero_features$rowname))\nncol(TxBcounts)-8 # Number of linguistic features remaining\n\n[1] 64\n\nCodecolnames(TxBcounts)\n\n [1] \"Filename\" \"Country\"  \"Series\"   \"Level\"    \"Register\" \"Words\"   \n [7] \"ACT\"      \"AMP\"      \"ASPECT\"   \"AWL\"      \"BEMA\"     \"CAUSE\"   \n[13] \"CC\"       \"COMM\"     \"COND\"     \"CONT\"     \"CUZ\"      \"DEMO\"    \n[19] \"DMA\"      \"DOAUX\"    \"DT\"       \"EMPH\"     \"EX\"       \"EXIST\"   \n[25] \"FPP1P\"    \"FPP1S\"    \"FPUH\"     \"FREQ\"     \"HDG\"      \"IN\"      \n[31] \"JJAT\"     \"JJPR\"     \"LD\"       \"MDCA\"     \"MDCO\"     \"MDNE\"    \n[37] \"MDWO\"     \"MDWS\"     \"MENTAL\"   \"NCOMP\"    \"NN\"       \"OCCUR\"   \n[43] \"PASS\"     \"PEAS\"     \"PIT\"      \"PLACE\"    \"POLITE\"   \"POS\"     \n[49] \"PROG\"     \"QUAN\"     \"QUPR\"     \"RB\"       \"RP\"       \"SPLIT\"   \n[55] \"SPP2\"     \"STPR\"     \"THATD\"    \"THRC\"     \"THSC\"     \"TIME\"    \n[61] \"TPP3P\"    \"TPP3S\"    \"TTR\"      \"VBD\"      \"VBG\"      \"VBN\"     \n[67] \"VIMP\"     \"VPRT\"     \"WHQU\"     \"WHSC\"     \"XX0\"      \"YNQU\"    \n\n\n\n3.2.2 Standardising normalised counts and identifying potential outliers\n“As an alternative to removing very sparse feature, we apply a signed logarithmic transformation to deskew the feature distributions.” (Neumann & Evert)\n\nCode# First scale the normalised counts (z-standardisation) to be able to compare the various features\nTxBcounts %&gt;%\n  select(-Words) %&gt;% \n  keep(is.numeric) %&gt;% \n  scale() -&gt;\n  TxBzcounts\n\nboxplot(TxBzcounts, las = 3, main = \"z-scores\") # Slow to open!\n\n\n\n\n\n\nCode# If necessary, remove any outliers at this stage.\n\nTxBdata &lt;- cbind(TxBcounts[,1:6], as.data.frame(TxBzcounts))\nnrow(TxBdata)\n\n[1] 1977\n\nCodestr(TxBdata)\n\n'data.frame':   1977 obs. of  72 variables:\n $ Filename: chr  \"Achievers_A1_Instructional_0012.txt\" \"Solutions_Pre-Intermediate_Instructional_0023.txt\" \"POC_4e_Spoken_0007.txt\" \"Achievers_A2_Personal_0003.txt\" ...\n $ Country : Factor w/ 3 levels \"France\",\"Germany\",..: 3 3 1 3 3 1 2 3 2 2 ...\n $ Series  : Factor w/ 9 levels \"Access\",\"Achievers\",..: 2 9 8 2 2 8 1 2 7 1 ...\n $ Level   : Factor w/ 5 levels \"A\",\"B\",\"C\",\"D\",..: 1 2 3 2 4 2 4 4 1 1 ...\n $ Register: Factor w/ 5 levels \"Conversation\",..: 4 4 1 5 3 1 2 4 1 2 ...\n $ Words   : int  931 889 750 979 690 694 547 967 927 840 ...\n $ ACT     : num  -0.2569 1.5417 -0.0539 0.8188 2.531 ...\n $ AMP     : num  -0.929 -0.493 0.104 -0.533 -0.368 ...\n $ ASPECT  : num  0.2073 1.689 -0.0762 -0.6702 0.9881 ...\n $ AWL     : num  1.08 0.776 -1.218 -0.89 1.541 ...\n $ BEMA    : num  -0.984 -1.0355 0.2643 -0.0374 -0.3312 ...\n $ CAUSE   : num  -0.98019 -0.47668 -0.31353 0.29904 0.00587 ...\n $ CC      : num  -0.647 0.133 1.444 -0.136 3.627 ...\n $ COMM    : num  1.824 1.191 -0.525 -0.731 0.275 ...\n $ COND    : num  -0.1074 -0.8371 -0.1484 0.0438 -0.8371 ...\n $ CONT    : num  -0.847 -1.124 0.339 1.149 -0.815 ...\n $ CUZ     : num  -0.7024 -0.7024 0.3261 -0.0446 -0.7024 ...\n $ DEMO    : num  -0.448 -0.393 -1.039 -0.952 -1.305 ...\n $ DMA     : num  -0.564 -0.697 1.795 -0.443 -0.697 ...\n $ DOAUX   : num  0.706 0.335 -1.148 2.711 -1.495 ...\n $ DT      : num  0.0457 1.1124 0.806 -1.3939 -0.5334 ...\n $ EMPH    : num  -1.25 -0.99 0.294 0.406 -0.579 ...\n $ EX      : num  -0.579 -0.897 -0.897 0.254 -0.897 ...\n $ EXIST   : num  -0.726 -0.152 2.473 -0.287 4.154 ...\n $ FPP1P   : num  -0.788 -0.788 1.734 2.314 2.081 ...\n $ FPP1S   : num  -0.869 -0.852 2.336 1.553 -0.909 ...\n $ FPUH    : num  -0.597 -0.391 0.869 -0.41 -0.597 ...\n $ FREQ    : num  0.1757 -0.0252 -0.1645 -0.8775 -0.4375 ...\n $ HDG     : num  0.0361 -0.6537 3.6281 -0.6537 0.2769 ...\n $ IN      : num  0.366 2.282 0.283 -0.733 0.628 ...\n $ JJAT    : num  -1.192 -1.042 0.853 -1.296 2.129 ...\n $ JJPR    : num  -0.922 -0.876 -0.247 -0.12 -0.273 ...\n $ LD      : num  1.713 -0.179 -0.714 0.203 1.585 ...\n $ MDCA    : num  -0.751251 -0.274434 -0.774825 -0.15878 -0.000748 ...\n $ MDCO    : num  -0.719 -0.719 0.302 -0.719 -0.719 ...\n $ MDNE    : num  -0.88 -0.88 0.847 1.697 1.674 ...\n $ MDWO    : num  -0.38 0.181 8.472 0.415 -0.683 ...\n $ MDWS    : num  -0.201 -0.667 0.212 -0.105 2.583 ...\n $ MENTAL  : num  0.356 -0.107 1.626 0.746 -0.157 ...\n $ NCOMP   : num  0.528 -0.906 -0.621 3.722 2.299 ...\n $ NN      : num  0.47 0.741 -0.996 -0.509 1.432 ...\n $ OCCUR   : num  -0.627 0.371 1.943 -0.931 -0.931 ...\n $ PASS    : num  -0.607 -0.293 -0.136 -0.367 0.171 ...\n $ PEAS    : num  -0.858 -0.858 -0.858 -0.627 0.212 ...\n $ PIT     : num  -1.253 -1.505 -0.318 0.468 0.601 ...\n $ PLACE   : num  -0.802 -0.371 -1.035 0.371 0.916 ...\n $ POLITE  : num  -0.5 -0.5 0.742 0.451 -0.5 ...\n $ POS     : num  0.191 -1.304 -1.304 -1.304 -0.632 ...\n $ PROG    : num  0.5597 -0.0602 -0.1744 1.1647 0.8727 ...\n $ QUAN    : num  -1.03058 -0.00848 1.56447 0.33431 -0.80474 ...\n $ QUPR    : num  -0.679 -1.011 1.496 -0.61 0.843 ...\n $ RB      : num  -0.716 -1.646 -0.22 0.379 -1.157 ...\n $ RP      : num  -1.205 0.727 0.33 -0.877 1.065 ...\n $ SPLIT   : num  -0.89 -0.89 -0.242 -0.475 1.028 ...\n $ SPP2    : num  0.156 0.557 0.161 0.553 0.798 ...\n $ STPR    : num  -0.249 1.604 0.253 1.234 -0.815 ...\n $ THATD   : num  1.748 -1.041 -1.041 0.883 -1.041 ...\n $ THRC    : num  -0.685 -0.685 -0.685 -0.685 -0.685 ...\n $ THSC    : num  -0.348 -0.121 -0.882 -0.56 -0.137 ...\n $ TIME    : num  -0.4164 -0.6909 0.4315 0.0699 -0.7051 ...\n $ TPP3P   : num  -0.294 -0.558 -1.215 -0.937 0.715 ...\n $ TPP3S   : num  -0.596 -0.32 -0.769 -0.807 -0.874 ...\n $ TTR     : num  -0.92586 -0.31359 0.91095 -0.00745 2.3979 ...\n $ VBD     : num  -0.951 -0.594 0.662 -0.858 -0.189 ...\n $ VBG     : num  -0.959 0.875 0.208 0.907 1.607 ...\n $ VBN     : num  -0.818 0.376 -0.818 -0.818 0.741 ...\n $ VIMP    : num  1.963 1.658 -0.703 -0.539 -0.542 ...\n $ VPRT    : num  -0.635 -0.796 -1.059 1.438 0.34 ...\n $ WHQU    : num  1.4997 0.8327 0.0894 0.54 -1.0783 ...\n $ WHSC    : num  -0.569 -0.375 -0.5 -0.84 1.346 ...\n $ XX0     : num  -1.049 -0.667 -0.14 0.233 -0.684 ...\n $ YNQU    : num  -0.1051 1.1592 0.4093 0.0709 -1.037 ...\n\nCodeoutliers &lt;- TxBdata %&gt;% \n  select(-c(Words, LD, TTR)) %&gt;% \n  filter(if_any(where(is.numeric), ~ .x &gt; 8)) %&gt;% \n  select(Filename)\n\noutliers\n\n                                            Filename\n1                             POC_4e_Spoken_0007.txt\n2             Solutions_Elementary_Personal_0001.txt\n3                       NGL_5_Instructional_0018.txt\n4                           Access_1_Spoken_0011.txt\n5                              EIM_1_Spoken_0012.txt\n6                              NGL_4_Spoken_0011.txt\n7      Solutions_Intermediate_Plus_Personal_0001.txt\n8           Solutions_Elementary_ELF_Spoken_0021.txt\n9                          NB_2_Informative_0009.txt\n10       Solutions_Intermediate_Plus_Spoken_0022.txt\n11     Solutions_Intermediate_Instructional_0025.txt\n12 Solutions_Pre-Intermediate_Instructional_0024.txt\n13                            POC_4e_Spoken_0010.txt\n14            Solutions_Intermediate_Spoken_0019.txt\n15                          Access_1_Spoken_0019.txt\n16    Solutions_Pre-Intermediate_ELF_Spoken_0005.txt\n\nCodeTxBcounts &lt;- TxBcounts %&gt;% \n  filter(!Filename %in% outliers$Filename)\n\nnrow(TxBcounts)\n\n[1] 1961\n\nCodeTxBcounts %&gt;%\n  select(-Words) %&gt;% \n  keep(is.numeric) %&gt;% \n  scale() -&gt;\n  TxBzcounts\n\nnrow(TxBzcounts)\n\n[1] 1961\n\nCodeboxplot(TxBzcounts, las = 3, main = \"z-scores\") # Slow to open!\n\n\n\n\n\n\nCode#saveRDS(TxBcounts, here(\"processed_data\", \"TxBcounts3.rds\")) # Last saved 16 Feb 2024\n\n\n\nCodeTxBzcounts %&gt;%\n  as.data.frame() %&gt;% \n  gather() %&gt;% # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2\n  ggplot(aes(value)) +\n    theme_bw() +\n    facet_wrap(~ key, scales = \"free\", ncol = 4) +\n    scale_x_continuous(expand=c(0,0)) +\n    geom_histogram(bins = 30, colour= \"darkred\", fill = \"darkred\", alpha = 0.5)\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TEC-zscores-HistogramsAllVariablesTEC-only.svg\"), width = 20, height = 45)\n\n\n\n3.2.3 Transforming the features to (partially) deskew these distributions\nSigned log transformation function inspired by the SignedLog function proposed in https://cran.r-project.org/web/packages/DataVisualizations/DataVisualizations.pdf\n\nCode# All features are signed log-transformed (this is also what Neumann & Evert 2021 do)\nsigned.log &lt;- function(x) {\n  sign(x) * log(abs(x) + 1)\n  }\n\nTxBzlogcounts &lt;- signed.log(TxBzcounts) # Standardise first, then signed log transform\n\n# The function above would only transform the most skewed variables. This is what Lee suggests doing but it makes the interpretation of the correlations quite tricky so I abandoned this idea.\n# TxBzlogcounts2 &lt;- TxBzcounts %&gt;%\n#   as.data.frame() %&gt;% \n#     mutate(across(.cols = c(AMP, ASPECT, CAUSE, COND, CUZ, DMA, EMPH, EX, EXIST, FPP1P, FPP1S, FPUH, FREQ, HDG, MDCA, MDCO, MDNE, MDWO, MDWS, OCCUR, PASS, PEAS, PLACE, POLITE, PROG, QUPR, RP, SPLIT, STPR, THATD, THRC, THSC, TPP3P, TPP3S, VBD, VBG, VBN, VIMP, WHQU, WHSC, YNQU),\n#         .fns = signed.log)) %&gt;% \n#     rename_with(.cols = c(AMP, ASPECT, CAUSE, COND, CUZ, DMA, EMPH, EX, EXIST, FPP1P, FPP1S, FPUH, FREQ, HDG, MDCA, MDCO, MDNE, MDWO, MDWS, OCCUR, PASS, PEAS, PLACE, POLITE, PROG, QUPR, RP, SPLIT, STPR, THATD, THRC, THSC, TPP3P, TPP3S, VBD, VBG, VBN, VIMP, WHQU, WHSC, YNQU),\n#                 .fn = ~paste0(., '_signedlog'))\n\nboxplot(TxBzlogcounts, las=3, main=\"log-transformed z-scores\")\n\n\n\n\n\n\nCode#saveRDS(TxBzlogcounts, here(\"processed_data\", \"TxBzlogcounts.rds\")) # Last saved 16 Feb 2024\n\n\n\nCodeTxBzlogcounts %&gt;%\n  as.data.frame() %&gt;% \n  gather() %&gt;% # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2\n  ggplot(aes(value, after_stat(density))) +\n  theme_bw() +\n  facet_wrap(~ key, scales = \"free\", ncol = 4) +\n  scale_x_continuous(expand=c(0,0)) +\n  scale_y_continuous(limits = c(0,NA)) +\n  geom_histogram(bins = 30, colour= \"black\", fill = \"grey\") +\n  geom_density(colour = \"darkred\", weight = 2, fill=\"darkred\", alpha = .4)\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"DensityPlotsAllVariablesSignedLog-TEC-only.svg\"), width = 15, height = 49)\n\n\nThese plots serve to illustrate the effects of the variable transformations performed in the above chunks.\n\nCode# This is a slightly amended version of the PerformanceAnalytics::chart.Correlation() function. It simply removes the significance stars that are meaningless with this many data points (see commented out lines below)\n\nchart.Correlation.nostars &lt;- function (R, histogram = TRUE, method = c(\"pearson\", \"kendall\", \"spearman\"), ...) {\n  x = checkData(R, method = \"matrix\")\n  if (missing(method)) \n    method = method[1]\n  panel.cor &lt;- function(x, y, digits = 2, prefix = \"\", use = \"pairwise.complete.obs\", method = \"pearson\", cex.cor, ...) {\n    usr &lt;- par(\"usr\")\n    on.exit(par(usr))\n    par(usr = c(0, 1, 0, 1))\n    r &lt;- cor(x, y, use = use, method = method)\n    txt &lt;- format(c(r, 0.123456789), digits = digits)[1]\n    txt &lt;- paste(prefix, txt, sep = \"\")\n    if (missing(cex.cor)) \n      cex &lt;- 0.8/strwidth(txt)\n    test &lt;- cor.test(as.numeric(x), as.numeric(y), method = method)\n    # Signif &lt;- symnum(test$p.value, corr = FALSE, na = FALSE, \n    #                  cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1), symbols = c(\"***\", \n    #                                                                           \"**\", \"*\", \".\", \" \"))\n    text(0.5, 0.5, txt, cex = cex * (abs(r) + 0.3)/1.3)\n    # text(0.8, 0.8, Signif, cex = cex, col = 2)\n  }\n  f &lt;- function(t) {\n    dnorm(t, mean = mean(x), sd = sd.xts(x))\n  }\n  dotargs &lt;- list(...)\n  dotargs$method &lt;- NULL\n  rm(method)\n  hist.panel = function(x, ... = NULL) {\n    par(new = TRUE)\n    hist(x, col = \"light gray\", probability = TRUE, \n         axes = FALSE, main = \"\", breaks = \"FD\")\n    lines(density(x, na.rm = TRUE), col = \"red\", lwd = 1)\n    rug(x)\n  }\n  if (histogram) \n    pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor, \n          diag.panel = hist.panel)\n  else pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor)\n}\n\n# Example plot without any variable transformation\nexample1 &lt;- TxBcounts %&gt;%\n  select(NN,PROG,SPLIT,ACT,FPP1S)\n\n#png(here(\"plots\", \"CorrChart-TEC-examples-normedcounts.png\"), width = 20, height = 20, units = \"cm\", res = 300)\nchart.Correlation.nostars(example1, histogram=TRUE, pch=19)\n\n\n\n\n\n\nCodedev.off()\n\nnull device \n          1 \n\nCode# Example plot with transformed variables\nexample2 &lt;- TxBzlogcounts %&gt;%\n  as.data.frame() %&gt;% \n  select(NN,PROG,SPLIT,ACT,FPP1S)\n\n#png(here(\"plots\", \"CorrChart-TEC-examples-zsignedlogcounts.png\"), width = 20, height = 20, units = \"cm\", res = 300)\nchart.Correlation.nostars(example2, histogram=TRUE, pch=19)\ndev.off()\n\nnull device \n          1 \n\n\n\n3.2.4 Visualisation of feature correlations\n\nCode# Simple heatmap in base R (inspired by Stephanie Evert's SIGIL code)\ncor.colours &lt;- c(\n  hsv(h=2/3, v=1, s=(10:1)/10), # blue = negative correlation \n  rgb(1,1,1), # white = no correlation \n  hsv(h=0, v=1, s=(1:10/10))) # red = positive correlation\n\n#png(here(\"plots\", \"heatmapzlogcounts-TEC-only.png\"), width = 30, height= 30, units = \"cm\", res = 300)\nheatmap(cor(TxBzlogcounts), \n        symm=TRUE, \n        zlim=c(-1,1), \n        col=cor.colours, \n        margins=c(7,7))\n\n\n\n\n\n\nCode#dev.off()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data import from MFTE output</span>"
    ]
  },
  {
    "objectID": "5a_TEC_data_prep.html#composition-of-tec-textsfiles-entered-in-the-mdas",
    "href": "5a_TEC_data_prep.html#composition-of-tec-textsfiles-entered-in-the-mdas",
    "title": "\n2  Data import from MFTE output\n",
    "section": "\n3.3 Composition of TEC texts/files entered in the MDAs",
    "text": "3.3 Composition of TEC texts/files entered in the MDAs\n\nCode# Total number of words\nTxBcounts %&gt;% summarise(sum(Words))\n\n  sum(Words)\n1    1693650\n\nCodemetadata &lt;- TxBcounts %&gt;% \n  select(Filename, Country, Series, Level, Register, Words) %&gt;% \n  mutate(Volume = paste(Series, Level)) %&gt;% \n  mutate(Volume = fct_rev(Volume)) %&gt;% \n  mutate(Volume = fct_reorder(Volume, as.numeric(Level))) %&gt;% \n  group_by(Volume) %&gt;% \n  mutate(wordcount = sum(Words)) %&gt;% \n  ungroup() %&gt;% \n  distinct(Volume, .keep_all = TRUE)\n\n# Plot for book\nmetadata2 &lt;- TxBcounts %&gt;% \n  select(Country, Series, Level, Register, Words) %&gt;% \n  mutate(Volume = paste(Series, Level)) %&gt;% \n  mutate(Volume = fct_rev(Volume)) %&gt;% \n  #mutate(Volume = fct_reorder(Volume, as.numeric(Level))) %&gt;% \n  group_by(Volume, Register) %&gt;% \n  mutate(wordcount = sum(Words)) %&gt;% \n  ungroup() %&gt;% \n  distinct(Volume, Register, .keep_all = TRUE)\n\n# This is the palette created above on the basis of the suffrager pakcage (but without needed to install the package)\npalette &lt;- c(\"#BD241E\", \"#A18A33\", \"#15274D\", \"#D54E1E\", \"#EA7E1E\", \"#4C4C4C\", \"#722672\", \"#F9B921\", \"#267226\")\n\nPlotSp &lt;- metadata2 %&gt;% \n  filter(Country==\"Spain\") %&gt;% \n  #arrange(Volume) %&gt;% \n  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + \n    geom_bar(stat = \"identity\", position = \"stack\") +\n    coord_flip(expand = FALSE) + # Removes those annoying ticks before each bar label\n    theme_minimal() + theme(legend.position = \"none\") +\n    labs(x = \"Spain\", y = \"Cumulative word count\") +\n    scale_fill_manual(values = palette[c(5,4,3,2,1)], \n                      guide = guide_legend(reverse = TRUE))\n\nPlotGer &lt;- metadata2 %&gt;% \n  filter(Country==\"Germany\") %&gt;% \n  #arrange(Volume) %&gt;% \n  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + \n    geom_bar(stat = \"identity\", position = \"stack\") +\n    coord_flip(expand = FALSE) +\n    labs(x = \"Germany\", y = \"\") +\n    scale_fill_manual(values = palette[c(5,4,3,2,1)], guide = guide_legend(reverse = TRUE)) +\n    theme_minimal() + theme(legend.position = \"none\")\n\nPlotFr &lt;- metadata2 %&gt;% \n  filter(Country==\"France\") %&gt;% \n  #arrange(Volume) %&gt;% \n  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + \n    geom_bar(stat = \"identity\", position = \"stack\") +\n    coord_flip(expand = FALSE) +\n    labs(x = \"France\", y  = \"\", fill = \"Register subcorpus\") +\n    scale_fill_manual(values = palette[c(5,4,3,2,1)], guide = guide_legend(reverse = TRUE, legend.hjust = 0)) +\n    theme_minimal() + theme(legend.position = \"top\", legend.justification = \"left\")\n\nPlotFr /\nPlotGer /\nPlotSp\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TEC-T_wordcounts_book.svg\"), width = 8, height = 12)\n\n\n\nCode# Meta-data on % of instructional language in each textbook\nmetadataInstr &lt;- TxBcounts %&gt;% \n  select(Country, Series, Level, Register, Words) %&gt;% \n  filter(Register==\"Instructional\") %&gt;% \n  mutate(Volume = paste(Series, Register)) %&gt;% \n  mutate(Volume = fct_rev(Volume)) %&gt;% \n  mutate(Volume = fct_reorder(Volume, as.numeric(Level))) %&gt;% \n  group_by(Volume, Register) %&gt;% \n  mutate(InstrWordcount = sum(Words)) %&gt;% \n  ungroup() %&gt;% \n  distinct(Volume, .keep_all = TRUE) %&gt;% \n  select(Series, InstrWordcount)\n\nmetadataInstr\n\n# A tibble: 9 × 2\n  Series    InstrWordcount\n  &lt;fct&gt;              &lt;int&gt;\n1 Achievers         109886\n2 Solutions          87829\n3 EIM                59928\n4 HT                 51550\n5 Access             60938\n6 NGL                79312\n7 JTT                48375\n8 GreenLine          54263\n9 POC                30548\n\nCodemetaWordcount &lt;- TxBcounts %&gt;% \n  select(Country, Series, Level, Register, Words) %&gt;% \n  group_by(Series) %&gt;% \n  mutate(TECwordcount = sum(Words)) %&gt;% \n  ungroup() %&gt;% \n  distinct(Series, .keep_all = TRUE) %&gt;% \n  select(Series, TECwordcount)\n\nwordcount &lt;- merge(metaWordcount, metadataInstr, by = \"Series\")\n\nwordcount %&gt;% \n  mutate(InstrucPercent = InstrWordcount/TECwordcount*100) %&gt;% \n  arrange(InstrucPercent) %&gt;% \n  mutate(InstrucPercent = round(InstrucPercent, 2))\n\n     Series TECwordcount InstrWordcount InstrucPercent\n1    Access       259679          60938          23.47\n2       NGL       278316          79312          28.50\n3 GreenLine       172267          54263          31.50\n4 Solutions       270278          87829          32.50\n5       JTT       137557          48375          35.17\n6        HT       142676          51550          36.13\n7       POC        76714          30548          39.82\n8       EIM       147185          59928          40.72\n9 Achievers       208978         109886          52.58",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data import from MFTE output</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  }
]