[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Textbook English: A Multi-Dimensional Approach",
    "section": "",
    "text": "Preface\nThis Quarto book is work in progress. It will eventually contain the online supplements to:\n\nLe Foll, Elen. to appear. Textbook English: A Multi-Dimensional Approach [Studies in Corpus Linguistics]. Amsterdam: John Benjamins.\n\nThe book is based on my PhD thesis, which is accessible in Open Access:\n\nLe Foll, Elen. 2022. Textbook English: A Corpus-Based Analysis of the Language of EFL textbooks used in Secondary Schools in France, Germany and Spain. Osnabrück, Germany: Osnabrück University. PhD thesis. https://doi.org/10.48693/278.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "5a_TEC_data_prep.html",
    "href": "5a_TEC_data_prep.html",
    "title": "\n2  A Model of Intra-Textbook Linguistic Variation: Data Preparation\n",
    "section": "",
    "text": "2.1 Packages required\nThe following packages must be installed and loaded to process the data.\n#renv::restore() # Restore the project's dependencies from the lockfile to ensure that same package versions are used as in the original study\n\nlibrary(caret) # For its confusion matrix function\nlibrary(DT) # To display interactive HTML tables\nlibrary(here) # For dynamic file paths\nlibrary(knitr) # Loaded to display the tables using the kable() function\nlibrary(patchwork) # Needed to put together Fig. 1\nlibrary(PerformanceAnalytics) # For the correlation plot\nlibrary(psych) # For various useful, stats function\nlibrary(tidyverse) # For data wrangling",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A Model of Intra-Textbook Linguistic Variation: Data Preparation</span>"
    ]
  },
  {
    "objectID": "5a_TEC_data_prep.html#data-import-from-mfte-output",
    "href": "5a_TEC_data_prep.html#data-import-from-mfte-output",
    "title": "\n2  A Model of Intra-Textbook Linguistic Variation: Data Preparation\n",
    "section": "\n2.2 Data import from MFTE output",
    "text": "2.2 Data import from MFTE output\nThe raw data used in this script is a tab-separated file that corresponds to the tabular output of mixed normalised frequencies as generated by the MFTE Perl v. 3.1 (Le Foll 2021a).\n\nCode# Read in Textbook Corpus data\nTxBcounts &lt;- read.delim(here(\"MFTE_data\", \"Outputs\", \"TxB900MDA_3.1_normed_complex_counts.tsv\"), header = TRUE, stringsAsFactors = TRUE)\nTxBcounts &lt;- TxBcounts |&gt; \n  filter(Filename!=\".DS_Store\") |&gt;  \n  droplevels()\n#str(TxBcounts) # Check sanity of data\n#nrow(TxBcounts) # Should be 2014 files\ndatatable(TxBcounts,\n  filter = \"top\",\n) |&gt; \n  formatRound(2:ncol(TxBcounts), digits=2)\n\n\n\n\n\nMetadata was added on the basis of the filenames.\n\n# Adding a textbook proficiency level\nTxBLevels &lt;- read.delim(here(\"metadata\", \"TxB900MDA_ProficiencyLevels.csv\"), sep = \",\")\nTxBcounts &lt;- full_join(TxBcounts, TxBLevels, by = \"Filename\") |&gt;  \n  mutate(Level = as.factor(Level)) |&gt;  \n  mutate(Filename = as.factor(Filename))\n\n# Check distribution and that there are no NAs\nsummary(TxBcounts$Level) |&gt; \n  kable(col.names = c(\"Textbook Level\", \"# of texts\"))\n\n\n\nTextbook Level\n# of texts\n\n\n\nA\n292\n\n\nB\n407\n\n\nC\n506\n\n\nD\n478\n\n\nE\n331\n\n\n\n\n# Check matching on random sample\n# TxBcounts |&gt;\n#   select(Filename, Level) |&gt;  \n#   sample_n(20) \n\n# Adding a register variable from the file names\nTxBcounts$Register &lt;- as.factor(stringr::str_extract(TxBcounts$Filename, \"Spoken|Narrative|Other|Personal|Informative|Instructional|Poetry\")) # Add a variable for Textbook Register\nsummary(TxBcounts$Register) |&gt; \n  kable(col.names = c(\"Textbook Register\", \"# of texts\"))\n\n\n\nTextbook Register\n# of texts\n\n\n\nInformative\n364\n\n\nInstructional\n647\n\n\nNarrative\n285\n\n\nPersonal\n88\n\n\nPoetry\n37\n\n\nSpoken\n593\n\n\n\n\nTxBcounts$Register &lt;- car::recode(TxBcounts$Register, \"'Narrative' = 'Fiction'; 'Spoken' = 'Conversation'\")\n#colnames(TxBcounts) # Check all the variables make sense\n\n# Adding a textbook series variable from the file names\nTxBcounts$Filename &lt;- stringr::str_replace(TxBcounts$Filename, \"English_In_Mind|English_in_Mind\", \"EIM\") \nTxBcounts$Filename &lt;- stringr::str_replace(TxBcounts$Filename, \"New_GreenLine\", \"NGL\") # Otherwise the regex for GreenLine will override New_GreenLine\nTxBcounts$Filename &lt;- stringr::str_replace(TxBcounts$Filename, \"Piece_of_cake\", \"POC\") # Shorten label for ease of plotting\nTxBcounts$Series &lt;- as.factor(stringr::str_extract(TxBcounts$Filename, \"Access|Achievers|EIM|GreenLine|HT|NB|NM|POC|JTT|NGL|Solutions\")) # Extract textbook series from (ammended) filenames\nsummary(TxBcounts$Series)  |&gt; \n  kable(col.names = c(\"Textbook Name\", \"# of texts\"))\n\n\n\nTextbook Name\n# of texts\n\n\n\nAccess\n315\n\n\nAchievers\n240\n\n\nEIM\n180\n\n\nGreenLine\n209\n\n\nHT\n115\n\n\nJTT\n129\n\n\nNB\n44\n\n\nNGL\n298\n\n\nNM\n59\n\n\nPOC\n98\n\n\nSolutions\n327\n\n\n\n\n# Including the French textbooks for the first year of Lycée to their corresponding publisher series from collège\nTxBcounts$Series &lt;-car::recode(TxBcounts$Series, \"c('NB', 'JTT') = 'JTT'; c('NM', 'HT') = 'HT'\") # # Recode final volumes of French series (see Section 4.3.1.1 on textbook selection for details)\nsummary(TxBcounts$Series) |&gt; \n  kable(col.names = c(\"Textbook Series\", \"# of texts\"))\n\n\n\nTextbook Series\n# of texts\n\n\n\nAccess\n315\n\n\nAchievers\n240\n\n\nEIM\n180\n\n\nGreenLine\n209\n\n\nHT\n174\n\n\nJTT\n173\n\n\nNGL\n298\n\n\nPOC\n98\n\n\nSolutions\n327\n\n\n\n\n# Adding a textbook country of use variable from the series variable\nTxBcounts$Country &lt;- TxBcounts$Series\nTxBcounts$Country &lt;- car::recode(TxBcounts$Series, \"c('Access', 'GreenLine', 'NGL') = 'Germany'; c('Achievers', 'EIM', 'Solutions') = 'Spain'; c('HT', 'NB', 'NM', 'POC', 'JTT') = 'France'\")\nsummary(TxBcounts$Country) |&gt; \n  kable(col.names = c(\"Country of Use\", \"# of texts\"))\n\n\n\nCountry of Use\n# of texts\n\n\n\nFrance\n445\n\n\nGermany\n822\n\n\nSpain\n747\n\n\n\n\n# Re-order variables\n#colnames(TxBcounts)\nTxBcounts &lt;- select(TxBcounts, order(names(TxBcounts))) %&gt;%\n  select(Filename, Country, Series, Level, Register, Words, everything())\n#colnames(TxBcounts)\n\n\n2.2.1 Corpus size\nThis table provides some summary statistics about the number of words included in the TEC texts originally tagged for this study.\n\nTxBcounts  |&gt;  \n  group_by(Register) |&gt;  \n  summarise(totaltexts = n(), totalwords = sum(Words), mean = as.integer(mean(Words)), sd = as.integer(sd(Words)), TTRmean = mean(TTR)) |&gt;  \n  kable(digits = 2, format.args = list(big.mark = \",\"))\n\n\n\nRegister\ntotaltexts\ntotalwords\nmean\nsd\nTTRmean\n\n\n\nConversation\n593\n505,147\n851\n301\n0.44\n\n\nFiction\n285\n241,512\n847\n208\n0.47\n\n\nInformative\n364\n304,695\n837\n177\n0.51\n\n\nInstructional\n647\n585,049\n904\n94\n0.42\n\n\nPersonal\n88\n69,570\n790\n177\n0.48\n\n\nPoetry\n37\n26,445\n714\n192\n0.44\n\n\n\n\n#TxBcounts &lt;- saveRDS(TxBcounts, here(\"processed_data\", \"TxBcounts.rds\"))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A Model of Intra-Textbook Linguistic Variation: Data Preparation</span>"
    ]
  },
  {
    "objectID": "5a_TEC_data_prep.html#data-preparation-for-pca",
    "href": "5a_TEC_data_prep.html#data-preparation-for-pca",
    "title": "\n2  A Model of Intra-Textbook Linguistic Variation: Data Preparation\n",
    "section": "\n2.3 Data preparation for PCA",
    "text": "2.3 Data preparation for PCA\nPoetry texts were removed for this analysis as there were too few compared to the other register categories.\n\nsummary(TxBcounts$Register) |&gt;  \n  kable(col.names = c(\"Register\", \"# texts\"))\n\n\n\nRegister\n# texts\n\n\n\nConversation\n593\n\n\nFiction\n285\n\n\nInformative\n364\n\n\nInstructional\n647\n\n\nPersonal\n88\n\n\nPoetry\n37\n\n\n\n\n\nThis led to the following distribution of texts across the five textbook English registers examined in the model of intra-textbook linguistic variation:\n\nTxBcounts &lt;- TxBcounts |&gt;  \n  filter(Register!=\"Poetry\") |&gt;  \n  droplevels()\n\nsummary(TxBcounts$Register) |&gt;  \n  kable(col.names = c(\"Register\", \"# texts\"))\n\n\n\nRegister\n# texts\n\n\n\nConversation\n593\n\n\nFiction\n285\n\n\nInformative\n364\n\n\nInstructional\n647\n\n\nPersonal\n88\n\n\n\n\n\n\n2.3.1 Feature distributions\nThe distributions of each linguistic features were examined by means of visualisation. As shown below, before transformation, many of the features displayed highly skewed distributions.\n\nCodeTxBcounts |&gt; \n  select(-Words) |&gt;  \n  keep(is.numeric) |&gt;  \n  tidyr::gather() |&gt;  # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2\n  ggplot(aes(value)) +\n    theme_bw() +\n    facet_wrap(~ key, scales = \"free\", ncol = 4) +\n    scale_x_continuous(expand=c(0,0)) +\n    geom_histogram(bins = 30, colour= \"darkred\", fill = \"darkred\", alpha = 0.5)\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TEC-HistogramPlotsAllVariablesTEC-only.svg\"), width = 20, height = 45)\n\n\n\n2.3.2 Feature removal\nA number of features were removed from the dataset as they are not linguistically interpretable. In the case of the TEC, this included the variable CD because numbers spellt out as digits were removed from the textbooks before these were tagged with the MFTE. In addition, the variables LIKE and SO because these are “bin” features included in the output of the MFTE to ensure that the counts for these polysemous words do not inflate other categories due to mistags (Le Foll 2021b).\nWhenever linguistically meaningful, very low-frequency features were merged. Finally, features absent from more than third of texts were also excluded. For the analysis intra-textbook register variation, the following linguistic features were excluded from the analysis due to low dispersion:\n\n# Removal of meaningless features:\nTxBcounts &lt;- TxBcounts |&gt;  \n  select(-c(CD, LIKE, SO))\n\n# Function to compute percentage of texts with occurrences meeting a condition\ncompute_percentage &lt;- function(data, condition, threshold) {\n  numeric_data &lt;- Filter(is.numeric, data)\n  percentage &lt;- round(colSums(condition[, sapply(numeric_data, is.numeric)])/nrow(data) * 100, 2)\n  percentage &lt;- as.data.frame(percentage)\n  colnames(percentage) &lt;- \"Percentage\"\n  percentage &lt;- percentage |&gt;  \n    filter(!is.na(Percentage)) |&gt; \n    rownames_to_column() |&gt; \n    arrange(Percentage)\n  if (!missing(threshold)) {\n    percentage &lt;- percentage |&gt;  \n      filter(Percentage &gt; threshold)\n  }\n  return(percentage)\n}\n\n# Calculate percentage of texts with 0 occurrences of each feature\nzero_features &lt;- compute_percentage(TxBcounts, TxBcounts == 0, 66.6)\n#print(zero_features)\n\n# Combine low frequency features into meaningful groups whenever this makes linguistic sense\nTxBcounts &lt;- TxBcounts |&gt;  \n  mutate(JJPR = ABLE + JJPR, ABLE = NULL) |&gt;  \n  mutate(PASS = PGET + PASS, PGET = NULL)\n\n# Re-calculate percentage of texts with 0 occurrences of each feature\nzero_features2 &lt;- compute_percentage(TxBcounts, TxBcounts == 0, 66.6)\nprint(zero_features2)\n\n   rowname Percentage\n1      GTO      67.07\n2     ELAB      69.30\n3     MDMM      70.81\n4     HGOT      73.75\n5     CONC      80.48\n6     DWNT      81.44\n7    QUTAG      85.99\n8      URL      96.51\n9      EMO      97.82\n10     PRP      98.33\n11     HST      99.44\n\n# Drop variables with low document frequency\nTxBcounts &lt;- select(TxBcounts, -one_of(zero_features2$rowname))\n#ncol(TxBcounts)-8 # Number of linguistic features remaining\n\n# List of features\n#colnames(TxBcounts)\n\nThese feature removal operations resulted in a feature set of 64 linguistic variables.\n\n2.3.3 Identifying potential outlier texts\nAll normalised frequencies were normalised to identify any potential outlier texts.\n\n# First scale the normalised counts (z-standardisation) to be able to compare the various features\nTxBcounts |&gt; \n  select(-Words) |&gt;  \n  keep(is.numeric) |&gt;  \n  scale() -&gt;\n  TxBzcounts\n\nboxplot(TxBzcounts, las = 3, main = \"z-scores\") # Slow to open!\n\n\n\n\n\n\n# If necessary, remove any outliers at this stage.\nTxBdata &lt;- cbind(TxBcounts[,1:6], as.data.frame(TxBzcounts))\n\noutliers &lt;- TxBdata |&gt;  \n  select(-c(Words, LD, TTR)) |&gt;  \n  filter(if_any(where(is.numeric), ~ .x &gt; 8)) |&gt;  \n  select(Filename)\n\nThe following outlier texts were identified and excluded in subsequent analyses.\n\nCodeoutliers\n\n                                            Filename\n1                             POC_4e_Spoken_0007.txt\n2             Solutions_Elementary_Personal_0001.txt\n3                       NGL_5_Instructional_0018.txt\n4                           Access_1_Spoken_0011.txt\n5                              EIM_1_Spoken_0012.txt\n6                              NGL_4_Spoken_0011.txt\n7      Solutions_Intermediate_Plus_Personal_0001.txt\n8           Solutions_Elementary_ELF_Spoken_0021.txt\n9                          NB_2_Informative_0009.txt\n10       Solutions_Intermediate_Plus_Spoken_0022.txt\n11     Solutions_Intermediate_Instructional_0025.txt\n12 Solutions_Pre-Intermediate_Instructional_0024.txt\n13                            POC_4e_Spoken_0010.txt\n14            Solutions_Intermediate_Spoken_0019.txt\n15                          Access_1_Spoken_0019.txt\n16    Solutions_Pre-Intermediate_ELF_Spoken_0005.txt\n\nCodeTxBcounts &lt;- TxBcounts |&gt;  \n  filter(!Filename %in% outliers$Filename)\n\nTxBcounts |&gt; \n  select(-Words) |&gt;  \n  keep(is.numeric) |&gt;  \n  scale() -&gt;\n  TxBzcounts\n\n\nThis resulted in 1,961 TEC texts being included in the model of intra-textbook linguistic variation with the following normalised feature distributions.\n\nCodeTxBzcounts |&gt; \n  as.data.frame() |&gt;  \n  gather() |&gt;  # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2\n  ggplot(aes(value)) +\n    theme_bw() +\n    facet_wrap(~ key, scales = \"free\", ncol = 4) +\n    scale_x_continuous(expand=c(0,0)) +\n    geom_histogram(bins = 30, colour= \"darkred\", fill = \"darkred\", alpha = 0.5)\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TEC-zscores-HistogramsAllVariablesTEC-only.svg\"), width = 20, height = 45)\n\n\n\n2.3.4 Signed log transformation\nA signed logarithmic transformation was applied to (further) deskew the feature distributions (Diwersy, Evert, and Neumann 2014; Neumann and Evert 2021).\nThe signed log transformation function was inspired by the SignedLog function proposed in https://cran.r-project.org/web/packages/DataVisualizations/DataVisualizations.pdf\n\n# All features are signed log-transformed (note that this is also what Neumann & Evert 2021 propose)\nsigned.log &lt;- function(x) {\n  sign(x) * log(abs(x) + 1)\n  }\n\nTxBzlogcounts &lt;- signed.log(TxBzcounts) # Standardise first, then signed log transform\n\n#saveRDS(TxBzlogcounts, here(\"processed_data\", \"TxBzlogcounts.rds\")) # Last saved 16 Feb 2024\n\nThe new feature distributions are visualised below.\n\nCodeTxBzlogcounts |&gt; \n  as.data.frame() |&gt;  \n  gather() |&gt;  # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2\n  ggplot(aes(value, after_stat(density))) +\n  theme_bw() +\n  facet_wrap(~ key, scales = \"free\", ncol = 4) +\n  scale_x_continuous(expand=c(0,0)) +\n  scale_y_continuous(limits = c(0,NA)) +\n  geom_histogram(bins = 30, colour= \"black\", fill = \"grey\") +\n  geom_density(colour = \"darkred\", weight = 2, fill=\"darkred\", alpha = .4)\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"DensityPlotsAllVariablesSignedLog-TEC-only.svg\"), width = 15, height = 49)\n\n\nThe following correlation plots serve to illustrate the effect of the variable transformations performed in the above chunks.\nExample feature distributions before transformations:\n\nCode# This is a slightly amended version of the PerformanceAnalytics::chart.Correlation() function. It simply removes the significance stars that are meaningless with this many data points (see commented out lines below)\n\nchart.Correlation.nostars &lt;- function (R, histogram = TRUE, method = c(\"pearson\", \"kendall\", \"spearman\"), ...) {\n  x = checkData(R, method = \"matrix\")\n  if (missing(method)) \n    method = method[1]\n  panel.cor &lt;- function(x, y, digits = 2, prefix = \"\", use = \"pairwise.complete.obs\", method = \"pearson\", cex.cor, ...) {\n    usr &lt;- par(\"usr\")\n    on.exit(par(usr))\n    par(usr = c(0, 1, 0, 1))\n    r &lt;- cor(x, y, use = use, method = method)\n    txt &lt;- format(c(r, 0.123456789), digits = digits)[1]\n    txt &lt;- paste(prefix, txt, sep = \"\")\n    if (missing(cex.cor)) \n      cex &lt;- 0.8/strwidth(txt)\n    test &lt;- cor.test(as.numeric(x), as.numeric(y), method = method)\n    # Signif &lt;- symnum(test$p.value, corr = FALSE, na = FALSE, \n    #                  cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1), symbols = c(\"***\", \n    #                                                                           \"**\", \"*\", \".\", \" \"))\n    text(0.5, 0.5, txt, cex = cex * (abs(r) + 0.3)/1.3)\n    # text(0.8, 0.8, Signif, cex = cex, col = 2)\n  }\n  f &lt;- function(t) {\n    dnorm(t, mean = mean(x), sd = sd.xts(x))\n  }\n  dotargs &lt;- list(...)\n  dotargs$method &lt;- NULL\n  rm(method)\n  hist.panel = function(x, ... = NULL) {\n    par(new = TRUE)\n    hist(x, col = \"light gray\", probability = TRUE, \n         axes = FALSE, main = \"\", breaks = \"FD\")\n    lines(density(x, na.rm = TRUE), col = \"red\", lwd = 1)\n    rug(x)\n  }\n  if (histogram) \n    pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor, \n          diag.panel = hist.panel)\n  else pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor)\n}\n\n# Example plot without any variable transformation\nexample1 &lt;- TxBcounts |&gt; \n  select(NN,PROG,SPLIT,ACT,FPP1S)\n\n#png(here(\"plots\", \"CorrChart-TEC-examples-normedcounts.png\"), width = 20, height = 20, units = \"cm\", res = 300)\nchart.Correlation.nostars(example1, histogram=TRUE, pch=19)\n\n\n\n\n\n\nCode#dev.off()\n\n\nExample feature distributions after transformations:\n\nCode# Example plot with transformed variables\nexample2 &lt;- TxBzlogcounts |&gt; \n  as.data.frame() |&gt;  \n  select(NN,PROG,SPLIT,ACT,FPP1S)\n\n#png(here(\"plots\", \"CorrChart-TEC-examples-zsignedlogcounts.png\"), width = 20, height = 20, units = \"cm\", res = 300)\nchart.Correlation.nostars(example2, histogram=TRUE, pch=19)\n\n\n\n\n\n\nCode#dev.off()\n\n\n\n2.3.5 Feature correlations\nThe correlations of the transformed feature frequencies can be visualised in the form of a heatmap. Negative correlations are rendered in blue, whereas positive ones are in red.\n\nCode# Simple heatmap in base R (inspired by Stephanie Evert's SIGIL code)\ncor.colours &lt;- c(\n  hsv(h=2/3, v=1, s=(10:1)/10), # blue = negative correlation \n  rgb(1,1,1), # white = no correlation \n  hsv(h=0, v=1, s=(1:10/10))) # red = positive correlation\n\n#png(here(\"plots\", \"heatmapzlogcounts-TEC-only.png\"), width = 30, height= 30, units = \"cm\", res = 300)\nheatmap(cor(TxBzlogcounts), \n        symm=TRUE, \n        zlim=c(-1,1), \n        col=cor.colours, \n        margins=c(0,0))\n\n\n\n\n\n\nCode#dev.off()\n\n# Calculate the sum of all the words in the tagged texts of the TEC\ntotalwords &lt;- TxBcounts |&gt;  \n  select(Words) |&gt; \n  sum() |&gt; \n  format(big.mark=\",\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A Model of Intra-Textbook Linguistic Variation: Data Preparation</span>"
    ]
  },
  {
    "objectID": "5a_TEC_data_prep.html#composition-of-tec-textsfiles",
    "href": "5a_TEC_data_prep.html#composition-of-tec-textsfiles",
    "title": "\n2  A Model of Intra-Textbook Linguistic Variation: Data Preparation\n",
    "section": "\n2.4 Composition of TEC texts/files",
    "text": "2.4 Composition of TEC texts/files\nThese figures and tables provide summary statistics on the texts/files of the TEC that were entered in the multi-dimensional model of intra-textbook linguistic variation. In total, the TEC texts entered amounted to 1,693,650 words.\n\nCodemetadata &lt;- TxBcounts |&gt;  \n  select(Filename, Country, Series, Level, Register, Words) |&gt;  \n  mutate(Volume = paste(Series, Level)) |&gt;  \n  mutate(Volume = fct_rev(Volume)) |&gt;  \n  mutate(Volume = fct_reorder(Volume, as.numeric(Level))) |&gt;  \n  group_by(Volume) |&gt;  \n  mutate(wordcount = sum(Words)) |&gt;  \n  ungroup() |&gt;  \n  distinct(Volume, .keep_all = TRUE)\n\n# Plot for book\nmetadata2 &lt;- TxBcounts |&gt;  \n  select(Country, Series, Level, Register, Words) |&gt;  \n  mutate(Volume = paste(Series, Level)) |&gt;  \n  mutate(Volume = fct_rev(Volume)) |&gt;  \n  #mutate(Volume = fct_reorder(Volume, as.numeric(Level))) |&gt;  \n  group_by(Volume, Register) |&gt;  \n  mutate(wordcount = sum(Words)) |&gt;  \n  ungroup() |&gt;  \n  distinct(Volume, Register, .keep_all = TRUE)\n\n# This is the palette created above on the basis of the suffrager pakcage (but without needed to install the package)\npalette &lt;- c(\"#BD241E\", \"#A18A33\", \"#15274D\", \"#D54E1E\", \"#EA7E1E\", \"#4C4C4C\", \"#722672\", \"#F9B921\", \"#267226\")\n\nPlotSp &lt;- metadata2 |&gt;  \n  filter(Country==\"Spain\") |&gt;  \n  #arrange(Volume) |&gt;  \n  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + \n    geom_bar(stat = \"identity\", position = \"stack\") +\n    coord_flip(expand = FALSE) + # Removes those annoying ticks before each bar label\n    theme_minimal() + theme(legend.position = \"none\") +\n    labs(x = \"Spain\", y = \"Cumulative word count\") +\n    scale_fill_manual(values = palette[c(5,4,3,2,1)], \n                      guide = guide_legend(reverse = TRUE))\n\nPlotGer &lt;- metadata2 |&gt;  \n  filter(Country==\"Germany\") |&gt;  \n  #arrange(Volume) |&gt;  \n  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + \n    geom_bar(stat = \"identity\", position = \"stack\") +\n    coord_flip(expand = FALSE) +\n    labs(x = \"Germany\", y = \"\") +\n    scale_fill_manual(values = palette[c(5,4,3,2,1)], guide = guide_legend(reverse = TRUE)) +\n    theme_minimal() + theme(legend.position = \"none\")\n\nPlotFr &lt;- metadata2 |&gt;  \n  filter(Country==\"France\") |&gt;  \n  #arrange(Volume) |&gt;  \n  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + \n    geom_bar(stat = \"identity\", position = \"stack\") +\n    coord_flip(expand = FALSE) +\n    labs(x = \"France\", y  = \"\", fill = \"Register subcorpus\") +\n    scale_fill_manual(values = palette[c(5,4,3,2,1)], guide = guide_legend(reverse = TRUE, legend.hjust = 0)) +\n    theme_minimal() + theme(legend.position = \"top\", legend.justification = \"left\")\n\nPlotFr /\nPlotGer /\nPlotSp\n\n\n\n\n\n\nCode#ggsave(here(\"plots\", \"TEC-T_wordcounts_book.svg\"), width = 8, height = 12)\n\n\nThe following table provides information about the proportion of instructional language featured in each textbook series.\n\nCodemetadataInstr &lt;- TxBcounts |&gt;  \n  select(Country, Series, Level, Register, Words) |&gt;  \n  filter(Register==\"Instructional\") |&gt;  \n  mutate(Volume = paste(Series, Register)) |&gt;  \n  mutate(Volume = fct_rev(Volume)) |&gt;  \n  mutate(Volume = fct_reorder(Volume, as.numeric(Level))) |&gt;  \n  group_by(Volume, Register) |&gt;  \n  mutate(InstrWordcount = sum(Words)) |&gt;  \n  ungroup() |&gt;  \n  distinct(Volume, .keep_all = TRUE) |&gt;  \n  select(Series, InstrWordcount)\n\nmetaWordcount &lt;- TxBcounts |&gt;  \n  select(Country, Series, Level, Register, Words) |&gt;  \n  group_by(Series) |&gt;  \n  mutate(TECwordcount = sum(Words)) |&gt;  \n  ungroup() |&gt;  \n  distinct(Series, .keep_all = TRUE) |&gt;  \n  select(Series, TECwordcount)\n\nwordcount &lt;- merge(metaWordcount, metadataInstr, by = \"Series\")\n\nwordcount |&gt;  \n  mutate(InstrucPercent = InstrWordcount/TECwordcount*100) |&gt;  \n  arrange(InstrucPercent) |&gt;  \n  mutate(InstrucPercent = round(InstrucPercent, 2)) |&gt;  \n  kable(col.names = c(\"Textbook Series\", \"Total words\", \"Instructional words\", \"% of textbook content\"), \n        digits = 2, \n        format.args = list(big.mark = \",\"))\n\n\n\n\n\n\n\n\n\nTextbook Series\nTotal words\nInstructional words\n% of textbook content\n\n\n\nAccess\n259,679\n60,938\n23.47\n\n\nNGL\n278,316\n79,312\n28.50\n\n\nGreenLine\n172,267\n54,263\n31.50\n\n\nSolutions\n270,278\n87,829\n32.50\n\n\nJTT\n137,557\n48,375\n35.17\n\n\nHT\n142,676\n51,550\n36.13\n\n\nPOC\n76,714\n30,548\n39.82\n\n\nEIM\n147,185\n59,928\n40.72\n\n\nAchievers\n208,978\n109,886\n52.58\n\n\n\n\n\n\n\n\n\n\n\nDiwersy, Sascha, Stephanie Evert, and Stella Neumann. 2014. “A Weakly Supervised Multivariate Approach to the Study of Language Variation.” In, edited by Benedikt Szmrecsanyi and Bernhard Wälchli, 174–204. Berlin: De Gruyter.\n\n\nLe Foll, Elen. 2021a. Introducing the Multi-Feature Tagger of English (MFTE). Osnabrück University. https://github.com/elenlefoll/MultiFeatureTaggerEnglish.\n\n\n———. 2021b. Introducing the Multi-Feature Tagger of English (MFTE). Osnabrück University. https://github.com/elenlefoll/MultiFeatureTaggerEnglish.\n\n\nNeumann, Stella, and Stephanie Evert. 2021. “A Register Variation Perspective on Varieties of English.” In, edited by Elena Seoane and Douglas Biber, 144178. Studies in Corpus Linguistics 103. Amsterdam: Benjamins.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A Model of Intra-Textbook Linguistic Variation: Data Preparation</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Diwersy, Sascha, Stephanie Evert, and Stella Neumann. 2014. “A\nWeakly Supervised Multivariate Approach to the Study of Language\nVariation.” In, edited by Benedikt Szmrecsanyi and Bernhard\nWälchli, 174–204. Berlin: De Gruyter.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nLe Foll, Elen. 2021a. Introducing the Multi-Feature Tagger of\nEnglish (MFTE). Osnabrück University. https://github.com/elenlefoll/MultiFeatureTaggerEnglish.\n\n\n———. 2021b. Introducing the Multi-Feature Tagger of English\n(MFTE). Osnabrück University. https://github.com/elenlefoll/MultiFeatureTaggerEnglish.\n\n\nNeumann, Stella, and Stephanie Evert. 2021. “A Register Variation\nPerspective on Varieties of English.” In, edited by Elena Seoane\nand Douglas Biber, 144178. Studies in Corpus Linguistics 103. Amsterdam:\nBenjamins.",
    "crumbs": [
      "References"
    ]
  }
]