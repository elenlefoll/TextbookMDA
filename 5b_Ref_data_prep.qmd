---
title: "Data preparation"
author: "Elen Le Foll"
date: "2024-02-13"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
  editor_options: 
    markdown: 
      wrap: sentence
bibliography: "packages.bib"
nocite: '@*'
      
---

Please note that the plot dimensions in this notebook have been optimised for the print version of the thesis.

*Built with R `r getRversion()`*  
*Last saved on `r format(Sys.Date(), "%d %B %Y")` at `r format(Sys.time(), "%H:%M")`*

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, paged.print=TRUE, fig.width = 10, warning=FALSE, message=FALSE)
options(scipen=999)

#renv::restore() # Restore the project's dependencies from the lockfile to ensure that same package versions are used as in the original thesis.

library(broom.mixed) # For checking singularity issues 
library(car) # For recoding data
library(cowplot) # For nice plots
library(emmeans) # Comparing group means of predicted values
library(GGally) # For ggpairs
library(gridExtra) # For making large faceted plots
library(here) # For ease of sharing
library(lme4) # For mixed effects modelling
library(scales) # For working with colours
library(sjPlot) # For nice tabular display of regression models
library(tidyverse) # For data wrangling and plotting
library(visreg) # For nice visualisations of model results
select <- dplyr::select
filter <- dplyr::filter

source(here("R_rainclouds.R")) # For geom_flat_violin rainplots

```

```{r palette}

# Colours used in Register Studies paper and included in Open Access plots published on Zenodo:
#colours <- suf_palette(name = "london", n = 6, type = "continuous") # Very nice, similar to OrRd palette
#scales::show_col(colours)
#colours <- colours[6:1] 

# Colour scheme used in PhD thesis:
colours = c("#F9B921", "#A18A33",  "#722672", "#BD241E", "#15274D", "#D54E1E") 
scales::show_col(colours)

```

# Data import from MFTE outputs

## Spoken BNC2014

The original corpus files of the Spoken BNC2014 can be downloaded for free for research purposes from: http://corpora.lancs.ac.uk/bnc2014/signup.php These counts were computed on the basis of the "John and Jill in Ivybridge" version of the Spoken BNC2014 with added full stops at speaker turns. The R script used to pre-process the untagged XML version of the Spoken BNC2014 into the format used in this study can be found at: https://github.com/elenlefoll/TextbookEnglish/blob/main/3_Data/BNCspoken_nomark-up_JackJill.R 

```{r SpokenBNC2014}

SpokenBNC2014 <- read.delim(here("MFTE_data", "Outputs", "SpokenBNC2014_3.1_normed_complex_counts.tsv"), header = TRUE, stringsAsFactors = TRUE)

SpokenBNC2014$Series <- "Spoken BNC2014"
SpokenBNC2014$Level <- "Ref."
SpokenBNC2014$Country <- "Spoken BNC2014"
SpokenBNC2014$Register <- "Spoken BNC2014"

```

### Youth Fiction corpus

These counts were computed on the basis of the random samples of approximately 5,000 words of the books of the Youth Fiction corpus (for details of the works included in this corpus, see https://github.com/elenlefoll/TextbookEnglish/blob/main/3_Data/3_Youth_Fiction_Index.csv).

```{r YouthFiction}

YouthFiction <- read.delim(here("MFTE_data", "Outputs", "YF_sampled_500_3.1_normed_complex_counts.tsv"), header = TRUE, stringsAsFactors = TRUE)

YouthFiction$Series <- "Youth Fiction"
YouthFiction$Level <- "Ref."
YouthFiction$Country <- "Youth Fiction"
YouthFiction$Register <- "Youth Fiction"

```

### Informative Texts for Teens (InfoTeens) corpus

```{r InfoTeencounts}

InfoTeen <- read.delim(here("MFTE_data", "Outputs", "InfoTeen_3.1_normed_complex_counts.tsv"), header = TRUE, stringsAsFactors = TRUE)
InfoTeen <- InfoTeen %>% filter(Filename!=".DS_Store" & Filename!="Revision_World_GCSE_10529068_wjec-level-law-past-papers.txt" & Filename!="Revision_World_GCSE_10528474_wjec-level-history-past-papers.txt" & Filename!="Revision_World_GCSE_10528472_edexcel-level-history-past-papers.txt")
# Removes three outlier files which should not have been included in the corpus as they contain exam papers only

InfoTeen$Series <- "Info Teens"
InfoTeen$Level <- "Ref."
InfoTeen$Country <- "Info Teens"
InfoTeen$Register <- "Info Teens"

```


## Merging TEC and reference corpora data

```{r DataMerging.only}

TxBncounts <- readRDS(here("processed_data", "TxBcounts.rds"))
All3Reg <- c("Conversation", "Fiction", "Informative")

TxBncounts3Reg <- TxBncounts %>% 
  filter(Register %in% All3Reg) %>% 
  droplevels(.)

ncounts <- bind_rows(TxBncounts3Reg, InfoTeen, SpokenBNC2014, YouthFiction, .id = "Corpus")

# Convert all character vectors to factors
ncounts[sapply(ncounts, is.character)] <- lapply(ncounts[sapply(ncounts, is.character)], as.factor)

# Change all NAs to 0
ncounts[is.na(ncounts)] <- 0

levels(ncounts$Corpus) <- list(Textbook.English="1", Informative.Teens="2", Spoken.BNC2014="3", Youth.Fiction="4")
summary(ncounts$Corpus)
summary(ncounts$Series)

# Wrangle metadata variables
ncounts$Subcorpus <- ncounts$Register
levels(ncounts$Subcorpus)
levels(ncounts$Subcorpus) <- c("Textbook Conversation", "Textbook Fiction", "Info Teens Ref.", "Textbook Informative", "Spoken BNC2014 Ref.", "Youth Fiction Ref.")
summary(ncounts$Subcorpus)

# Re-order registers
levels(ncounts$Register)
levels(ncounts$Register) <- c("Conversation", "Fiction", "Informative", "Informative", "Conversation", "Fiction")
summary(ncounts$Register)

# Re-order variables
colnames(ncounts)
ncounts <- ncounts %>% 
  select(order(names(.))) %>% # Order alphabetically first
  select(Filename, Register, Level, Series, Country, Corpus, Subcorpus, Words, everything()) # Then place the metadata variable at the front of the table

#saveRDS(ncounts, here("processed_data", "counts3Reg.rds")) # Last saved 26 Feb 2024

```

# Data preparation for PCA

## Feature distributions
```{r distribution-viz, fig.height=40, warning=FALSE}

#ncounts <- readRDS(here("processed_data", "counts3Reg.rds"))

# Plot inspired by: https://drsimonj.svbtle.com/quick-plot-of-all-variables

ncounts %>%
  select(-Words) %>% 
  keep(is.numeric) %>% 
  gather() %>% # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2
  ggplot(aes(value, after_stat(density))) +
    theme_bw() +
    facet_wrap(~ key, scales = "free", ncol = 4) +
    scale_x_continuous(expand=c(0,0)) +
    scale_y_continuous(limits = c(0,NA)) +
    geom_histogram(bins = 30, colour= "black", fill = "grey") +
    geom_density(colour = "darkred", weight = 2, fill="darkred", alpha = .4)

#ggsave(here("plots", "DensityPlotsAllVariables.svg"), width = 15, height = 49)

ncounts %>%
  select(-Words) %>% 
  keep(is.numeric) %>% 
  gather() %>% # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2
  ggplot(aes(value)) +
    theme_bw() +
    facet_wrap(~ key, scales = "free", ncol = 4) +
    scale_x_continuous(expand=c(0,0)) +
    geom_histogram(bins = 30, colour= "darkred", fill = "darkred", alpha = 0.5)

#ggsave(here("plots", "HistogramPlotsAllVariables.svg"), width = 20, height = 45)

```

## Feature removal due to low text frequency
```{r feature-removal-low-frequency}

# Removal of meaningless feature: CD because numbers as digits were mostly removed from the textbooks, LIKE and SO because they are dustbin categories
ncounts <- ncounts %>% 
  select(-c(CD, LIKE, SO))

# Function to compute percentage of texts with occurrences meeting a condition
compute_percentage <- function(data, condition, threshold) {
  numeric_data <- Filter(is.numeric, data)
  percentage <- round(colSums(condition[, sapply(numeric_data, is.numeric)])/nrow(data) * 100, 2)
  percentage <- as.data.frame(percentage)
  colnames(percentage) <- "Percentage"
  percentage <- percentage %>% 
    filter(!is.na(Percentage)) %>%
    rownames_to_column() %>%
    arrange(Percentage)
  if (!missing(threshold)) {
    percentage <- percentage %>% 
      filter(Percentage > threshold)
  }
  return(percentage)
}

# Calculate percentage of texts with 0 occurrences of each feature
zero_features <- compute_percentage(ncounts, ncounts == 0, 66.6)
print(zero_features)

# Combine low frequency features into meaningful groups whenever this makes linguistic sense
ncounts <- ncounts %>% 
  mutate(JJPR = JJPR + ABLE, ABLE = NULL) %>% 
  mutate(PASS = PGET + PASS, PGET = NULL) %>% 
  mutate(TPP3 = TPP3S + TPP3P, TPP3P = NULL, TPP3S = NULL) %>% # Merged due to TTP3P having an individual MSA < 0.5
  mutate(FQTI = FREQ + TIME, FREQ = NULL, TIME = NULL) # Merged due to TIME communality < 0.2 (see below) 

# Drop variables with low document frequency
ncounts2 <- select(ncounts, -one_of(zero_features$rowname))
ncol(ncounts2)-8 # Number of linguistic features remaining

# With five TEC registers
#saveRDS(ncounts2, here("processed_data", "ncounts2.rds")) # Last saved 26 Feb. 2024

```

### Standardising normalised counts and identifying potential outliers

"As an alternative to removing very sparse feature, we apply a signed logarithmic transformation to deskew the feature distributions." (Neumann & Evert)

```{r standardisation-outliers}

# First scale the normalised counts (z-standardisation) to be able to compare the various features
ncounts2 %>%
  select(-Words) %>% 
  keep(is.numeric) %>% 
  scale() ->
  zcounts

boxplot(zcounts, las = 3, main = "z-scores") # Slow

# If necessary, remove any outliers at this stage.
data <- cbind(ncounts2[,1:8], as.data.frame(zcounts))
outliers <- data %>% 
 filter(if_any(where(is.numeric) & !Words,  .fns = function(x){x > 8}))  %>% 
  select(Filename, Corpus, Series, Register, Level, Words)

# These are potential outlier texts according to the above conditions:
outliers

# Checking that outlier texts are not particularly long or short texts
summary(outliers$Words)
hist(outliers$Words, breaks = 30)
# Distribution of outlier texts across the four corpora
summary(outliers$Corpus)

# Report on the manual check of a sample of these outliers:

# Encyclopedia_Kinds_au_10085347_Nobel_Prize_in_Chemistry.txt is essentially a list of Nobel prize winners but with some additional information. Hence a good representative of the type of texts of the ITTC.
# Solutions_Elementary_ELF_Spoken_0013 --> Has a lot of "going to" constructions because they are learnt in this chapter but is otherwise a well-formed text.
# Teen_Kids_News_10403972_a-brief-history-of-white-house-weddings --> No issues
# Teen_Kids_News_10403301_golden-globe-winners-2019-the-complete-list --> Similar to the Nobel prize laureates text.
# Revision_World_GCSE_10528123_gender-written-textual-analysis-framework --> Text includes bullet points tokenised as the letter "o" but otherwise a fairly typical informative text.

# Removing the outliers
ncounts3 <- ncounts2 %>% 
  filter(!Filename %in% outliers$Filename)

nrow(ncounts3)

#saveRDS(ncounts3, here("processed_data", "ncounts3_3Reg.rds")) # Last saved 26 Feb 2022

ncounts3 %>%
  select(-Words) %>% 
  keep(is.numeric) %>% 
  scale() ->
  zcounts3

nrow(zcounts3)
boxplot(zcounts3, las = 3, main = "z-scores") # Slow to open!

```

### Transforming the features to (partially) deskew these distributions

```{r signed.log.transformation}

signed.log <- function(x) {sign(x)*log(abs(x)+1)}

zlogcounts <- signed.log(zcounts3) # Standardise first, then signed log transform

boxplot(zlogcounts, las=3, main="log-transformed z-scores")

#saveRDS(zlogcounts, here("processed_data", "zlogcounts_3Reg.rds")) # Last saved 26 Feb 2022

zlogcounts %>%
  as.data.frame() %>% 
  gather() %>% # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2
  ggplot(aes(value, after_stat(density))) +
  theme_bw() +
  facet_wrap(~ key, scales = "free", ncol = 4) +
  scale_x_continuous(expand=c(0,0)) +
  scale_y_continuous(limits = c(0,NA)) +
  geom_histogram(bins = 30, colour= "black", fill = "grey") +
  geom_density(colour = "darkred", weight = 2, fill="darkred", alpha = .4)

#ggsave(here("plots", "DensityPlotsAllVariablesSignedLog.svg"), width = 15, height = 49)

```


### Merging of data for MDA

```{r import-final-ncounts-data}

# With three TEC registers
zlogcounts <- readRDS(here("processed_data", "zlogcounts_3Reg.rds")) 
#nrow(zlogcounts)
#colnames(zlogcounts)

ncounts3 <- readRDS(here("processed_data", "ncounts3_3Reg.rds"))
#nrow(ncounts3)
#colnames(ncounts3)

data <- cbind(ncounts3[,1:8], as.data.frame(zlogcounts))
colnames(data)

#saveRDS(data, here("processed_data", "datazlogcounts_3Reg.rds")) # Last saved 26 Feb 2022

```

## Performing the PCA of Textbook English vs. Reference corpora
### Quick import
```{r prepare-data}

data <- readRDS(here("processed_data", "datazlogcounts_3Reg.rds"))
summary(data$Subcorpus)

# This rearranges the levels in the desired order for the plot legends:
data <- data %>% 
  mutate(Subcorpus = fct_relevel(Subcorpus, "Info Teens Ref.", after = 9))

```

## Testing factorability of data

### Correlation matrix
```{r corrsimple-function, eval=TRUE}

# From: https://towardsdatascience.com/how-to-create-a-correlation-matrix-with-too-many-variables-309cc0c0a57
## Function adapated to my needs ##

corr <- cor(data[9:ncol(data)])
#prepare to drop duplicates and correlations of 1     
corr[lower.tri(corr,diag=TRUE)] <- NA 
#drop perfect correlations
corr[corr == 1] <- NA   
#turn into a 3-column table
corr <- as.data.frame(as.table(corr))
#remove the NA values from above 
corr <- na.omit(corr)   

#Uninteresting variable correlations?
lowcor <- subset(corr, abs(Freq) < 0.3)
#lowcor %>% filter(Var2=="CC"|Var1=="CC") %>% round(Freq, 2)
#select significant correlations 
corr <- subset(corr, abs(Freq) > 0.3)
#sort by highest correlation
corr <- corr[order(-abs(corr$Freq)),]   
#see which variables might be eliminated: the ones with correlation > 0.3
eliminate <- as.data.frame((summary(corr$Var1) + summary(corr$Var2)))
(LowcCommunality <- eliminate %>% filter(`(summary(corr$Var1) + summary(corr$Var2))`==0))

# Potentially problematic collinear variables that may need to be removed:
highcor <- subset(corr, abs(Freq) > 0.95)
highcor

#variables which are retained
corr$Var1 <- droplevels(corr$Var1)
corr$Var2 <- droplevels(corr$Var2)
features <- unique(c(levels(corr$Var1), levels(corr$Var2))) 
features # 68 variables
#turn corr back into matrix in order to plot with corrplot
mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
#plot correlations in a manageable way
library(corrplot)
plot.margin=unit(c(0,0,0,0), "mm")
corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ", tl.cex = 0.5)
#save as SVG with Rstudio e.g. 1000 x 1000
```

### Visualisation of feature correlations
```{r heatmap}

# Simple heatmap in base R (inspired by Stephanie Evert's SIGIL code)
cor.colours <- c(
  hsv(h=2/3, v=1, s=(10:1)/10), # blue = negative correlation 
  rgb(1,1,1), # white = no correlation 
  hsv(h=0, v=1, s=(1:10/10))) # red = positive correlation

png(here("plots", "heatmapzlogcounts.png"), width = 30, height= 30, units = "cm", res = 300)
heatmap(cor(zlogcounts), 
        symm=TRUE, 
        zlim=c(-1,1), 
        col=cor.colours, 
        margins=c(7,7))

#dev.off()
```

### MSA, communalities and scree plot
```{r factorability-screeplot}

# Eliminate highly collinear variable
cor(data$VPRT, data$VBD)

data <- data %>% 
  select(-c(VPRT))

colnames(data)
kmo <- KMO(data[,9:ncol(data)])
kmo # # Overall MSA = 0.95 
kmo$MSAi[order(kmo$MSAi)] # All features have individual MSAs of > 0.5 (but only because TPP3P was merged into a larger category earlier on)

#png(here("plots", "screeplot-TEC-Ref_3Reg.png"), width = 20, height= 12, units = "cm", res = 300)
scree(data[,9:ncol(data)], factors = FALSE, pc = TRUE) # 6 components
#dev.off()

# Attempting to take a closer look at the break point in the screeplot by visualising the eigenvalues differently
screeplot <- scree(data[,9:ncol(data)], factors = FALSE, pc = TRUE) 
plot(screeplot$pcv[2:10], type = "b") # 6 components

# Perform PCA
pca1 <- psych::principal(data[9:ncol(data)], 
                         nfactors = 6)

pca1$communality %>% sort(.) # If features with communalities of < 0.2 are removed, we remove TIME (therefore merged TIME and FREQ further up the line)

# Final number of features
ncol(data)-6
# Final number of texts
nrow(data)

#saveRDS(data, here("processed_data", "dataforPCA.rds")) # Last saved on 26 Feb 2022

```

```{r sessionInfo}
sessionInfo()
```
