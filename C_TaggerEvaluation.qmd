# Evaluation of the Multi-Feature Tagger of English (MFTE)

For more information on the evaluation data and methods, see Le Foll [-@lefoll2021] and <https://github.com/elenlefoll/MultiFeatureTaggerEnglish>.

## Packages required

The following packages must be installed and loaded to process the evaluation data.

```{r setup}

#renv::restore() # Restore the project's dependencies from the lockfile to ensure that same package versions are used as in the original thesis.

library(caret) # For computing confusion matrices
library(harrypotter) # Only for colour scheme
library(here) # For path management
library(knitr) # Loaded to display the tables using the kable() function
library(paletteer) # For nice colours
library(readxl) # For the direct import of Excel files
library(tidyverse) # For everything else!

```

## Data import from evaluation files

The data is imported directly from the Excel files in which the manual tag check and corrections was performed. A number of data wrangling steps need to be made for the data to be converted to a tidy format.

```{r import-wrangle-functions}
#| code-fold: true

# Function to import and wrangle the evaluation data from the Excel files in which the manual evaluation was conducted
importEval3 <- function(file, fileID, register, corpus) {
  Tag1 <- file |> 
  add_column(FileID = fileID, Register = register, Corpus = corpus) |>
  select(FileID, Corpus, Register, Output, Tokens, Tag1, Tag1Gold) |> 
  rename(Tag = Tag1, TagGold = Tag1Gold, Token = Tokens) |> 
  mutate(Evaluation = ifelse(is.na(TagGold), TRUE, FALSE)) |> 
  mutate(TagGold = ifelse(is.na(TagGold), as.character(Tag), as.character(TagGold))) |>
  filter(!is.na(Tag)) |> 
  mutate_if(is.character, as.factor)
  
  Tag2 <- file |> 
  add_column(FileID = fileID, Register = register, Corpus = corpus) |>
  select(FileID, Corpus, Register, Output, Tokens, Tag2, Tag2Gold) |> 
  rename(Tag = Tag2, TagGold = Tag2Gold, Token = Tokens) |> 
  mutate(Evaluation = ifelse(is.na(TagGold), TRUE, FALSE)) |> 
  mutate(TagGold = ifelse(is.na(TagGold), as.character(Tag), as.character(TagGold))) |>
  filter(!is.na(Tag)) |> 
  mutate_if(is.character, as.factor)

Tag3 <- file |> 
  add_column(FileID = fileID, Register = register, Corpus = corpus) |>
  select(FileID, Corpus, Register, Output, Tokens, Tag3, Tag3Gold) |> 
  rename(Tag = Tag3, TagGold = Tag3Gold, Token = Tokens) |> 
  mutate(Evaluation = ifelse(is.na(TagGold), TRUE, FALSE)) |> 
  mutate(TagGold = ifelse(is.na(TagGold), as.character(Tag), as.character(TagGold))) |>
  filter(!is.na(Tag)) |> 
  mutate_if(is.character, as.factor)

output <- rbind(Tag1, Tag2, Tag3) |> 
  mutate(across(where(is.factor), str_remove_all, pattern = fixed(" "))) |> # Removes all white spaces which are found in the excel files
  filter(!is.na(Output)) |> 
  mutate_if(is.character, as.factor)
}

# Second function to import and wrangle the evaluation data for Excel files with four tag columns as opposed to three
importEval4 <- function(file, fileID, register, corpus) {
  Tag1 <- file |> 
  add_column(FileID = fileID, Register = register, Corpus = corpus) |>
  select(FileID, Corpus, Register, Output, Tokens, Tag1, Tag1Gold) |> 
  rename(Tag = Tag1, TagGold = Tag1Gold, Token = Tokens) |> 
  mutate(Evaluation = ifelse(is.na(TagGold), TRUE, FALSE)) |> 
  mutate(TagGold = ifelse(is.na(TagGold), as.character(Tag), as.character(TagGold))) |>
  filter(!is.na(Tag)) |> 
  mutate_if(is.character, as.factor)
  
  Tag2 <- file |> 
  add_column(FileID = fileID, Register = register, Corpus = corpus) |>
  select(FileID, Corpus, Register, Output, Tokens, Tag2, Tag2Gold) |> 
  rename(Tag = Tag2, TagGold = Tag2Gold, Token = Tokens) |> 
  mutate(Evaluation = ifelse(is.na(TagGold), TRUE, FALSE)) |> 
  mutate(TagGold = ifelse(is.na(TagGold), as.character(Tag), as.character(TagGold))) |>
  filter(!is.na(Tag)) |> 
  mutate_if(is.character, as.factor)

Tag3 <- file |> 
  add_column(FileID = fileID, Register = register, Corpus = corpus) |>
  select(FileID, Corpus, Register, Output, Tokens, Tag3, Tag3Gold) |> 
  rename(Tag = Tag3, TagGold = Tag3Gold, Token = Tokens) |> 
  mutate(Evaluation = ifelse(is.na(TagGold), TRUE, FALSE)) |> 
  mutate(TagGold = ifelse(is.na(TagGold), as.character(Tag), as.character(TagGold))) |>
  filter(!is.na(Tag)) |> 
  mutate_if(is.character, as.factor)

Tag4 <- file |> 
  add_column(FileID = fileID, Register = register, Corpus = corpus) |>
  select(FileID, Corpus, Register, Output, Tokens, Tag4, Tag4Gold) |> 
  rename(Tag = Tag4, TagGold = Tag4Gold, Token = Tokens) |> 
  mutate(Evaluation = ifelse(is.na(TagGold), TRUE, FALSE)) |> 
  mutate(TagGold = ifelse(is.na(TagGold), as.character(Tag), as.character(TagGold))) |>
  filter(!is.na(Tag)) |> 
  mutate_if(is.character, as.factor)

output <- rbind(Tag1, Tag2, Tag3, Tag4) |> 
  mutate(across(where(is.factor), str_remove_all, pattern = fixed(" "))) |> # Removes all white spaces which are found in the excel files
  filter(!is.na(Tag)) |> 
  mutate_if(is.character, as.factor)

}

# Function to decide which of the two above functions should be used
importEval <- function(file, fileID, register, corpus) { 
  if(sum(!is.na(file$Tag4)) > 0) {
    output = importEval4(file = file, fileID = fileID, register = register, corpus = corpus)
  }
  else{
    output = importEval3(file = file, fileID = fileID, register = register, corpus = corpus)
  }
}

Solutions_Intermediate_Spoken_0032 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "Solutions_Intermediate_Spoken_0032_Evaluation.xlsx")), fileID = "Solutions_Intermediate_Spoken_0032", register = "Conversation", corpus = "TEC-Sp")

HT_5_Poetry_0001 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "HT_5_Poetry_0001_Evaluation.xlsx")), fileID = "HT_5_Poetry_0001", register = "Poetry", corpus = "TEC-Fr")

Achievers_A1_Informative_0006 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "Achievers_A1_Informative_0006_Evaluation.xlsx")), fileID = "Achievers_A1_Informative_0006", register = "Informative", corpus = "TEC-Sp")

New_GreenLine_5_Personal_0003 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "New_GreenLine_5_Personal_0003_Evaluation.xlsx")), fileID = "New_GreenLine_5_Personal_0003", register = "Personal communication", corpus = "TEC-Ger")

Piece_of_cake_3e_Instructional_0006 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "Piece_of_cake_3e_Instructional_0006_Evaluation.xlsx")), fileID = "Piece_of_cake_3e_Instructional_0006", register = "Instructional", corpus = "TEC-Fr")

Access_4_Narrative_0006 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "Access_4_Narrative_0006_Evaluation.xlsx")), fileID = "Access_4_Narrative_0006", register = "Fiction", corpus = "TEC-Ger")

BNCBFict_b2 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "BNCBFict_b2.xlsx")), fileID = "BNCBFict_b2", register = "fiction", corpus = "BNC2014")

BNCBFict_m54 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "BNCBFict_m54.xlsx")), fileID = "BNCBFict_m54", register = "fiction", corpus = "BNC2014")

BNCBFict_e27 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "BNCBFict_e27.xlsx")), fileID = "BNCBFict_e27", register = "fiction", corpus = "BNC2014")

BNCBMass16 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "BNCBMass16.xlsx")), fileID = "BNCBMass16", register = "news", corpus = "BNC2014")

BNCBMass23 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "BNCBMass23.xlsx")), fileID = "BNCBMass23", register = "news", corpus = "BNC2014")

BNCBReg111 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "BNCBReg111.xlsx")), fileID = "BNCBReg111", register = "news", corpus = "BNC2014")

BNCBReg750 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "BNCBReg750.xlsx")), fileID = "BNCBReg750", register = "news", corpus = "BNC2014")

BNCBSer486 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "BNCBSer486.xlsx")), fileID = "BNCBSer486", register = "news", corpus = "BNC2014")

BNCBSer562 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "BNCBSer562.xlsx")), fileID = "BNCBSer562", register = "news", corpus = "BNC2014")

BNCBEBl8 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "BNCBEBl8.xlsx")), fileID = "BNCBEBl8", register = "internet", corpus = "BNC2014")

BNCBEFor32 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "BNCBEFor32.xlsx")), fileID = "BNCBEFor32", register = "internet", corpus = "BNC2014")

S2DD <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "S2DD.xlsx")), fileID = "S2DD", register = "spoken", corpus = "BNC2014")

S3AV <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "S3AV.xlsx")), fileID = "S3AV", register = "spoken", corpus = "BNC2014")

SEL5 <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "SEL5.xlsx")), fileID = "SEL5", register = "spoken", corpus = "BNC2014")

SVLK <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "SVLK.xlsx")), fileID = "SVLK", register = "spoken", corpus = "BNC2014")

SZXQ <- importEval(file = read_excel(here("data", "MFTE", "evaluation", "SZXQ.xlsx")), fileID = "SZXQ", register = "spoken", corpus = "BNC2014")

TaggerEval <- rbind(Solutions_Intermediate_Spoken_0032, HT_5_Poetry_0001, Achievers_A1_Informative_0006, New_GreenLine_5_Personal_0003, Piece_of_cake_3e_Instructional_0006, Access_4_Narrative_0006, BNCBEBl8, BNCBFict_b2, BNCBFict_m54, BNCBFict_e27, BNCBEFor32, BNCBMass16, BNCBMass23, BNCBReg111, BNCBReg750, BNCBSer486, BNCBSer562, S2DD, S3AV, SEL5, SVLK, SZXQ)
```

Some tags had to be merged to account for changes made to the MFTE between the evaluation and the tagging of the corpora included in the present study.

```{r feature merging}
#| code-fold: true

TaggerEval <- TaggerEval |> 
  mutate(Tag = ifelse(Tag == "PHC", "CC", as.character(Tag))) |> 
  mutate(TagGold = ifelse(TagGold == "PHC", "CC", as.character(TagGold))) |> 
  mutate(Tag = ifelse(Tag == "QLIKE", "LIKE", as.character(Tag))) |> 
  mutate(TagGold = ifelse(TagGold == "QLIKE", "LIKE", as.character(TagGold))) |> 
  mutate(Tag = ifelse(Tag == "TO", "IN", as.character(Tag))) |> 
  mutate(TagGold = ifelse(TagGold == "TO", "IN", as.character(TagGold))) |> 
  mutate_if(is.character, as.factor) |> 
  mutate(Evaluation = ifelse(as.character(Tag) == as.character(TagGold), TRUE, FALSE))

# head(TaggerEval) # Check sanity of data
# summary(TaggerEval) # Check sanity of data

# saveRDS(TaggerEval, here("data", "processed", "MFTE_Evaluation_Results.rds"))

# write.csv(TaggerEval, here("data", "processed", "MFTE_Evaluation_Results.csv"))

```

```{r quick-import}
#| include: false

#TaggerEval <- readRDS(here("data", "MFTE", "evaluation", "MFTE_PhD_Evaluation_Results.rds")) 

```

This table provides a summary of the complete evaluation dataset. It comprises `{r} nrow(TaggerEval) |> format(big.mark=",")` tags that were checked (and, if needs be, corrected) by at least one human annotator. This number includes tags for punctuation marks, which make up a considerable proportion of the tags.

```{r summary}
#| echo: false

summary(TaggerEval)

nTEC <- TaggerEval |> 
  filter(Corpus %in% c("TEC-Fr", "TEC-Ger", "TEC-Sp")) |> 
  nrow() 

```

## Estimating MFTE accuracy for Textbook English

In total, `{r} nTEC |> format(big.mark=",")` tags from the TEC were manually checked. This chunk calculates the recall and precision rates of each feature, ignoring all punctuation and symbols.

```{r tagger-accuracy-TEC}
#| code-fold: true

data <- TaggerEval |> 
  filter(Corpus %in% c("TEC-Fr", "TEC-Ger", "TEC-Sp")) |> 
  filter(TagGold != "UNCLEAR") |> 
  filter(Tag %in% c(str_extract(Tag, "[A-Z0-9]+"))) |> # Remove punctuation tags which are uninteresting here.
  filter(Tag != "SYM" & Tag != "``") |> 
  droplevels() |> 
  mutate(Tag = factor(Tag, levels = union(levels(Tag), levels(TagGold)))) |> # Ensure that the factor levels are the same for the next caret operation
  mutate(TagGold = factor(TagGold, levels = union(levels(Tag), levels(TagGold))))

# Spot gold tag corrections that are not actually errors (should return zero rows if all is well)
# data[data$Tag==data$TagGold & data$Evaluation == FALSE,] |> as.data.frame()
```

The breakdown of inaccurate vs. accurate tags in this TEC evaluation sample is:

```{r}
#| echo: false

summary(data$Evaluation)

cm <- caret::confusionMatrix(data$Tag, data$TagGold) # Create confusion matrix
```

Note that the following accuracy metrics calculated using the `caret::confusionMatrix` are not very representative because they include tags, which were not entered in the study, e.g., LS and FW.

```{r}
#| echo: false

cm$overall |> round(2)
```

Accuracy metrics per feature are more interesting and relevant.

```{r}
#| echo: false

cm$byClass[,5:7] |> 
  kable(digits = 2)
```

## MFTE accuracy for reference corpora (or comparable corpora)

### Conversation

These are extracts from the Spoken BNC2014 (as entered in the study). The evaluation data for this sample excludes `{r} TaggerEval |> filter(Register == "spoken") |>  filter(TagGold == "UNCLEAR") |> nrow()` tokens deemed *unclear* by at least one human annotator.

```{r tagger-accuracy-SpokenBNC2014}
#| code-fold: true

data <- TaggerEval |> 
  filter(Register == "spoken") |> 
  filter(TagGold != "UNCLEAR") |> 
  filter(Tag %in% c(str_extract(Tag, "[A-Z0-9]+"))) |> # Remove all punctuation tags which are uninteresting here.
  droplevels() |> 
  mutate(Tag = factor(Tag, levels = union(levels(Tag), levels(TagGold)))) |> # Ensure that the factor levels are the same for the next caret operation
  mutate(TagGold = factor(TagGold, levels = union(levels(Tag), levels(TagGold))))

# Spot gold tag corrections that are not actually errors (should return zero rows if all is well)
# data[data$Tag==data$TagGold & data$Evaluation == FALSE,] |> as.data.frame()

```

The breakdown of inaccurate vs. accurate tags in this evaluation sample is:

```{r}
#| echo: false

summary(data$Evaluation)

cm <- caret::confusionMatrix(data$Tag, data$TagGold) # Create confusion matrix
```

Note that the following accuracy metrics calculated using the `caret::confusionMatrix` are not very representative because they include tags, which were not entered in the study, e.g., LS and FW.

```{r}
#| echo: false

cm$overall |> round(2)

# cm$byClass[,5:7] |> 
  # kable(digits = 2)
```

### Fiction

The evaluation data for this sample excludes `{r} TaggerEval |> filter(Register == "fiction") |>  filter(TagGold == "UNCLEAR") |> nrow()` tokens deemed *unclear* by at least one human annotator.

```{r tagger-accuracy-Fiction}

data <- TaggerEval |> 
  filter(Register == "fiction") |> 
  filter(TagGold != "UNCLEAR") |> 
  filter(Tag %in% c(str_extract(Tag, "[A-Z0-9]+"))) |> # Remove all punctuation tags which are uninteresting here.
  filter(Tag != "SYM" & Tag != "``") |> 
  droplevels() |> 
  mutate(Tag = factor(Tag, levels = union(levels(Tag), levels(TagGold)))) |> # Ensure that the factor levels are the same for the next caret operation
  mutate(TagGold = factor(TagGold, levels = union(levels(Tag), levels(TagGold))))

# Spot gold tag corrections that are not actually errors (should return zero rows if all is well)
# data[data$Tag==data$TagGold & data$Evaluation == FALSE,] |> as.data.frame()
```

The breakdown of inaccurate vs. accurate tags in this evaluation sample is:

```{r}
#| echo: false

summary(data$Evaluation)

cm <- caret::confusionMatrix(data$Tag, data$TagGold) # Create confusion matrix
```

Note that the following accuracy metrics calculated using the `caret::confusionMatrix` are not very representative because they include tags, which were not entered in the study, e.g., LS and FW.

```{r}
#| echo: false

cm$overall |> round(2)

# cm$byClass[,5:7] |> 
  # kable(digits = 2)
```

### Informative

The evaluation data for this sample excludes `{r} TaggerEval |> filter(Register == "news" | FileID %in% c("BNCBEFor32", "BNCBEBl8")) |>  filter(TagGold == "UNCLEAR") |> nrow()` tokens deemed *unclear* by at least one human annotator.

```{r tagger-accuracy-Informative}

data <- TaggerEval |> 
  filter(Register == "news" | FileID %in% c("BNCBEFor32", "BNCBEBl8")) |> 
  filter(TagGold != "UNCLEAR") |> 
  filter(Tag %in% c(str_extract(Tag, "[A-Z0-9]+"))) |> # Remove all punctuation tags which are uninteresting here.
  filter(Tag != "SYM" & Tag != "``") |> 
  droplevels() |> 
  mutate(Tag = factor(Tag, levels = union(levels(Tag), levels(TagGold)))) |> # Ensure that the factor levels are the same for the next caret operation
  mutate(TagGold = factor(TagGold, levels = union(levels(Tag), levels(TagGold))))

# Spot gold tag corrections that are not actually errors (should return zero rows if all is well)
# data[data$Tag==data$TagGold & data$Evaluation == FALSE,] |> as.data.frame()

```

The breakdown of inaccurate vs. accurate tags in this evaluation sample is:

```{r}
#| echo: false

summary(data$Evaluation)

cm <- caret::confusionMatrix(data$Tag, data$TagGold) # Create confusion matrix
```

Note that the following accuracy metrics calculated using the `caret::confusionMatrix` are not very representative because they include tags, which were not entered in the study, e.g., LS and FW.

```{r}
#| echo: false

cm$overall |> round(2)

# cm$byClass[,5:7] |> 
  # kable(digits = 2)

```

## Estimating the overall MFTE accuracy for corpora used in the study

```{r overall-accuracy}
#| code-fold: true

data <- TaggerEval |> 
  filter(TagGold != "UNCLEAR") |> 
  filter(Tag %in% c(str_extract(Tag, "[A-Z0-9]+"))) |> # Remove all punctuation tags which are uninteresting here.
  filter(Tag != "SYM" & Tag != "``") |> 
  filter(TagGold != "SYM" & TagGold != "``") |> 
  droplevels() |> 
  mutate(Tag = factor(Tag, levels = union(levels(Tag), levels(TagGold)))) |> # Ensure that the factor levels are the same for the next caret operation
  mutate(TagGold = factor(TagGold, levels = union(levels(Tag), levels(TagGold))))

# Generate a better formatted results table for export: recall, precision and f1
confusion_matrix <- cm$table
total <- sum(confusion_matrix)
number_of_classes <- nrow(confusion_matrix)
correct <- diag(confusion_matrix)
# sum all columns
total_actual_class <- apply(confusion_matrix, 2, sum)
# sum all rows
total_pred_class <- apply(confusion_matrix, 1, sum)
# Precision = TP / all that were predicted as positive
precision <- correct / total_pred_class
# Recall = TP / all that were actually positive
recall <- correct / total_actual_class
# F1
f1 <- (2 * precision * recall) / (precision + recall)
# create data frame to output results
results <- data.frame(precision, recall, f1, total_actual_class)

results |> 
  kable(digits = 2)

```

```{r}
#| code-fold: true
#| fig-height: 13

resultslong <- results |> 
  drop_na() %>%
  mutate(tag = row.names(.)) |> 
  filter(tag != "NULL" & tag != "SYM" & tag != "OCR" & tag != "FW" & tag != "USEDTO") |> 
  rename(n = total_actual_class) |> 
  pivot_longer(cols = c("precision", "recall", "f1"), names_to = "metric", values_to = "value") |> 
  mutate(metric = factor(metric, levels = c("precision", "recall", "f1")))

# summary(resultslong$n)

ggplot(resultslong, aes(y = reorder(tag, desc(tag)), x = value, group = metric, colour = n)) +
  geom_point(size = 2) +
  ylab("") +
  xlab("") +
  facet_wrap(~ metric) +
  scale_color_paletteer_c("harrypotter::harrypotter", trans = "log", breaks = c(1,10, 100, 1000), labels = c(1,10, 100, 1000), name = "# tokens \nmanually\nevaluated") +
  theme_bw() +
  theme(panel.grid.major.y = element_line(colour = "darkgrey")) +
  theme(legend.position = "right")

#ggsave(here("plots", "TaggerAccuracyPlot.svg"), width = 7, height = 12)

```

## Exploring tagger errors

To inspect regular/systematic tagger errors, we add an error tag with the incorrectly assigned tag and underscore and then the correct "gold" label.

```{r errors}
#| code-fold: true

errors <- TaggerEval |> 
  filter(Evaluation=="FALSE") |> 
  filter(TagGold != "UNCLEAR") |> 
  mutate(Error = paste(Tag, TagGold, sep = " -> "))

FreqErrors <- errors |> 
  #filter(Corpus %in% c("TEC-Fr", "TEC-Ger", "TEC-Sp")) |> 
  count(Error) |> 
  arrange(desc(n))

# Number of error types that only occur once
once <- FreqErrors |> 
  filter(n == 1) |> 
  nrow()

```

The total number of errors is `{r} nrow(errors)`. Of those, `{r} once` occur just once. In total, there are `{r} nrow(FreqErrors)` different types of errors. The most frequent 10 are:

```{r}
#| code-fold: true

FreqErrors |> 
  filter(n > 10) |> 
  kable(digits = 2)
```

The code in the following chunk can be used to take a closer look at specific types of frequent errors.

```{r}

errors |> 
  filter(Error == "NN -> JJAT") |> 
  select(-Output, -Corpus, -Tag, -TagGold) |> 
  filter(grepl(x = Token, pattern = "[A-Z]+.")) |> 
  kable(digits = 2)

errors |> 
  filter(Error %in% c("NN -> VB", "VB -> NN", "NN -> VPRT", "VPRT -> NN")) |> 
  count(Token) |> 
  arrange(desc(n)) |> 
  filter(n > 1) |> 
  kable(digits = 2) 

errors |> 
  filter(Error == "ACT -> NULL") |> 
  count(Token) |> 
  arrange(desc(n)) |> 
  kable(digits = 2) 

```

For more information on the MFTE evaluation, see [@lefoll2021] and <https://github.com/elenlefoll/MultiFeatureTaggerEnglish>.
