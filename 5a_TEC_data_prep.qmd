This document outlines the steps taken to pre-process the Textbook English Corpus (TEC) data.

```{r}
#| label: set-up

#renv::restore() # Restore the project's dependencies from the lockfile to ensure that same package versions are used as in the original thesis.

library(caret) # For its confusion matrix function
library(here) # For dynamic file paths
library(patchwork) # For Fig. 1
library(PerformanceAnalytics)
library(psych) # For various useful stats function
library(tidyverse)

```

# Data import from MFTE output

```{r}
#| label: TxBcounts

# Read in Textbook Corpus data
# This .tsv file corresponds to the "mixed normalised frequency" output of the MFTE Perl v. 3.1
TxBcounts <- read.delim(here("MFTE_data", "Outputs", "TxB900MDA_3.1_normed_complex_counts.tsv"), header = TRUE, stringsAsFactors = TRUE)
TxBcounts <- TxBcounts %>% filter(Filename!=".DS_Store") %>% droplevels(.)
str(TxBcounts) # Check sanity of data
nrow(TxBcounts) # Should be 2014 files

# Adding a textbook proficiency level
TxBLevels <- read.delim(here("metadata", "TxB900MDA_ProficiencyLevels.csv"), sep = ",")
TxBcounts <- full_join(TxBcounts, TxBLevels, by = "Filename") %>% 
  mutate(Level = as.factor(Level)) %>% 
  mutate(Filename = as.factor(Filename))
summary(TxBcounts$Level) # Check distribution and that there are no NAs
TxBcounts %>% select(Filename, Level) %>% sample_n(20) # Check matching on random sample

# Adding a register variable from the file names
TxBcounts$Register <- as.factor(stringr::str_extract(TxBcounts$Filename, "Spoken|Narrative|Other|Personal|Informative|Instructional|Poetry")) # Add a variable for Textbook Register
summary(TxBcounts$Register)
TxBcounts$Register <- car::recode(TxBcounts$Register, "'Narrative' = 'Fiction'; 'Spoken' = 'Conversation'")
colnames(TxBcounts) # Check all the variables make sense

# Adding a textbook series variable from the file names
TxBcounts$Filename <- stringr::str_replace(TxBcounts$Filename, "English_In_Mind|English_in_Mind", "EIM") 
TxBcounts$Filename <- stringr::str_replace(TxBcounts$Filename, "New_GreenLine", "NGL") # Otherwise the regex for GreenLine will override New_GreenLine
TxBcounts$Filename <- stringr::str_replace(TxBcounts$Filename, "Piece_of_cake", "POC") # Shorten label for ease of plotting
TxBcounts$Series <- as.factor(stringr::str_extract(TxBcounts$Filename, "Access|Achievers|EIM|GreenLine|HT|NB|NM|POC|JTT|NGL|Solutions"))
summary(TxBcounts$Series) # Extract textbook series from (ammended) filenames

# Including the French textbooks for the first year of Lycée to their corresponding publisher series from collège
TxBcounts$Series <-car::recode(TxBcounts$Series, "c('NB', 'JTT') = 'JTT'; c('NM', 'HT') = 'HT'") # # Recode final volumes of French series (see Section 4.3.1.1 on textbook selection for details)
summary(TxBcounts$Series)

# Adding a textbook country of use variable from the series variable
TxBcounts$Country <- TxBcounts$Series
TxBcounts$Country <- car::recode(TxBcounts$Series, "c('Access', 'GreenLine', 'NGL') = 'Germany'; c('Achievers', 'EIM', 'Solutions') = 'Spain'; c('HT', 'NB', 'NM', 'POC', 'JTT') = 'France'")
summary(TxBcounts$Country)

# Re-order variables
colnames(TxBcounts)
TxBcounts <- TxBcounts %>% 
  select(order(names(.))) %>% # Order alphabetically first
  select(Filename, Country, Series, Level, Register, Words, everything())

```

## Summary statistics

```{r}
#| label: TEC-summary-stats

TxBcounts %>% 
  group_by(Register) %>% 
  summarise(totaltexts = n(), totalwords = sum(Words), mean = as.integer(mean(Words)), sd = as.integer(sd(Words)), TTRmean = mean(TTR))

#TxBcounts <- saveRDS(TxBcounts, here("processed_data", "TxBcounts.rds"))

```

# Data preparation for PCA

## Removal of Poetry texts

```{r}
#| label: Poetry-removal

nrow(TxBcounts)

TxBcounts <- TxBcounts %>% 
  filter(Register!="Poetry") %>% 
  droplevels(.)

nrow(TxBcounts)

summary(TxBcounts$Register)

```

## Feature distributions

```{r}
#| label: distribution-viz
#| fig-height: 40

TxBcounts %>%
  select(-Words) %>% 
  keep(is.numeric) %>% 
  tidyr::gather() %>% # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2
  ggplot(aes(value)) +
    theme_bw() +
    facet_wrap(~ key, scales = "free", ncol = 4) +
    scale_x_continuous(expand=c(0,0)) +
    geom_histogram(bins = 30, colour= "darkred", fill = "darkred", alpha = 0.5)

#ggsave(here("plots", "TEC-HistogramPlotsAllVariablesTEC-only.svg"), width = 20, height = 45)

```

### Feature removal I

```{r}
#| label: feature-removal

# Removal of meaningless features:
# CD because numbers as digits were mostly removed from the textbooks
# LIKE and SO because they are "bin" features designed to ensure that the counts for these two words don't inflate other categories due to mistags.
TxBcounts <- TxBcounts %>% 
  select(-c(CD, LIKE, SO))

# Function to compute percentage of texts with occurrences meeting a condition
compute_percentage <- function(data, condition, threshold) {
  numeric_data <- Filter(is.numeric, data)
  percentage <- round(colSums(condition[, sapply(numeric_data, is.numeric)])/nrow(data) * 100, 2)
  percentage <- as.data.frame(percentage)
  colnames(percentage) <- "Percentage"
  percentage <- percentage %>% 
    filter(!is.na(Percentage)) %>%
    rownames_to_column() %>%
    arrange(Percentage)
  if (!missing(threshold)) {
    percentage <- percentage %>% 
      filter(Percentage > threshold)
  }
  return(percentage)
}

# Calculate percentage of texts with 0 occurrences of each feature
zero_features <- compute_percentage(TxBcounts, TxBcounts == 0, 66.6)
print(zero_features)

# Combine low frequency features into meaningful groups whenever this makes linguistic sense
TxBcounts <- TxBcounts %>% 
  mutate(JJPR = ABLE + JJPR, ABLE = NULL) %>% 
  mutate(PASS = PGET + PASS, PGET = NULL)

# Re-calculate percentage of texts with 0 occurrences of each feature
zero_features <- compute_percentage(TxBcounts, TxBcounts == 0, 66.6)
print(zero_features)

# Drop variables with low document frequency
TxBcounts <- select(TxBcounts, -one_of(zero_features$rowname))
ncol(TxBcounts)-8 # Number of linguistic features remaining
colnames(TxBcounts)

```

### Standardising normalised counts and identifying potential outliers

"As an alternative to removing very sparse feature, we apply a signed logarithmic transformation to deskew the feature distributions." (Neumann & Evert)

```{r}
#| label: z-standardisation-outliers

# First scale the normalised counts (z-standardisation) to be able to compare the various features
TxBcounts %>%
  select(-Words) %>% 
  keep(is.numeric) %>% 
  scale() ->
  TxBzcounts

boxplot(TxBzcounts, las = 3, main = "z-scores") # Slow to open!

# If necessary, remove any outliers at this stage.

TxBdata <- cbind(TxBcounts[,1:6], as.data.frame(TxBzcounts))
nrow(TxBdata)
str(TxBdata)

outliers <- TxBdata %>% 
  select(-c(Words, LD, TTR)) %>% 
  filter(if_any(where(is.numeric), ~ .x > 8)) %>% 
  select(Filename)

outliers

TxBcounts <- TxBcounts %>% 
  filter(!Filename %in% outliers$Filename)

nrow(TxBcounts)

TxBcounts %>%
  select(-Words) %>% 
  keep(is.numeric) %>% 
  scale() ->
  TxBzcounts

nrow(TxBzcounts)

boxplot(TxBzcounts, las = 3, main = "z-scores") # Slow to open!

#saveRDS(TxBcounts, here("processed_data", "TxBcounts3.rds")) # Last saved 16 Feb 2024

```

```{r}
#| label: z-transformed-distributions
#| fig-height: 40

TxBzcounts %>%
  as.data.frame() %>% 
  gather() %>% # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2
  ggplot(aes(value)) +
    theme_bw() +
    facet_wrap(~ key, scales = "free", ncol = 4) +
    scale_x_continuous(expand=c(0,0)) +
    geom_histogram(bins = 30, colour= "darkred", fill = "darkred", alpha = 0.5)

#ggsave(here("plots", "TEC-zscores-HistogramsAllVariablesTEC-only.svg"), width = 20, height = 45)

```

### Transforming the features to (partially) deskew these distributions

Signed log transformation function inspired by the SignedLog function proposed in https://cran.r-project.org/web/packages/DataVisualizations/DataVisualizations.pdf

```{r}
#| label: signed.log.transformation

# All features are signed log-transformed (this is also what Neumann & Evert 2021 do)
signed.log <- function(x) {
  sign(x) * log(abs(x) + 1)
  }

TxBzlogcounts <- signed.log(TxBzcounts) # Standardise first, then signed log transform

# The function above would only transform the most skewed variables. This is what Lee suggests doing but it makes the interpretation of the correlations quite tricky so I abandoned this idea.
# TxBzlogcounts2 <- TxBzcounts %>%
#   as.data.frame() %>% 
#     mutate(across(.cols = c(AMP, ASPECT, CAUSE, COND, CUZ, DMA, EMPH, EX, EXIST, FPP1P, FPP1S, FPUH, FREQ, HDG, MDCA, MDCO, MDNE, MDWO, MDWS, OCCUR, PASS, PEAS, PLACE, POLITE, PROG, QUPR, RP, SPLIT, STPR, THATD, THRC, THSC, TPP3P, TPP3S, VBD, VBG, VBN, VIMP, WHQU, WHSC, YNQU),
#         .fns = signed.log)) %>% 
#     rename_with(.cols = c(AMP, ASPECT, CAUSE, COND, CUZ, DMA, EMPH, EX, EXIST, FPP1P, FPP1S, FPUH, FREQ, HDG, MDCA, MDCO, MDNE, MDWO, MDWS, OCCUR, PASS, PEAS, PLACE, POLITE, PROG, QUPR, RP, SPLIT, STPR, THATD, THRC, THSC, TPP3P, TPP3S, VBD, VBG, VBN, VIMP, WHQU, WHSC, YNQU),
#                 .fn = ~paste0(., '_signedlog'))

boxplot(TxBzlogcounts, las=3, main="log-transformed z-scores")

#saveRDS(TxBzlogcounts, here("processed_data", "TxBzlogcounts.rds")) # Last saved 16 Feb 2024
```

```{r}
#| label: signed.log.transformation-distributions
#| fig-height: 40

TxBzlogcounts %>%
  as.data.frame() %>% 
  gather() %>% # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2
  ggplot(aes(value, after_stat(density))) +
  theme_bw() +
  facet_wrap(~ key, scales = "free", ncol = 4) +
  scale_x_continuous(expand=c(0,0)) +
  scale_y_continuous(limits = c(0,NA)) +
  geom_histogram(bins = 30, colour= "black", fill = "grey") +
  geom_density(colour = "darkred", weight = 2, fill="darkred", alpha = .4)

#ggsave(here("plots", "DensityPlotsAllVariablesSignedLog-TEC-only.svg"), width = 15, height = 49)

```

These plots serve to illustrate the effects of the variable transformations performed in the above chunks.

```{r}
#| label: example-correlation-plots
#| fig-height: 30

# This is a slightly amended version of the PerformanceAnalytics::chart.Correlation() function. It simply removes the significance stars that are meaningless with this many data points (see commented out lines below)

chart.Correlation.nostars <- function (R, histogram = TRUE, method = c("pearson", "kendall", "spearman"), ...) {
  x = checkData(R, method = "matrix")
  if (missing(method)) 
    method = method[1]
  panel.cor <- function(x, y, digits = 2, prefix = "", use = "pairwise.complete.obs", method = "pearson", cex.cor, ...) {
    usr <- par("usr")
    on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- cor(x, y, use = use, method = method)
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste(prefix, txt, sep = "")
    if (missing(cex.cor)) 
      cex <- 0.8/strwidth(txt)
    test <- cor.test(as.numeric(x), as.numeric(y), method = method)
    # Signif <- symnum(test$p.value, corr = FALSE, na = FALSE, 
    #                  cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1), symbols = c("***", 
    #                                                                           "**", "*", ".", " "))
    text(0.5, 0.5, txt, cex = cex * (abs(r) + 0.3)/1.3)
    # text(0.8, 0.8, Signif, cex = cex, col = 2)
  }
  f <- function(t) {
    dnorm(t, mean = mean(x), sd = sd.xts(x))
  }
  dotargs <- list(...)
  dotargs$method <- NULL
  rm(method)
  hist.panel = function(x, ... = NULL) {
    par(new = TRUE)
    hist(x, col = "light gray", probability = TRUE, 
         axes = FALSE, main = "", breaks = "FD")
    lines(density(x, na.rm = TRUE), col = "red", lwd = 1)
    rug(x)
  }
  if (histogram) 
    pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor, 
          diag.panel = hist.panel)
  else pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor)
}

# Example plot without any variable transformation
example1 <- TxBcounts %>%
  select(NN,PROG,SPLIT,ACT,FPP1S)

#png(here("plots", "CorrChart-TEC-examples-normedcounts.png"), width = 20, height = 20, units = "cm", res = 300)
chart.Correlation.nostars(example1, histogram=TRUE, pch=19)
dev.off()

# Example plot with transformed variables
example2 <- TxBzlogcounts %>%
  as.data.frame() %>% 
  select(NN,PROG,SPLIT,ACT,FPP1S)

#png(here("plots", "CorrChart-TEC-examples-zsignedlogcounts.png"), width = 20, height = 20, units = "cm", res = 300)
chart.Correlation.nostars(example2, histogram=TRUE, pch=19)
dev.off()
```

### Visualisation of feature correlations

```{r}
#| label: heatmap
#| fig-height: 30

# Simple heatmap in base R (inspired by Stephanie Evert's SIGIL code)
cor.colours <- c(
  hsv(h=2/3, v=1, s=(10:1)/10), # blue = negative correlation 
  rgb(1,1,1), # white = no correlation 
  hsv(h=0, v=1, s=(1:10/10))) # red = positive correlation

#png(here("plots", "heatmapzlogcounts-TEC-only.png"), width = 30, height= 30, units = "cm", res = 300)
heatmap(cor(TxBzlogcounts), 
        symm=TRUE, 
        zlim=c(-1,1), 
        col=cor.colours, 
        margins=c(7,7))
#dev.off()

```

## Composition of TEC texts/files entered in the MDAs

```{r}
#| label: TEC-metadata

# Total number of words
TxBcounts %>% summarise(sum(Words))

metadata <- TxBcounts %>% 
  select(Filename, Country, Series, Level, Register, Words) %>% 
  mutate(Volume = paste(Series, Level)) %>% 
  mutate(Volume = fct_rev(Volume)) %>% 
  mutate(Volume = fct_reorder(Volume, as.numeric(Level))) %>% 
  group_by(Volume) %>% 
  mutate(wordcount = sum(Words)) %>% 
  ungroup() %>% 
  distinct(Volume, .keep_all = TRUE)

# Plot for book
metadata2 <- TxBcounts %>% 
  select(Country, Series, Level, Register, Words) %>% 
  mutate(Volume = paste(Series, Level)) %>% 
  mutate(Volume = fct_rev(Volume)) %>% 
  #mutate(Volume = fct_reorder(Volume, as.numeric(Level))) %>% 
  group_by(Volume, Register) %>% 
  mutate(wordcount = sum(Words)) %>% 
  ungroup() %>% 
  distinct(Volume, Register, .keep_all = TRUE)

# This is the palette created above on the basis of the suffrager pakcage (but without needed to install the package)
palette <- c("#BD241E", "#A18A33", "#15274D", "#D54E1E", "#EA7E1E", "#4C4C4C", "#722672", "#F9B921", "#267226")

PlotSp <- metadata2 %>% 
  filter(Country=="Spain") %>% 
  #arrange(Volume) %>% 
  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + 
    geom_bar(stat = "identity", position = "stack") +
    coord_flip(expand = FALSE) + # Removes those annoying ticks before each bar label
    theme_minimal() + theme(legend.position = "none") +
    labs(x = "Spain", y = "Cumulative word count") +
    scale_fill_manual(values = palette[c(5,4,3,2,1)], 
                      guide = guide_legend(reverse = TRUE))

PlotGer <- metadata2 %>% 
  filter(Country=="Germany") %>% 
  #arrange(Volume) %>% 
  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + 
    geom_bar(stat = "identity", position = "stack") +
    coord_flip(expand = FALSE) +
    labs(x = "Germany", y = "") +
    scale_fill_manual(values = palette[c(5,4,3,2,1)], guide = guide_legend(reverse = TRUE)) +
    theme_minimal() + theme(legend.position = "none")

PlotFr <- metadata2 %>% 
  filter(Country=="France") %>% 
  #arrange(Volume) %>% 
  ggplot(aes(x = Volume, y = wordcount, fill = fct_rev(Register))) + 
    geom_bar(stat = "identity", position = "stack") +
    coord_flip(expand = FALSE) +
    labs(x = "France", y  = "", fill = "Register subcorpus") +
    scale_fill_manual(values = palette[c(5,4,3,2,1)], guide = guide_legend(reverse = TRUE, legend.hjust = 0)) +
    theme_minimal() + theme(legend.position = "top", legend.justification = "left")

PlotFr /
PlotGer /
PlotSp

#ggsave(here("plots", "TEC-T_wordcounts_book.svg"), width = 8, height = 12)

```

```{r}
#| label: TEC-metadata2

# Meta-data on % of instructional language in each textbook
metadataInstr <- TxBcounts %>% 
  select(Country, Series, Level, Register, Words) %>% 
  filter(Register=="Instructional") %>% 
  mutate(Volume = paste(Series, Register)) %>% 
  mutate(Volume = fct_rev(Volume)) %>% 
  mutate(Volume = fct_reorder(Volume, as.numeric(Level))) %>% 
  group_by(Volume, Register) %>% 
  mutate(InstrWordcount = sum(Words)) %>% 
  ungroup() %>% 
  distinct(Volume, .keep_all = TRUE) %>% 
  select(Series, InstrWordcount)

metadataInstr

metaWordcount <- TxBcounts %>% 
  select(Country, Series, Level, Register, Words) %>% 
  group_by(Series) %>% 
  mutate(TECwordcount = sum(Words)) %>% 
  ungroup() %>% 
  distinct(Series, .keep_all = TRUE) %>% 
  select(Series, TECwordcount)

wordcount <- merge(metaWordcount, metadataInstr, by = "Series")

wordcount %>% 
  mutate(InstrucPercent = InstrWordcount/TECwordcount*100) %>% 
  arrange(InstrucPercent) %>% 
  mutate(InstrucPercent = round(InstrucPercent, 2))

```

# Packages used in this script

```{r}
#| label: package-citations
#| include: false

#packages.bib <- sapply(1:length(loadedNamespaces()), function(i) toBibtex(citation(loadedNamespaces()[i])))

knitr::write_bib(c(.packages(), "knitr"), "packages.bib")

sessionInfo()

```
