# Data Preparation for the Model of Textbook English vs. ‘real-world’ English

This script documents the steps taken to pre-process the data extracted from the Textbook English Corpus (TEC) and the three reference corpora that were ultimately entered in the comparative multi-dimensional model of Textbook English as compared to English outside the EFL classroom (Chapter 7).

## Packages required

The following packages must be installed and loaded to process the data.

```{r}
#| label: set-up

#renv::restore() # Restore the project's dependencies from the lockfile to ensure that same package versions are used as in the original thesis.

library(broom.mixed) # For checking singularity issues 
library(car) # For recoding data
library(corrplot) # For the feature correlation matrix
library(cowplot) # For nice plots
library(DT) # To display interactive HTML tables
library(emmeans) # Comparing group means of predicted values
library(GGally) # For ggpairs
library(gridExtra) # For making large faceted plots
library(here) # For ease of sharing
library(knitr) # Loaded to display the tables using the kable() function
library(lme4) # For mixed effects modelling
library(psych) # For various useful stats function, including KMO()
library(scales) # For working with colours
library(sjPlot) # For nice tabular display of regression models
library(tidyverse) # For data wrangling and plotting
library(visreg) # For nice visualisations of model results
select <- dplyr::select
filter <- dplyr::filter
```

```{r palette}
#| echo: false
# Colours used in Register Studies paper and included in Open Access plots published on Zenodo:
#colours <- suf_palette(name = "london", n = 6, type = "continuous") # Very nice, similar to OrRd palette
#scales::show_col(colours)
#colours <- colours[6:1] 

# Colour scheme used in PhD thesis:
colours = c("#F9B921", "#A18A33",  "#722672", "#BD241E", "#15274D", "#D54E1E") 
# scales::show_col(colours)

```

## Data import from MFTE outputs

The raw data used in this script comes from the matrices of mixed normalised frequencies as output by the [MFTE Perl v. 3.1](https://github.com/mshakirDr/MultiFeatureTaggerEnglish) [@lefoll2021].

### Spoken BNC2014

```{r SpokenBNC2014}
#| code-fold: true

SpokenBNC2014 <- read.delim(here("data", "MFTE", "SpokenBNC2014_3.1_normed_complex_counts.tsv"), header = TRUE, stringsAsFactors = TRUE)

SpokenBNC2014$Series <- "Spoken BNC2014"
SpokenBNC2014$Level <- "Ref."
SpokenBNC2014$Country <- "Spoken BNC2014"
SpokenBNC2014$Register <- "Spoken BNC2014"

```

These normalised frequencies were computed on the basis of my own "John and Jill in Ivybridge" version of the Spoken BNC2014 with added full stops at speaker turns (see Appendix B for details). This corpus comprises of `{r} nrow(SpokenBNC2014) |> format(big.mark=",")` texts, all of which were used in the following analyses.

```{r}
#| echo: false

datatable(SpokenBNC2014,
  filter = "top",) |> 
  formatRound(3:ncol(SpokenBNC2014), digits=2)

```


### Youth Fiction corpus

```{r YouthFiction}
#| code-fold: true

YouthFiction <- read.delim(here("data", "MFTE", "YF_sampled_500_3.1_normed_complex_counts.tsv"), header = TRUE, stringsAsFactors = TRUE)

YouthFiction$Series <- "Youth Fiction"
YouthFiction$Level <- "Ref."
YouthFiction$Country <- "Youth Fiction"
YouthFiction$Register <- "Youth Fiction"

```

These normalised frequencies were computed on the basis of the random samples of approximately 5,000 words of the books of the Youth Fiction corpus (for details of the works included in this corpus, see Appendix B). The sampling procedure is described in Section 4.3.2.4 of the book. This dataset consists of `{r} nrow(YouthFiction) |> format(big.mark=",")` files.

```{r}
#| echo: false

datatable(YouthFiction,
  filter = "top",) |> 
  formatRound(3:ncol(YouthFiction), digits=2)

```

### Informative Texts for Teens (InfoTeens) corpus

```{r InfoTeencounts}
#| code-fold: true

InfoTeen <- read.delim(here("data", "MFTE", "InfoTeen_3.1_normed_complex_counts.tsv"), header = TRUE, stringsAsFactors = TRUE)

# Removes three outlier files which should not have been included in the corpus as they contain exam papers only
InfoTeen <- InfoTeen |> 
  filter(Filename!=".DS_Store" & Filename!="Revision_World_GCSE_10529068_wjec-level-law-past-papers.txt" & Filename!="Revision_World_GCSE_10528474_wjec-level-history-past-papers.txt" & Filename!="Revision_World_GCSE_10528472_edexcel-level-history-past-papers.txt")

InfoTeen$Series <- "Info Teens"
InfoTeen$Level <- "Ref."
InfoTeen$Country <- "Info Teens"
InfoTeen$Register <- "Info Teens"

```

Details of the composition of the Info Teens corpus can be found in Section 4.3.2.5 of the book. The version used in the present study comprises `{r} nrow(InfoTeen) |> format(big.mark=",")` texts.

```{r}
#| echo: false

datatable(InfoTeen,
  filter = "top",) |> 
  formatRound(3:ncol(InfoTeen), digits=2)

```


## Merging TEC and reference corpora data

```{r DataMerging}
#| code-fold: true
#| echo: false

TxBncounts <- readRDS(here("data", "processed", "TxBcounts.rds"))
All3Reg <- c("Conversation", "Fiction", "Informative")

TxBncounts3Reg <- TxBncounts |> 
  filter(Register %in% All3Reg) |> 
  droplevels()

ncounts <- bind_rows(TxBncounts3Reg, InfoTeen, SpokenBNC2014, YouthFiction, .id = "Corpus")

# Convert all character vectors to factors
ncounts[sapply(ncounts, is.character)] <- lapply(ncounts[sapply(ncounts, is.character)], as.factor)

# Change all NAs to 0
ncounts[is.na(ncounts)] <- 0

levels(ncounts$Corpus) <- list(Textbook.English="1", Informative.Teens="2", Spoken.BNC2014="3", Youth.Fiction="4")
# summary(ncounts$Corpus)
# summary(ncounts$Series)

# Wrangle metadata variables
ncounts$Subcorpus <- ncounts$Register
#levels(ncounts$Subcorpus)
levels(ncounts$Subcorpus) <- c("Textbook Conversation", "Textbook Fiction", "Info Teens Ref.", "Textbook Informative", "Spoken BNC2014 Ref.", "Youth Fiction Ref.")

# Re-order registers
levels(ncounts$Register) <- c("Conversation", "Fiction", "Informative", "Informative", "Conversation", "Fiction")
#summary(ncounts$Register)

# Re-order variables
ncounts <- select(ncounts, order(names(ncounts))) |> 
  select(Filename, Register, Level, Series, Country, Corpus, Subcorpus, Words, everything()) # Then place the metadata variable at the front of the table

#saveRDS(ncounts, here("data", "processed", "counts3Reg.rds")) # Last saved 6 March 2024

```

### Corpus size

These tables provide some summary statistics about the texts/files whose normalised feature frequencies were entered in the model of Textbook English vs. real-world English described in Chapter 7.

```{r}
#| label: corpora-summary-stats
#| code-fold: true

summary(ncounts$Subcorpus) |> 
  kable(col.names = c("(Sub)corpus", "# texts"),
        format.args = list(big.mark = ","))

ncounts  |>  
  group_by(Register) |>  
  summarise(totaltexts = n(), 
            totalwords = sum(Words), 
            mean = as.integer(mean(Words)), 
            sd = as.integer(sd(Words)), 
            TTRmean = mean(TTR)) |>  
  kable(digits = 2, 
        format.args = list(big.mark = ","),
        col.names = c("Register", "# texts/files", "# words", "mean # words per text", "SD", "mean TTR"))

```

## Data preparation for PCA

### Feature distributions

The distributions of each linguistic features were examined by means of visualisation. As shown below, before transformation, many of the features displayed highly skewed distributions.

```{r}
#| label: distribution-viz
#| code-fold: true
#| fig-height: 40

#ncounts <- readRDS(here("data", "processed", "counts3Reg.rds"))

ncounts |>
  select(-Words) |> 
  keep(is.numeric) |> 
  gather() |> # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2
  ggplot(aes(value, after_stat(density))) +
    theme_bw() +
    facet_wrap(~ key, scales = "free", ncol = 4) +
    scale_x_continuous(expand=c(0,0)) +
    scale_y_continuous(limits = c(0,NA)) +
    geom_histogram(bins = 30, colour= "black", fill = "grey") +
    geom_density(colour = "darkred", weight = 2, fill="darkred", alpha = .4)

#ggsave(here("plots", "DensityPlotsAllVariables.svg"), width = 15, height = 49)

```

### Feature removal

A number of features were removed from the dataset as they are not linguistically interpretable. In the case of the TEC, this included the variable CD because numbers spelt out as digits were removed from the textbooks before these were tagged with the MFTE. In addition, the variables LIKE and SO because these are "bin" features included in the output of the MFTE to ensure that the counts for these polysemous words do not inflate other categories due to mistags [@lefoll2021a].

Whenever linguistically meaningful, very low-frequency features, features with low MSA or communalities (see chunks below) were merged. Finally, features absent from more than third of texts were also excluded. For the comparative analysis of TEC and the reference corpora, the following linguistic features were excluded from the analysis due to low dispersion:

```{r}
#| label: feature-removal
#| code-fold: true

# Removal of meaningless feature: CD because numbers as digits were mostly removed from the textbooks, LIKE and SO because they are dustbin categories
ncounts <- ncounts |> 
  select(-c(CD, LIKE, SO))

# Combine problematic features into meaningful groups whenever this makes linguistic sense
ncounts <- ncounts |> 
  mutate(JJPR = JJPR + ABLE, ABLE = NULL) |> 
  mutate(PASS = PGET + PASS, PGET = NULL) |> 
  mutate(TPP3 = TPP3S + TPP3P, TPP3P = NULL, TPP3S = NULL) |> # Merged due to TTP3P having an individual MSA < 0.5
  mutate(FQTI = FREQ + TIME, FREQ = NULL, TIME = NULL) # Merged due to TIME communality < 0.2 (see below)

# Function to compute percentage of texts with occurrences meeting a condition
compute_percentage <- function(data, condition, threshold) {
  numeric_data <- Filter(is.numeric, data)
  percentage <- round(colSums(condition[, sapply(numeric_data, is.numeric)])/nrow(data) * 100, 2)
  percentage <- as.data.frame(percentage)
  colnames(percentage) <- "Percentage"
  percentage <- percentage |> 
    filter(!is.na(Percentage)) |>
    rownames_to_column() |>
    arrange(Percentage)
  if (!missing(threshold)) {
    percentage <- percentage |> 
      filter(Percentage > threshold)
  }
  return(percentage)
}

# Calculate percentage of texts with 0 occurrences of each feature
zero_features <- compute_percentage(ncounts, ncounts == 0, 66.6)
zero_features |> 
  kable(col.names = c("Feature", "% texts with zero occurrences"))

# Drop variables with low document frequency
ncounts2 <- select(ncounts, -one_of(zero_features$rowname))

```

These feature removal operations resulted in a feature set of `{r} ncol(ncounts2)-8` linguistic variables.

### Identifying outlier texts

All normalised frequencies were normalised to identify any potential outlier texts.

```{r}
#| label: z-standardisation-outliers
#| fig-width: 15
#| fig-height: 10

# First scale the normalised counts (z-standardisation) to be able to compare the various features
zcounts <- ncounts2 |>
  select(-Words) |> 
  keep(is.numeric) |> 
  scale()

# If necessary, remove any outliers at this stage.
data <- cbind(ncounts2[,1:8], as.data.frame(zcounts))
outliers <- data |> 
 filter(if_any(where(is.numeric) & !Words,  .fns = function(x){x > 8}))  |>
  select(Filename, Corpus, Register, Words) 
```

The following outlier texts were identified according to the above conditions and excluded in subsequent analyses.

```{r}
#| label: list-outliers
#| code-fold: true

# These are potential outlier texts :
outliers |> 
  kable(col.names = c("Filename", "Corpus", "Register", "# words"))
```

We check that that outlier texts are not particularly long or short texts by looking at the distribution of text/file length of the outliers.

```{r}
#| code-fold: true

summary(outliers$Words)

hist(outliers$Words, breaks = 30)
```

We also check the distribution of outlier texts across the four corpora. The majority come from the Info Teens corpus, though quite a few are also from the TEC.

```{r}
#| code-fold: true

summary(outliers$Corpus) |> 
  kable(col.names = c("(Sub)corpus", "# outlier texts"))

# Report on the manual check of a sample of these outliers:

# Encyclopedia_Kinds_au_10085347_Nobel_Prize_in_Chemistry.txt is essentially a list of Nobel prize winners but with some additional information. In other words, not a bad representative of the type of texts of the Info Teen corpus.
# Solutions_Elementary_ELF_Spoken_0013 --> Has a lot of "going to" constructions because they are learnt in this chapter but is otherwise a well-formed text.
# Teen_Kids_News_10403972_a-brief-history-of-white-house-weddings --> No issues
# Teen_Kids_News_10403301_golden-globe-winners-2019-the-complete-list --> Similar to the Nobel prize laureates text.
# Revision_World_GCSE_10528123_gender-written-textual-analysis-framework --> Text includes bullet points tokenised as the letter "o" but otherwise a fairly typical informative text.

# Removing the outliers at the request of the reviewers (but comparisons of models including the outliers showed that the results are very similar):
ncounts3 <- ncounts2 |> 
  filter(!Filename %in% outliers$Filename)

#saveRDS(ncounts3, here("data", "processed", "ncounts3_3Reg.rds")) # Last saved 6 March 2024

```

This resulted in `{r} nrow(ncounts3) |> format(big.mark=",")` texts/files being included in the comparative model of Textbook English vs. 'real-world' English. These standardised feature frequencies were distributed as follows:

```{r}
#| label: z-transformed-distributions
#| fig-width: 15
#| fig-height: 10
#| code-fold: true

zcounts3 <- ncounts3 |>
  select(-Words) |> 
  keep(is.numeric) |> 
  scale()

boxplot(zcounts3, las = 3, main = "z-scores") # Slow

```

### Signed log transformation

A signed logarithmic transformation was applied to (further) deskew the feature distributions [see @diwersy2014; @neumann2021].

The signed log transformation function was inspired by the SignedLog function proposed in <https://cran.r-project.org/web/packages/DataVisualizations/DataVisualizations.pdf>.

```{r}
#| label: signed.log.transformation

signed.log <- function(x) {sign(x)*log(abs(x)+1)}

# Standardise first, then sign log transform
zlogcounts <- signed.log(zcounts3) 

```

```{r}
#| echo: false

#saveRDS(zlogcounts, here("data", "processed", "zlogcounts_3Reg.rds")) # Last saved 16 March 2024

```

The new feature distributions are visualised below.

```{r}
#| label: signed.log.transformation-distributions
#| fig-height: 40
#| code-fold: true

zlogcounts |>
  as.data.frame() |> 
  gather() |> # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2
  ggplot(aes(value, after_stat(density))) +
  theme_bw() +
  facet_wrap(~ key, scales = "free", ncol = 4) +
  scale_x_continuous(expand=c(0,0)) +
  scale_y_continuous(limits = c(0,NA)) +
  geom_histogram(bins = 30, colour= "black", fill = "grey") +
  geom_density(colour = "darkred", weight = 2, fill="darkred", alpha = .4)

#ggsave(here("plots", "DensityPlotsAllVariablesSignedLog.svg"), width = 15, height = 49)

```

### Merging of data for MDA

```{r}
#| code-fold: true

zlogcounts <- readRDS(here("data", "processed", "zlogcounts_3Reg.rds")) 
#nrow(zlogcounts)
#colnames(zlogcounts)

ncounts3 <- readRDS(here("data", "processed", "ncounts3_3Reg.rds"))
#nrow(ncounts3)
#colnames(ncounts3)

data <- cbind(ncounts3[,1:8], as.data.frame(zlogcounts))
#saveRDS(data, here("data", "processed", "datazlogcounts_3Reg.rds")) # Last saved 16 March 2024

```

The final dataset comprises of `{r} nrow(data) |> format(big.mark=",")` texts/files, divided as follows:

```{r}
#| echo: false

summary(data$Subcorpus) |> 
  kable(col.names = c("(Sub)corpus", "# texts/files"))

```

```{r prepare-data}
#| echo: false

# Quick import
#data <- readRDS(here("data", "processed", "datazlogcounts_3Reg.rds"))

# This rearranges the levels in the desired order for the plot legends:
data <- data |> 
  mutate(Subcorpus = fct_relevel(Subcorpus, "Info Teens Ref.", after = 9))

```

## Testing factorability of data

### Visualisation of feature correlations

We begin by visualising the correlations of the transformed feature frequencies using the `heatmap` function of the `stats` library. Negative correlations are rendered in blue; positive ones are in red.

```{r heatmap}
#| code-fold: true
#| fig-width: 15

# Simple heatmap in base R (inspired by Stephanie Evert's SIGIL code)
cor.colours <- c(
  hsv(h=2/3, v=1, s=(10:1)/10), # blue = negative correlation 
  rgb(1,1,1), # white = no correlation 
  hsv(h=0, v=1, s=(1:10/10))) # red = positive correlation

#png(here("plots", "heatmapzlogcounts.png"), width = 30, height= 30, units = "cm", res = 300)
heatmap(cor(zlogcounts), 
        symm=TRUE, 
        zlim=c(-1,1), 
        col=cor.colours, 
        margins=c(7,7))

#dev.off()
```

### Collinearity

As a result of the normalisation unit of finite verb phrases for verb-based features, the present tense (VPRT) and past tense (VBD) variables are correlated to a very high degree:

```{r collinear}
cor(data$VPRT, data$VBD) |> round(2)
```

We therefore remove the least marked of the pair of collinear variables: VPRT.

```{r collinear2}
data <- data |> 
  select(-c(VPRT))
```

### MSA

```{r factorability.TEC}

kmo <- KMO(data[,9:ncol(data)]) # The first eight columns contain metadata.

```

The overall MSA value of the dataset is `{r} kmo$MSA |> round(2)`. The features have the following individual MSA values (ordered from lowest to largest):

```{r}
#| code-fold: true

kmo$MSAi[order(kmo$MSAi)] |>  round(2)
```

We aim to remove features with an individual MSA \< 0.5. All features have individual MSAs of \> 0.5 (but only because TPP3P was merged into a broader category in an earlier chunk).

### Scree plot

Six components were originally retained on the basis of the following screeplot, though only the first four were found to be interpretable and were therefore included in the model.

```{r}
#| code-fold: true

# png(here("plots", "screeplot-TEC-Ref_3Reg.png"), width = 20, height= 12, units = "cm", res = 300)
scree(data[,9:ncol(data)], factors = FALSE, pc = TRUE) # 
# dev.off()

# Perform PCA
pca1 <- psych::principal(data[9:ncol(data)], 
                         nfactors = 6)
```

### Communalities

If features with final communalities of \< 0.2 are removed, TIME would have to be removed. TIME was therefore merged with FREQ in an earlier chunk so that now all features have final communalities of \> 0.2 (note that this is a very generous threshold!).

```{r communalities}
#| code-fold: true

pca1$communality |> sort() |> round(2)

#saveRDS(data, here("data", "processed", "dataforPCA.rds")) # Last saved on 6 March 2024

```

The final dataset entered in the analysis described in Chapter 7 therefore comprises `{r} nrow(data)  |> format(big.mark=",")` texts/files, each with logged standardised normalised frequencies for `{r} ncol(data)-8` linguistic features.
